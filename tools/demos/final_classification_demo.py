#!/usr/bin/env python3
"""
üéØ Final Demo: Complete Arabic Particles Classification & Segregation
Showcases all 8 categories with comprehensive analysis capabilities
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


import_data json
import_data sys
from pathlib import_data Path
from typing import_data Dict, List

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from comprehensive_particle_classification import_data ComprehensiveParticleAnalyzer

def create_category_demonstration():
    """Create demonstration text covering all 8 particle categories"""
    
    # Texts designed to showcase each category
    category_examples = {
        "ÿ¥ÿ±ÿ∑": [
            "ÿ•ŸÜ ÿØÿ±ÿ≥ÿ™ ÿ¨ŸäÿØÿßŸã ŸÅÿ≥ÿ™ŸÜÿ¨ÿ≠",  # Conditional particle
            "ÿ•ÿ∞ÿß ÿ¨ÿßÿ° ÿßŸÑÿ£ÿ≥ÿ™ÿßÿ∞ ÿ≥ŸÜÿ®ÿØÿ£ ÿßŸÑÿØÿ±ÿ≥",
            "ŸÉŸÑŸÖÿß ŸÇÿ±ÿ£ÿ™ ÿ™ÿπŸÑŸÖÿ™ ÿ¥Ÿäÿ¶ÿßŸã ÿ¨ÿØŸäÿØÿßŸã"
        ],
        "ÿßÿ≥ÿ™ŸÅŸáÿßŸÖ": [
            "ŸáŸÑ ŸÅŸáŸÖÿ™ ÿßŸÑÿØÿ±ÿ≥ÿü",  # Yes/No question
            "ŸÖŸÜ ÿ¨ÿßÿ° ÿ•ŸÑŸâ ÿßŸÑŸÖÿØÿ±ÿ≥ÿ© ÿßŸÑŸäŸàŸÖÿü",  # Who question
            "ŸÖÿßÿ∞ÿß ÿ™ÿ±ŸäÿØ ÿ£ŸÜ ÿ™ÿØÿ±ÿ≥ÿü",  # What question
            "ÿ£ŸäŸÜ ÿ∞Ÿáÿ® ÿßŸÑÿ∑ŸÑÿßÿ®ÿü",  # Where question
            "ŸÖÿ™Ÿâ ÿ≥Ÿäÿ®ÿØÿ£ ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜÿü",  # When question
            "ŸÉŸäŸÅ ÿ™ÿ≠ŸÑ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ≥ÿ£ŸÑÿ©ÿü"  # How question
        ],
        "ÿßÿ≥ÿ™ÿ´ŸÜÿßÿ°": [
            "ÿ¨ÿßÿ° ŸÉŸÑ ÿßŸÑÿ∑ŸÑÿßÿ® ÿ•ŸÑÿß ÿ£ÿ≠ŸÖÿØ",  # Exception
            "ŸÑÿß ÿ£ÿ≠ÿ® ÿ¥Ÿäÿ¶ÿßŸã ÿ∫Ÿäÿ± ÿßŸÑŸÇÿ±ÿßÿ°ÿ©",
            "ŸÉŸÑ ÿ¥Ÿäÿ° ÿ¨ŸÖŸäŸÑ ÿ≥ŸàŸâ Ÿáÿ∞ÿß"
        ],
        "ŸÜŸÅŸä": [
            "ŸÑÿß ÿ™ÿ™ŸÉŸÑŸÖ ŸÅŸä ÿßŸÑÿµŸÅ",  # Negation
            "ŸÑŸÜ ÿ£ŸÜÿ≥Ÿâ Ÿáÿ∞ÿß ÿßŸÑÿØÿ±ÿ≥ ÿ£ÿ®ÿØÿßŸã",  # Future negation
            "ŸÑŸÖ Ÿäÿ≠ÿ∂ÿ± ÿßŸÑÿ∑ÿßŸÑÿ® ÿ£ŸÖÿ≥",  # Past negation
            "ŸÑŸäÿ≥ ÿßŸÑÿ∑ŸÇÿ≥ ÿ¨ŸÖŸäŸÑÿßŸã ÿßŸÑŸäŸàŸÖ"  # Present negation
        ],
        "ÿ•ÿ¥ÿßÿ±ÿ©": [
            "Ÿáÿ∞ÿß ŸÉÿ™ÿßÿ® ŸÖŸÅŸäÿØ ÿ¨ÿØÿßŸã",  # Near demonstrative (masculine)
            "Ÿáÿ∞Ÿá ŸÖÿØÿ±ÿ≥ÿ© ŸÖŸÖÿ™ÿßÿ≤ÿ©",  # Near demonstrative (feminine)
            "ÿ∞ŸÑŸÉ ÿßŸÑÿ®Ÿäÿ™ ÿ®ÿπŸäÿØ ÿπŸÜÿß",  # Far demonstrative (masculine)
            "ÿ™ŸÑŸÉ ÿßŸÑÿ≥Ÿäÿßÿ±ÿ© ÿ≥ÿ±Ÿäÿπÿ©",  # Far demonstrative (feminine)
            "Ÿáÿ§ŸÑÿßÿ° ÿßŸÑÿ∑ŸÑÿßÿ® ŸÖÿ¨ÿ™ŸáÿØŸàŸÜ",  # Near plural
            "ÿ£ŸàŸÑÿ¶ŸÉ ÿßŸÑÿ£ÿ≥ÿßÿ™ÿ∞ÿ© ÿÆÿ®ÿ±ÿßÿ°",  # Far plural
            "ŸáŸÜÿß ŸÖŸÉÿßŸÜ ÿ¨ŸÖŸäŸÑ ŸÑŸÑÿØÿ±ÿßÿ≥ÿ©",  # Here
            "ŸáŸÜÿßŸÉ ŸÖŸÉÿ™ÿ®ÿ© ŸÉÿ®Ÿäÿ±ÿ©"  # There
        ],
        "ŸÜÿØÿßÿ°": [
            "Ÿäÿß ÿ£ÿ≠ŸÖÿØ ÿ™ÿπÿßŸÑ ŸáŸÜÿß",  # Common vocative
            "ÿ£Ÿä ÿ∑ÿßŸÑÿ® ÿßÿ¨ÿ™ŸáÿØ ŸÅŸä ÿØÿ±Ÿàÿ≥Ÿá",  # Near vocative
            "ÿ£Ÿäÿß ŸÖŸÜ ÿ≥ŸÖÿπ ŸÅŸÑŸäÿ¨ÿ®",  # Far vocative
            "Ÿàÿß ÿ≠ÿ≥ÿ±ÿ™ÿßŸá ÿπŸÑŸâ ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ∂ÿßÿ¶ÿπ"  # Exclamatory vocative
        ],
        "ŸÖŸàÿµŸàŸÑ": [
            "ÿßŸÑÿ∑ÿßŸÑÿ® ÿßŸÑÿ∞Ÿä ÿØÿ±ÿ≥ ŸÜÿ¨ÿ≠",  # Masculine rational
            "ÿßŸÑÿ∑ÿßŸÑÿ®ÿ© ÿßŸÑÿ™Ÿä ÿßÿ¨ÿ™ŸáÿØÿ™ ÿ™ŸÅŸàŸÇÿ™",  # Feminine rational
            "ÿßŸÑÿ∑ŸÑÿßÿ® ÿßŸÑÿ∞ŸäŸÜ ÿ≠ÿ∂ÿ±Ÿàÿß ŸÅŸáŸÖŸàÿß",  # Masculine plural rational
            "ÿßŸÑÿ∑ÿßŸÑÿ®ÿßÿ™ ÿßŸÑŸÑŸàÿßÿ™Ÿä ÿ¥ÿßÿ±ŸÉŸÜ ÿ£ÿ®ÿØÿπŸÜ",  # Feminine plural rational
            "ÿßŸÑŸÉÿ™ÿßÿ® ÿßŸÑÿ∞Ÿä ŸÇÿ±ÿ£ÿ™Ÿá ŸÖŸÅŸäÿØ",  # Thing/object
            "ŸÖÿß ÿ™ÿπŸÑŸÖÿ™Ÿá ÿßŸÑŸäŸàŸÖ ŸÖŸáŸÖ"  # Non-rational relative
        ],
        "ÿ∂ŸÖŸäÿ±": [
            "ÿ£ŸÜÿß ÿ£ÿ≠ÿ® ÿßŸÑÿπŸÑŸÖ ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ©",  # First person singular
            "ÿ£ŸÜÿ™ ÿ∑ÿßŸÑÿ® ŸÖÿ¨ÿ™ŸáÿØ ŸàŸÜÿ¥Ÿäÿ∑",  # Second person singular masculine
            "ÿ£ŸÜÿ™Ÿê ÿ∑ÿßŸÑÿ®ÿ© ŸÖÿ™ŸÅŸàŸÇÿ© ŸàŸÖÿ®ÿØÿπÿ©",  # Second person singular feminine
            "ŸáŸà ÿ£ÿ≥ÿ™ÿßÿ∞ ÿÆÿ®Ÿäÿ± ŸàŸÖÿ™ŸÖŸÉŸÜ",  # Third person singular masculine
            "ŸáŸä ŸÖÿπŸÑŸÖÿ© ŸÖÿ™ŸÖŸäÿ≤ÿ© ŸàŸÖÿ®ÿØÿπÿ©",  # Third person singular feminine
            "ŸÜÿ≠ŸÜ ÿ∑ŸÑÿßÿ® ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©",  # First person plural
            "ÿ£ŸÜÿ™ŸÖ ÿ™ÿØÿ±ÿ≥ŸàŸÜ ÿ®ÿ¨ÿØ Ÿàÿßÿ¨ÿ™ŸáÿßÿØ",  # Second person plural masculine
            "ŸáŸÖ Ÿäÿ≠ÿ®ŸàŸÜ ÿßŸÑŸÇÿ±ÿßÿ°ÿ© ŸàÿßŸÑŸÉÿ™ÿßÿ®ÿ©",  # Third person plural masculine
            "ŸáŸÜ ŸÖÿπŸÑŸÖÿßÿ™ ŸÅŸä ÿßŸÑŸÖÿØÿ±ÿ≥ÿ©"  # Third person plural feminine
        ]
    }
    
    return category_examples

def demonstrate_complete_system():
    """Demonstrate complete particle classification and segregation system"""
    
    print("üéØ COMPLETE ARABIC PARTICLES CLASSIFICATION & SEGREGATION SYSTEM")
    print("=" * 80)
    print("üìö Comprehensive Analysis of All 8 Grammatical Categories")
    print("=" * 80)
    
    # Initialize analyzer
    analyzer = ComprehensiveParticleAnalyzer()
    
    # Get category examples
    category_examples = create_category_demonstration()
    
    print("\nüìã CATEGORY-BY-CATEGORY ANALYSIS:")
    print("-" * 50)
    
    total_particles_found = 0
    total_texts_analyzed = 0
    category_statistics = {}
    
    for category, texts in category_examples.items():
        print(f"\nüè∑Ô∏è CATEGORY: {category}")
        print(f"   ({analyzer.category_definitions[category]['name']})")
        print(f"   {analyzer.category_definitions[category]['description']}")
        print("-" * 40)
        
        category_particles = []
        category_subcategories = set()
        
        for i, text in enumerate(texts, 1):
            analysis = analyzer.classify_and_segregate_text(text)
            total_texts_analyzed += 1
            
            particles_in_text = analysis['classification_summary']['particles_found']
            total_particles_found += particles_in_text
            
            print(f"\n   Text {i}: {text}")
            print(f"   üìä Particles: {particles_in_text} | Density: {analysis['statistics']['particle_density']:.1f}%")
            
            if particles_in_text > 0:
                text_particles = []
                for particle_data in analysis['detailed_analysis']:
                    if particle_data['category'] == category:
                        text_particles.append(particle_data['word'])
                        category_particles.append(particle_data['word'])
                        if 'subcategory' in particle_data:
                            category_subcategories.add(particle_data['subcategory'])
                
                if text_particles:
                    print(f"   üéØ Found: {', '.join(text_particles)}")
        
        # Category summary
        unique_particles = list(set(category_particles))
        category_statistics[category] = {
            'total_occurrences': len(category_particles),
            'unique_particles': len(unique_particles),
            'subcategories': len(category_subcategories),
            'particles_list': unique_particles,
            'subcategories_list': list(category_subcategories)
        }
        
        print(f"\n   üìà Category Summary:")
        print(f"      Total Occurrences: {len(category_particles)}")
        print(f"      Unique Particles: {len(unique_particles)} ({', '.join(unique_particles)})")
        print(f"      Subcategories: {len(category_subcategories)}")
        if category_subcategories:
            print(f"      Types: {', '.join(category_subcategories)}")
    
    # Comprehensive analysis of a complex sentence
    print(f"\n{'='*80}")
    print("üîç COMPREHENSIVE SENTENCE ANALYSIS")
    print("="*80)
    
    complex_sentence = ("ŸáŸÑ ÿ™ÿπÿ±ŸÅ Ÿäÿß ÿµÿØŸäŸÇŸä ÿßŸÑÿ∑ÿßŸÑÿ® ÿßŸÑÿ∞Ÿä ÿ•ŸÜ ŸÑŸÖ ŸäÿØÿ±ÿ≥ ŸÅŸÑŸÜ ŸäŸÜÿ¨ÿ≠ ÿ•ŸÑÿß ÿ•ÿ∞ÿß "
                       "ÿßÿ¨ÿ™ŸáÿØÿå ŸàŸáÿ∞ÿß ŸÖÿß ÿ£ÿÆÿ®ÿ±ÿ™ŸÉ ÿ•ŸäÿßŸá ÿ£ŸÜÿß ÿ£ŸÖÿ≥ ŸáŸÜÿßŸÉ ŸÅŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿ©ÿü")
    
    print(f"Complex Sentence:")
    print(f"{complex_sentence}")
    print("-" * 60)
    
    comprehensive_analysis = analyzer.classify_and_segregate_text(complex_sentence)
    
    print(f"üìä Overall Analysis:")
    print(f"   Total Words: {comprehensive_analysis['total_words']}")
    print(f"   Particles Found: {comprehensive_analysis['classification_summary']['particles_found']}")
    print(f"   Particle Density: {comprehensive_analysis['statistics']['particle_density']:.1f}%")
    print(f"   Categories Present: {len(comprehensive_analysis['classification_summary']['categories_present'])}")
    print(f"   Subcategories Present: {len(comprehensive_analysis['classification_summary']['subcategories_present'])}")
    
    print(f"\nüè∑Ô∏è Complete Category Breakdown:")
    for category, count in comprehensive_analysis['classification_summary']['category_counts'].items():
        particles = [p['word'] for p in comprehensive_analysis['segregation_by_category'][category]]
        category_name = analyzer.category_definitions[category]['name']
        print(f"   {category} ({category_name}) - {count} particles: {', '.join(particles)}")
    
    print(f"\nüîñ Complete Subcategory Breakdown:")
    for subcat, particles_data in comprehensive_analysis['segregation_by_subcategory'].items():
        particles = [p['word'] for p in particles_data]
        print(f"   {subcat}: {', '.join(particles)}")
    
    print(f"\nüìà Detailed Particle Analysis:")
    for particle_data in comprehensive_analysis['detailed_analysis']:
        word = particle_data['word']
        category = particle_data['category']
        subcategory = particle_data.get('subcategory', 'ÿ∫Ÿäÿ± ŸÖÿ≠ÿØÿØ')
        phonemes = len(particle_data['phonemes'])
        syllabic_units = len(particle_data['syllabic_units'])
        
        print(f"   '{word}' ‚Üí {category} ({subcategory}) | {phonemes} phonemes, {syllabic_units} syllabic_units")
    
    # System statistics
    print(f"\n{'='*80}")
    print("üìä COMPLETE SYSTEM STATISTICS")
    print("="*80)
    
    print(f"üéØ Analysis Summary:")
    print(f"   Total Texts Analyzed: {total_texts_analyzed}")
    print(f"   Total Particles Found: {total_particles_found}")
    print(f"   Average Particles per Text: {total_particles_found/total_texts_analyzed:.1f}")
    
    print(f"\nüè∑Ô∏è Category Distribution:")
    for category, stats in category_statistics.items():
        percentage = (stats['total_occurrences'] / total_particles_found * 100) if total_particles_found > 0 else 0
        print(f"   {category}: {stats['total_occurrences']} occurrences ({percentage:.1f}%) | {stats['unique_particles']} unique")
    
    print(f"\nüìã System Capabilities:")
    all_categories = analyzer.get_all_categories_with_examples()
    total_system_particles = sum(data['total_particles'] for data in all_categories.values())
    total_subcategories = sum(len(data['subcategories']) for data in all_categories.values())
    
    print(f"   ‚úÖ Categories Supported: {len(all_categories)}")
    print(f"   ‚úÖ Total Particles in Database: {total_system_particles}")
    print(f"   ‚úÖ Total Subcategories: {total_subcategories}")
    print(f"   ‚úÖ Morphological Analysis: Phonemes + SyllabicUnits")
    print(f"   ‚úÖ Statistical Metrics: Density, Distribution, Diversity")
    print(f"   ‚úÖ Segregation: By Category and Subcategory")
    print(f"   ‚úÖ Context-Sensitive Classification")
    print(f"   ‚úÖ Real-time Processing")
    
    print(f"\nüí° Classification Categories:")
    for category, data in all_categories.items():
        print(f"   üè∑Ô∏è {category} ({data['definition']['name']}):")
        print(f"      Function: {data['definition']['function']}")
        print(f"      Particles: {data['total_particles']} | Subcategories: {len(data['subcategories'])}")
    
    print(f"\n‚úÖ COMPLETE CLASSIFICATION & SEGREGATION SYSTEM DEMONSTRATION FINISHED!")
    print("üéØ All 8 Arabic grammatical particle categories successfully analyzed!")

if __name__ == "__main__":
    try:
        demonstrate_complete_system()
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import_data traceback
        traceback.print_exc()
