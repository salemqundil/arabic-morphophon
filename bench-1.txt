# tests/performance/test_benchmarks.py
"""
Performance benchmarks for the Python project
"""
import pytest
import time
import random
from typing import List


@pytest.fixture
def sample_data():
    """Generate sample data for benchmarking"""
    return [random.randint(1, 1000) for _ in range(10000)]


def test_list_comprehension_performance(benchmark, sample_data):
    """Benchmark list comprehension performance"""
    
    def list_comp_operation():
        return [x * 2 for x in sample_data if x % 2 == 0]
    
    result = benchmark(list_comp_operation)
    assert len(result) > 0


def test_filter_map_performance(benchmark, sample_data):
    """Benchmark filter/map performance"""
    
    def filter_map_operation():
        return list(map(lambda x: x * 2, filter(lambda x: x % 2 == 0, sample_data)))
    
    result = benchmark(filter_map_operation)
    assert len(result) > 0


def test_string_concatenation_performance(benchmark):
    """Benchmark string concatenation methods"""
    
    def string_join_operation():
        strings = [f"item_{i}" for i in range(1000)]
        return "".join(strings)
    
    result = benchmark(string_join_operation)
    assert len(result) > 0


@pytest.mark.parametrize("size", [100, 1000, 10000])
def test_data_processing_scalability(benchmark, size):
    """Test performance scalability with different data sizes"""
    
    def process_data():
        data = list(range(size))
        return sum(x ** 2 for x in data if x % 3 == 0)
    
    result = benchmark(process_data)
    assert isinstance(result, int)


class TestAPIPerformance:
    """Performance tests for API endpoints"""
    
    def test_response_time_simulation(self, benchmark):
        """Simulate API response time"""
        
        def simulate_api_call():
            # Simulate some processing time
            time.sleep(0.001)  # 1ms simulation
            return {"status": "success", "data": list(range(100))}
        
        result = benchmark(simulate_api_call)
        assert result["status"] == "success"