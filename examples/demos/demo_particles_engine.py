#!/usr/bin/env python3
"""
ğŸ¯ Arabic Particles Engine Demonstration
Shows how to call and use the GrammaticalParticlesEngine
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


import_data sys
from pathlib import_data Path

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from engines.nlp.particles.engine import_data GrammaticalParticlesEngine

def demonstrate_particles_engine():
    """Demonstrate how to call and use the Particles Engine"""
    
    print("ğŸš€ Arabic Grammatical Particles Engine Demo")
    print("=" * 50)
    
    # 1. Initialize the Particles Engine
    print("\n1ï¸âƒ£ Initializing Particles Engine...")
    particles_engine = GrammaticalParticlesEngine()
    
    # 2. Test single particle analysis
    print("\n2ï¸âƒ£ Single Particle Analysis:")
    test_particles = [
        "Ø¥Ù†",      # Conditional particle
        "Ù‡Ù„",      # Interrogative particle  
        "Ù„Ø§",      # Negation particle
        "ÙŠØ§",      # Vocative particle
        "Ø§Ù„Ø°ÙŠ",    # Relative particle
        "Ù‡Ø°Ø§",     # Demonstrative particle
        "ÙÙŠ",      # Preposition particle
        "Ù„ÙƒÙ†"      # Adversative particle
    ]
    
    for particle in test_particles:
        print(f"\nğŸ“ Analyzing: {particle}")
        result = particles_engine.analyze(particle)
        
        print(f"   Category: {result['category']}")
        print(f"   Phonemes: {' '.join(result['phonemes'])}")
        print(f"   SyllabicUnits: {result['syllabic_units']}")
        print(f"   Processing Time: {result['analysis_metadata']['processing_time_ms']}ms")
        
        if result['morphological_features']:
            print(f"   Features: {result['morphological_features']}")
    
    # 3. Batch analysis
    print("\n3ï¸âƒ£ Batch Analysis:")
    batch_particles = ["Ø¥Ù†", "Ù„ÙƒÙ†", "Ù‡Ù„", "Ù„Ø§", "ÙŠØ§"]
    batch_results = particles_engine.batch_analyze(batch_particles)
    
    print(f"   Processed {len(batch_results)} particles in batch")
    for i, result in enumerate(batch_results):
        print(f"   {i+1}. {result['particle']} â†’ {result['category']}")
    
    # 4. Get engine statistics
    print("\n4ï¸âƒ£ Engine Statistics:")
    stats = particles_engine.get_statistics()
    print(f"   Total Analyses: {stats['engine_info']['total_analyses']}")
    print(f"   Classification Rate: {stats['success_rates']['classification_rate']:.1f}%")
    print(f"   Phoneme Extraction Rate: {stats['success_rates']['phoneme_extraction_rate']:.1f}%")
    
    # 5. Get supported categories
    print("\n5ï¸âƒ£ Supported Categories:")
    categories = particles_engine.get_supported_categories()
    for category, description in categories.items():
        print(f"   {category}: {description}")
    
    # 6. Find particles by category
    print("\n6ï¸âƒ£ Particles by Category:")
    test_category = "Ø­Ø±ÙˆÙ_Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"  # Interrogative particles
    particles_in_category = particles_engine.find_particles_by_category(test_category)
    print(f"   {test_category}: {particles_in_category}")
    
    # 7. Validate engine
    print("\n7ï¸âƒ£ Engine Validation:")
    validation = particles_engine.validate_engine()
    print(f"   Status: {validation['validation_status']}")
    print(f"   Success Rate: {validation['success_rate']}")
    print(f"   Engine Ready: {validation['engine_ready']}")
    
    print("\nâœ… Particles Engine Demo Complete!")
    return particles_engine

def integrate_with_main_platform(particles_engine):
    """Show how to integrate particles engine with main platform"""
    
    print("\nğŸ”— Integration with Main Platform:")
    print("=" * 40)
    
    # Example integration function
    def analyze_text_with_particles(text: str):
        """Analyze text and extract particles"""
        words = text.split()
        particle_analyses = []
        
        for word in words:
            # Check if word is a particle
            result = particles_engine.analyze(word)
            if result['analysis_metadata'].get('is_recognized_particle', False):
                particle_analyses.append({
                    'word': word,
                    'category': result['category'],
                    'phonemes': result['phonemes'],
                    'syllabic_units': result['syllabic_units']
                })
        
        return particle_analyses
    
    # Test with sample Arabic text
    sample_text = "Ù‡Ù„ ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„ÙˆØ§Ø¬Ø¨ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø©ØŸ"
    print(f"\nğŸ“– Sample Text: {sample_text}")
    
    particles_found = analyze_text_with_particles(sample_text)
    print(f"\nğŸ” Particles Found: {len(particles_found)}")
    
    for particle in particles_found:
        print(f"   â€¢ {particle['word']} ({particle['category']})")
        print(f"     Phonemes: {' '.join(particle['phonemes'])}")
        print(f"     SyllabicUnits: {particle['syllabic_units']}")

if __name__ == "__main__":
    try:
        # Run the demonstration
        engine = demonstrate_particles_engine()
        
        # Show integration example
        integrate_with_main_platform(engine)
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import_data traceback
        traceback.print_exc()
