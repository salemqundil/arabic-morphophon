#!/usr/bin/env python3
"""
ğŸ¯ Test Comprehensive Arabic Particles Classification
Demonstrates complete classification and segregation capabilities
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


import_data sys
from pathlib import_data Path

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from comprehensive_particle_classification import_data ComprehensiveParticleAnalyzer

def test_comprehensive_classification():
    """Test comprehensive classification with various Arabic texts"""
    
    print("ğŸ¯ TESTING COMPREHENSIVE ARABIC PARTICLES CLASSIFICATION")
    print("=" * 70)
    
    # Initialize analyzer
    analyzer = ComprehensiveParticleAnalyzer()
    
    # Test cases covering all 8 categories
    test_cases = [
        {
            "name": "Ø§Ø³ØªÙÙ‡Ø§Ù… Ùˆ Ø´Ø±Ø· (Interrogative & Conditional)",
            "text": "Ù‡Ù„ Ø¬Ø§Ø¡ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¥Ù† ÙƒØ§Ù† Ù…Ø±ÙŠØ¶Ø§Ù‹ØŸ"
        },
        {
            "name": "Ù†Ø¯Ø§Ø¡ Ùˆ Ø¥Ø´Ø§Ø±Ø© Ùˆ Ù…ÙˆØµÙˆÙ„ (Vocative, Demonstrative & Relative)",
            "text": "ÙŠØ§ Ø£Ø­Ù…Ø¯ØŒ Ù‡Ø°Ø§ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø°ÙŠ Ø·Ù„Ø¨ØªÙ‡"
        },
        {
            "name": "Ù†ÙÙŠ Ùˆ Ø§Ø³ØªØ«Ù†Ø§Ø¡ (Negation & Exception)",
            "text": "Ù„Ø§ ØªØ°Ù‡Ø¨ Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡ÙŠØª Ù…Ù† Ø§Ù„Ø¹Ù…Ù„"
        },
        {
            "name": "Ø¶Ù…Ø§Ø¦Ø± (Personal Pronouns)",
            "text": "Ø£Ù†Ø§ Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ‡Ùˆ ÙŠØ­Ø¨ Ø°Ù„Ùƒ"
        },
        {
            "name": "Ø¬Ù…Ù„Ø© Ø´Ø§Ù…Ù„Ø© (Comprehensive Sentence)",
            "text": "Ù‡Ù„ ØªØ¹Ø±Ù Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø°ÙŠ ÙŠØ§ Ø£Ø®ÙŠ Ø¥Ù† Ù„Ù… ÙŠØ£Øª ÙÙ„Ù† Ù†Ø¨Ø¯Ø£ Ø¥Ù„Ø§ Ø¨Ø¹Ø¯ ÙˆØµÙˆÙ„Ù‡ØŸ"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ” TEST {i}: {test_case['name']}")
        print("-" * 60)
        print(f"Text: {test_case['text']}")
        
        # Perform comprehensive analysis
        analysis = analyzer.classify_and_segregate_text(test_case['text'])
        
        # Display results
        print(f"\nğŸ“Š Analysis Results:")
        print(f"   Total Words: {analysis['total_words']}")
        print(f"   Particles Found: {analysis['classification_summary']['particles_found']}")
        print(f"   Particle Density: {analysis['statistics']['particle_density']:.1f}%")
        print(f"   Categories Present: {len(analysis['classification_summary']['categories_present'])}")
        
        # Categories breakdown
        if analysis['classification_summary']['category_counts']:
            print(f"\nğŸ·ï¸ Categories Found:")
            for category, count in analysis['classification_summary']['category_counts'].items():
                particles = [p['word'] for p in analysis['segregation_by_category'][category]]
                print(f"   {category} ({count}): {', '.join(particles)}")
        
        # Subcategories breakdown
        if analysis['segregation_by_subcategory']:
            print(f"\nğŸ”– Subcategories:")
            for subcat, particles_data in analysis['segregation_by_subcategory'].items():
                particles = [p['word'] for p in particles_data]
                print(f"   {subcat}: {', '.join(particles)}")
        
        # Statistical insights
        if analysis['statistics']['most_common_category']:
            print(f"\nğŸ“ˆ Most Common Category: {analysis['statistics']['most_common_category']}")
    
    print("\n" + "="*70)
    print("ğŸ“‹ SYSTEM OVERVIEW")
    print("="*70)
    
    # Get system overview
    categories = analyzer.get_all_categories_with_examples()
    
    print(f"\nğŸ¯ Available Categories ({len(categories)}):")
    for category, data in categories.items():
        print(f"\nğŸ·ï¸ {category} ({data['definition']['name']}):")
        print(f"   Description: {data['definition']['description']}")
        print(f"   Function: {data['definition']['function']}")
        print(f"   Total Particles: {data['total_particles']}")
        print(f"   Examples: {', '.join(data['examples'])}")
        
        # Show subcategories
        print(f"   Subcategories ({len(data['subcategories'])}):")
        for subcat, particles in data['subcategories'].items():
            print(f"      â€¢ {subcat}: {len(particles)} particles")
    
    print(f"\nğŸ“Š System Statistics:")
    print(f"   Total Categories: {len(categories)}")
    print(f"   Total Particles: {sum(data['total_particles'] for data in categories.values())}")
    print(f"   Total Subcategories: {sum(len(data['subcategories']) for data in categories.values())}")
    
    print("\nâœ… COMPREHENSIVE CLASSIFICATION TEST COMPLETE!")
    print("\nğŸ’¡ System Capabilities:")
    print("   âœ“ 8 Main grammatical categories")
    print("   âœ“ 20+ Subcategory classifications")  
    print("   âœ“ Morphological feature analysis")
    print("   âœ“ Statistical insights & metrics")
    print("   âœ“ Complete text segregation")
    print("   âœ“ Context-sensitive classification")
    print("   âœ“ Real-time particle detection")
    print("   âœ“ Comprehensive linguistic metadata")

if __name__ == "__main__":
    try:
        test_comprehensive_classification()
    except Exception as e:
        print(f"âŒ Error: {e}")
        import_data traceback
        traceback.print_exc()
