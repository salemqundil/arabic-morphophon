#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” Arabic Word Tracer - Advanced Browser Interface
Ù…ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - ÙˆØ§Ø¬Ù‡Ø© Ù…ØªØµÙØ­ Ù…ØªÙ‚Ø¯Ù…Ø©

Features:
ğŸ¯ Complete linguistic tracing from phonemes to roots
ğŸ“Š Interactive visualizations and diagrams
ğŸš€ Real-time analysis with step-by-step breakdown
ğŸ¨ Professional UI with Arabic language support
ğŸ§  Expert NLP system integration
ğŸ“± Responsive design for all devices
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


import_data json
import_data logging
import_data os
import_data sys
import_data time
from datetime import_data datetime
from pathlib import_data Path
from typing import_data Any, Dict, List, Optional, Tuple

from flask import_data Flask, jsonify, render_template, request, send_from_directory
from flask_cors import_data CORS

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©
try:
    from engines.nlp.particles.engine import_data GrammaticalParticlesEngine
except ImportError:
    GrammaticalParticlesEngine = None

try:
    from engines.nlp.morphology.engine import_data MorphologyEngine
except ImportError:
    MorphologyEngine = None

try:
    from unified_phonemes from unified_phonemes import get_unified_phonemes, extract_phonemes, get_phonetic_features, is_emphatic
except ImportError:
    PhonologyEngine = None

try:
    from engines.nlp.frozen_root.engine import_data FrozenRootsEngine
except ImportError:
    FrozenRootsEngine = None

try:
    from arabic_morphophon.models.patterns import_data PatternRepository
except ImportError:
    PatternRepository = None

try:
    from arabic_morphophon.models.roots import_data ArabicRoot
except ImportError:
    ArabicRoot = None

try:
    from unified_phonemes from unified_phonemes import get_unified_phonemes, extract_phonemes, get_phonetic_features, is_emphatic
        AdvancedPhonologyEngine,
        SyllabicUnitEngine,
    )
except ImportError:
    SyllabicUnitEngine = None
    AdvancedPhonologyEngine = None

# Ù…Ø­Ø±ÙƒØ§Øª Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
class MockEngine:
    def __init__(self, name="Mock"): 
        self.name = name
    def analyze(self, text, **kwargs): 
        return {
            "analysis": f"ØªØ­Ù„ÙŠÙ„ {self.name} Ù„Ù„Ù†Øµ: {text}",
            "status": "mock",
            "text": text,
            "engine": self.name
        }

class MockPatternRepository:
    def __init__(self):
        self.patterns = []
    def find_matching_patterns(self, word):
        return []

# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)
app.config['SECRET_KEY'] = 'arabic_word_tracer_2024'
app.config['JSON_AS_ASCII'] = False
CORS(app)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ArabicWordTracer:
    """
    ğŸ” Ù…ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    ÙŠØªØªØ¨Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØªØ¨Ø¹ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©"""
        self.engines = {}
        self.pattern_repository = None
        self.performance_stats = {
            'total_traces': 0,
            'successful_traces': 0,
            'average_processing_time': 0.0,
            'last_reset': datetime.now()
        }
        self._initialize_engines()
    
    def _initialize_engines(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ©"""
        try:
            # Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¬Ø³ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©
            if GrammaticalParticlesEngine:
                self.engines['particles'] = GrammaticalParticlesEngine()
            else:
                self.engines['particles'] = MockEngine('particles')
            
            # Ù…Ø­Ø±Ùƒ Ø§Ù„ØµØ±Ù
            if MorphologyEngine:
                try:
                    self.engines['morphology'] = MorphologyEngine("morphology", {})
                except TypeError:
                    self.engines['morphology'] = MorphologyEngine()
            else:
                self.engines['morphology'] = MockEngine('morphology')
            
            # Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£ØµÙˆØ§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ø·Ø¹
            if PhonologyEngine:
                try:
                    self.engines['phonology'] = PhonologyEngine("phonology", {})
                except TypeError:
                    self.engines['phonology'] = PhonologyEngine()
            else:
                self.engines['phonology'] = MockEngine('phonology')
            
            if SyllabicUnitEngine:
                self.engines['syllabic_unit'] = SyllabicUnitEngine()
            else:
                self.engines['syllabic_unit'] = MockEngine('syllabic_unit')
                
            if AdvancedPhonologyEngine:
                self.engines['advanced_phonology'] = AdvancedPhonologyEngine()
            else:
                self.engines['advanced_phonology'] = MockEngine('advanced_phonology')
            
            # Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ø¬Ø§Ù…Ø¯Ø©
            if FrozenRootsEngine:
                try:
                    self.engines['frozen_root'] = FrozenRootsEngine("frozen_root", {})
                except TypeError:
                    self.engines['frozen_root'] = FrozenRootsEngine()
            else:
                self.engines['frozen_root'] = MockEngine('frozen_root')
            
            # Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
            if PatternRepository:
                self.pattern_repository = PatternRepository()
            else:
                self.pattern_repository = MockPatternRepository()
            
            logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª: {e}")
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±ÙƒØ§Øª Ù…Ø­Ø§ÙƒØ§Ø©
            for engine_name in ['particles', 'morphology', 'phonology', 'syllabic_unit', 'frozen_root', 'advanced_phonology']:
                self.engines[engine_name] = MockEngine(engine_name)
            self.pattern_repository = MockPatternRepository()
    
    def trace_word_complete(self, word: str) -> Dict[str, Any]:
        """
        ØªØªØ¨Ø¹ ÙƒØ§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©
        
        Args:
            word: Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØªØ¨Ø¹Ù‡Ø§
            
        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ ÙŠØ´Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©
        """
        begin_time = time.time()
        
        trace_result = {
            'input_word': word,
            'trace_timestamp': datetime.now().isoformat(),
            'trace_id': self._generate_trace_id(),
            'linguistic_levels': {},
            'trace_summary': {},
            'metadata': {}
        }
        
        try:
            # 1. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙˆØ§Øª (Phonemes)
            trace_result['linguistic_levels']['phonemes'] = self._trace_phonemes(word)
            
            # 2. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø­Ø±ÙƒØ§Øª (Harakat)
            trace_result['linguistic_levels']['harakat'] = self._trace_harakat(word)
            
            # 3. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ (SyllabicUnits)
            trace_result['linguistic_levels']['syllabic_units'] = self._trace_syllabic_units(word)
            
            # 4. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø³ÙŠÙ…Ø§Øª (Particles)
            trace_result['linguistic_levels']['particles'] = self._trace_particles(word)
            
            # 5. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø£ÙØ¹Ø§Ù„ (Nouns & Verbs)
            trace_result['linguistic_levels']['word_class'] = self._trace_word_class(word)
            
            # 6. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Patterns)
            trace_result['linguistic_levels']['patterns'] = self._trace_patterns(word)
            
            # 7. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ (Weight)
            trace_result['linguistic_levels']['weight'] = self._trace_morphological_weight(word)
            
            # 8. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø°Ø± (Root)
            trace_result['linguistic_levels']['root'] = self._trace_root(word)
            
            # 9. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ (Affixes)
            trace_result['linguistic_levels']['affixes'] = self._trace_affixes(word)
            
            # 10. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø¯ (Infinitive & Pure)
            trace_result['linguistic_levels']['infinitive_pure'] = self._trace_infinitive_pure(word)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„ØªØªØ¨Ø¹
            trace_result['trace_summary'] = self._generate_trace_summary(trace_result['linguistic_levels'])
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
            processing_time = time.time() - begin_time
            trace_result['metadata'] = {
                'processing_time_ms': round(processing_time * 1000, 2),
                'engines_used': list(self.engines.keys()),
                'analysis_depth': len(trace_result['linguistic_levels']),
                'status': 'success'
            }
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self._update_performance_stats(processing_time, success=True)
            
            return trace_result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø© {word}: {e}")
            trace_result['metadata'] = {
                'error': str(e),
                'status': 'error',
                'processing_time_ms': round((time.time() - begin_time) * 1000, 2)
            }
            self._update_performance_stats(time.time() - begin_time, success=False)
            return trace_result
    
    def _trace_phonemes(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙˆØ§Øª (Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª)"""
        try:
            if 'advanced_phonology' in self.engines:
                result = self.engines['advanced_phonology'].extract_phonemes(word)
                
                return {
                    'phonemes_list': result if isinstance(result, list) else [result],
                    'phoneme_count': len(result) if isinstance(result, list) else 1,
                    'phoneme_types': self._classify_phonemes(result if isinstance(result, list) else [result]),
                    'ipa_representation': self._to_ipa(word),
                    'status': 'success'
                }
            else:
                # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙŠÙ„
                return self._simple_phoneme_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_harakat(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø­Ø±ÙƒØ§Øª"""
        try:
            harakat = {
                'fatha': word.count('Ù'),
                'kasra': word.count('Ù'),
                'damma': word.count('Ù'),
                'sukun': word.count('Ù’'),
                'tanween_fath': word.count('Ù‹'),
                'tanween_kasr': word.count('Ù'),
                'tanween_damm': word.count('ÙŒ'),
                'shadda': word.count('Ù‘'),
                'madd': word.count('Ù“')
            }
            
            total_harakat = sum(harakat.values())
            clean_word = self._remove_harakat(word)
            
            return {
                'harakat_breakdown': harakat,
                'total_harakat': total_harakat,
                'harakat_density': round(total_harakat / len(clean_word) if clean_word else 0, 2),
                'diacritization_level': self._assess_diacritization_level(harakat),
                'clean_word': clean_word,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙƒØ§Øª: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_syllabic_units(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""
        try:
            if 'syllabic_unit' in self.engines:
                syllabic_units_data = self.engines['syllabic_unit'].syllabic_analyze_word(word)
                
                return {
                    'syllabic_units': [s.text for s in syllabic_units_data] if syllabic_units_data else [],
                    'syllabic_unit_count': len(syllabic_units_data) if syllabic_units_data else 0,
                    'syllabic_unit_patterns': [s.pattern for s in syllabic_units_data] if syllabic_units_data else [],
                    'syllabic_unit_types': [s.type.value for s in syllabic_units_data] if syllabic_units_data else [],
                    'cv_pattern': self._extract_cv_pattern(word),
                    'prosodic_weight': self._calculate_prosodic_weight(syllabic_units_data),
                    'status': 'success'
                }
            else:
                return self._simple_syllabic_unit_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_particles(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø³ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©"""
        try:
            if 'particles' in self.engines:
                result = self.engines['particles'].analyze(word)
                
                return {
                    'is_particle': result.get('particles_found', 0) > 0,
                    'particle_type': result.get('particles', [{}])[0].get('category', 'none') if result.get('particles') else 'none',
                    'particle_function': result.get('particles', [{}])[0].get('function', 'none') if result.get('particles') else 'none',
                    'particle_details': result.get('particles', []),
                    'categories_summary': result.get('categories_summary', {}),
                    'status': 'success'
                }
            else:
                return self._simple_particle_check(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø³ÙŠÙ…Ø§Øª: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_word_class(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø© (Ø§Ø³Ù…/ÙØ¹Ù„/Ø­Ø±Ù)"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø±Ùƒ Ø§Ù„ØµØ±Ù Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø©
            if 'morphology' in self.engines:
                result = self.engines['morphology'].analyze(word)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©
                word_class_info = {
                    'primary_class': self._determine_primary_class(result, word),
                    'sub_class': self._determine_sub_class(result, word),
                    'grammatical_features': self._extract_grammatical_features(result),
                    'confidence_score': self._calculate_classification_confidence(result),
                    'alternative_classes': self._get_alternative_classifications(result),
                    'status': 'success'
                }
                
                return word_class_info
            else:
                return self._simple_word_class_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_patterns(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©"""
        try:
            if self.pattern_repository:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
                matching_patterns = self._find_matching_patterns(word)
                
                return {
                    'matching_patterns': matching_patterns,
                    'pattern_count': len(matching_patterns),
                    'most_likely_pattern': matching_patterns[0] if matching_patterns else None,
                    'pattern_families': self._group_patterns_by_family(matching_patterns),
                    'derivation_potential': self._assess_derivation_potential(word),
                    'status': 'success'
                }
            else:
                return self._simple_pattern_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_morphological_weight(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ
            clean_word = self._remove_harakat(word)
            weight_analysis = {
                'morphological_weight': self._calculate_morphological_weight(clean_word),
                'letter_count': len(clean_word),
                'root_letters': self._count_root_letters(clean_word),
                'augmentation_letters': self._count_augmentation_letters(clean_word),
                'weight_category': self._categorize_weight(clean_word),
                'weight_distribution': self._analyze_weight_distribution(clean_word),
                'status': 'success'
            }
            
            return weight_analysis
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_root(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ø§Ù„Ø¬Ø°Ø±"""
        try:
            if 'morphology' in self.engines:
                result = self.engines['morphology'].analyze(word)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø°Ø±
                root_info = self._extract_root_info(result, word)
                
                return {
                    'identified_root': root_info.get('root', ''),
                    'root_type': root_info.get('type', 'unknown'),
                    'root_length': len(root_info.get('root', '')),
                    'root_radicals': list(root_info.get('root', '')),
                    'weak_letters': self._identify_weak_letters(root_info.get('root', '')),
                    'semantic_field': root_info.get('semantic_field', ''),
                    'derivation_family': self._get_derivation_family(root_info.get('root', '')),
                    'confidence': root_info.get('confidence', 0.0),
                    'status': 'success'
                }
            else:
                return self._simple_root_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø±: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_affixes(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚"""
        try:
            if 'morphology' in self.engines:
                result = self.engines['morphology'].analyze(word)
                
                affixes_info = self._extract_affixes_info(result, word)
                
                return {
                    'prefixes': affixes_info.get('prefixes', []),
                    'suffixes': affixes_info.get('suffixes', []),
                    'infixes': affixes_info.get('infixes', []),
                    'prefix_count': len(affixes_info.get('prefixes', [])),
                    'suffix_count': len(affixes_info.get('suffixes', [])),
                    'total_affixes': len(affixes_info.get('prefixes', [])) + len(affixes_info.get('suffixes', [])),
                    'affixation_pattern': self._determine_affixation_pattern(affixes_info),
                    'status': 'success'
                }
            else:
                return self._simple_affixes_analysis(word)
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ÙˆØ§ØµÙ‚: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _trace_infinitive_pure(self, word: str) -> Dict[str, Any]:
        """ØªØªØ¨Ø¹ Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø¯"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¬Ø±Ø¯
            infinitive_analysis = {
                'infinitive_form': self._derive_infinitive(word),
                'pure_form': self._extract_pure_form(word),
                'base_form': self._get_base_form(word),
                'derivational_level': self._assess_derivational_level(word),
                'morphological_complexity': self._calculate_morphological_complexity(word),
                'canonical_form': self._get_canonical_form(word),
                'status': 'success'
            }
            
            return infinitive_analysis
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø¯: {e}")
            return {'error': str(e), 'status': 'error'}
    
    def _generate_trace_summary(self, linguistic_levels: Dict) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØªØ¨Ø¹"""
        try:
            summary = {
                'word_complexity_score': 0.0,
                'linguistic_features': [],
                'dominant_characteristics': [],
                'analysis_confidence': 0.0,
                'recommendations': []
            }
            
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            complexity_factors = []
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
            if 'syllabic_units' in linguistic_levels and linguistic_levels['syllabic_units'].get('status') == 'success':
                syllabic_unit_count = linguistic_levels['syllabic_units'].get('syllabic_unit_count', 0)
                complexity_factors.append(min(syllabic_unit_count / 5.0, 1.0))
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø±ÙƒØ§Øª
            if 'harakat' in linguistic_levels and linguistic_levels['harakat'].get('status') == 'success':
                harakat_density = linguistic_levels['harakat'].get('harakat_density', 0)
                complexity_factors.append(min(harakat_density, 1.0))
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù† Ø§Ù„Ù„ÙˆØ§ØµÙ‚
            if 'affixes' in linguistic_levels and linguistic_levels['affixes'].get('status') == 'success':
                total_affixes = linguistic_levels['affixes'].get('total_affixes', 0)
                complexity_factors.append(min(total_affixes / 3.0, 1.0))
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·
            summary['word_complexity_score'] = sum(complexity_factors) / len(complexity_factors) if complexity_factors else 0.0
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
            if linguistic_levels.get('particles', {}).get('is_particle'):
                summary['dominant_characteristics'].append('Ø¬Ø³ÙŠÙ… Ù†Ø­ÙˆÙŠ')
            
            if linguistic_levels.get('root', {}).get('root_length', 0) == 3:
                summary['dominant_characteristics'].append('Ø¬Ø°Ø± Ø«Ù„Ø§Ø«ÙŠ')
            elif linguistic_levels.get('root', {}).get('root_length', 0) == 4:
                summary['dominant_characteristics'].append('Ø¬Ø°Ø± Ø±Ø¨Ø§Ø¹ÙŠ')
            
            # Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            confidence_scores = []
            for level_data in linguistic_levels.values():
                if isinstance(level_data, dict) and 'confidence' in level_data:
                    confidence_scores.append(level_data['confidence'])
            
            summary['analysis_confidence'] = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.8
            
            return summary
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„ØªØªØ¨Ø¹: {e}")
            return {'error': str(e), 'status': 'error'}
    
    # Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø³ÙŠØ· (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª)
    def _simple_phoneme_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØµÙˆØªÙŠ Ø¨Ø³ÙŠØ·"""
        clean_word = self._remove_harakat(word)
        return {
            'phonemes_list': list(clean_word),
            'phoneme_count': len(clean_word),
            'status': 'simple_analysis'
        }
    
    def _simple_syllabic_unit_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù‚Ø·Ø¹ÙŠ Ø¨Ø³ÙŠØ·"""
        # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ­Ø±ÙƒØ©
        vowels = 'Ø§ÙˆÙŠ'
        syllabic_units = []
        current_syllabic_unit = ''
        
        for char in word:
            current_syllabic_unit += char
            if char in vowels:
                syllabic_units.append(current_syllabic_unit)
                current_syllabic_unit = ''
        
        if current_syllabic_unit:
            syllabic_units.append(current_syllabic_unit)
        
        return {
            'syllabic_units': syllabic_units,
            'syllabic_unit_count': len(syllabic_units),
            'status': 'simple_analysis'
        }
    
    def _simple_particle_check(self, word: str) -> Dict[str, Any]:
        """ÙØ­Øµ Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ø³ÙŠÙ…Ø§Øª"""
        common_particles = ['ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ø¨Ø¹Ø¯', 'Ù‚Ø¨Ù„', 'Ù…Ø¹', 'Ø¶Ø¯', 'Ø­ÙˆÙ„']
        clean_word = self._remove_harakat(word)
        
        return {
            'is_particle': clean_word in common_particles,
            'particle_type': 'Ø­Ø±Ù Ø¬Ø±' if clean_word in common_particles else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
            'status': 'simple_analysis'
        }
    
    # Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
    def _remove_harakat(self, text: str) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        harakat = 'ÙÙÙÙ’Ù“ÙŒÙÙ‹Ù‘'
        return ''.join(char for char in text if char not in harakat)
    
    def _generate_trace_id(self) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„ØªØªØ¨Ø¹"""
        return f"trace_{int(time.time())}_{id(self) % 1000}"
    
    def _update_performance_stats(self, processing_time: float, success: bool):
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        self.performance_stats['total_traces'] += 1
        if success:
            self.performance_stats['successful_traces'] += 1
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        current_avg = self.performance_stats['average_processing_time']
        total_traces = self.performance_stats['total_traces']
        new_avg = ((current_avg * (total_traces - 1)) + processing_time) / total_traces
        self.performance_stats['average_processing_time'] = new_avg
    
    # Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© (placeholder implementations)
    def _classify_phonemes(self, phonemes: List[str]) -> Dict[str, int]:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª"""
        return {'consonants': 0, 'vowels': 0, 'semivowels': 0}
    
    def _to_ipa(self, word: str) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠØ© Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©"""
        return word  # placeholder
    
    def _assess_diacritization_level(self, harakat: Dict) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ´ÙƒÙŠÙ„"""
        total = sum(harakat.values())
        if total == 0: return 'ØºÙŠØ± Ù…Ø´ÙƒÙ„'
        elif total < 3: return 'ØªØ´ÙƒÙŠÙ„ Ø¬Ø²Ø¦ÙŠ'
        else: return 'Ù…Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„Ø§Ù‹'
    
    def _extract_cv_pattern(self, word: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø· CV"""
        # ØªØ·Ø¨ÙŠÙ‚ Ø¨Ø³ÙŠØ·
        return 'CVC'  # placeholder
    
    def _calculate_prosodic_weight(self, syllabic_units_data) -> str:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠ"""
        return 'Ù…ØªÙˆØ³Ø·'  # placeholder
    
    def _determine_primary_class(self, result, word: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©"""
        return 'Ø§Ø³Ù…'  # placeholder
    
    def _determine_sub_class(self, result, word: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙØ±Ø¹ÙŠ"""
        return 'Ø§Ø³Ù… Ù…ÙØ±Ø¯'  # placeholder
    
    def _extract_grammatical_features(self, result) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ø­ÙˆÙŠØ©"""
        return ['Ù…Ø°ÙƒØ±', 'Ù…ÙØ±Ø¯']  # placeholder
    
    def _calculate_classification_confidence(self, result) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ"""
        return 0.85  # placeholder
    
    def _get_alternative_classifications(self, result) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª Ø¨Ø¯ÙŠÙ„Ø©"""
        return []  # placeholder
    
    def _simple_word_class_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø©"""
        return {
            'primary_class': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯',
            'confidence_score': 0.5,
            'status': 'simple_analysis'
        }
    
    def _find_matching_patterns(self, word: str) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        return []  # placeholder
    
    def _group_patterns_by_family(self, patterns: List) -> Dict:
        """ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø§Ø¦Ù„Ø©"""
        return {}  # placeholder
    
    def _assess_derivation_potential(self, word: str) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚"""
        return 'Ù…ØªÙˆØ³Ø·'  # placeholder
    
    def _simple_pattern_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£ÙˆØ²Ø§Ù†"""
        return {
            'matching_patterns': [],
            'pattern_count': 0,
            'status': 'simple_analysis'
        }
    
    def _calculate_morphological_weight(self, word: str) -> str:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
        return f"{'Ù' * len(word)}"  # placeholder
    
    def _count_root_letters(self, word: str) -> int:
        """Ø¹Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø¬Ø°Ø±"""
        return min(len(word), 3)  # placeholder
    
    def _count_augmentation_letters(self, word: str) -> int:
        """Ø¹Ø¯ Ø£Ø­Ø±Ù Ø§Ù„Ø²ÙŠØ§Ø¯Ø©"""
        return max(0, len(word) - 3)  # placeholder
    
    def _categorize_weight(self, word: str) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ²Ù†"""
        length = len(word)
        if length <= 3: return 'Ù…Ø¬Ø±Ø¯'
        elif length <= 5: return 'Ù…Ø²ÙŠØ¯'
        else: return 'Ù…Ø²ÙŠØ¯ Ø¨ÙƒØ«Ø±Ø©'
    
    def _analyze_weight_distribution(self, word: str) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆØ²Ù†"""
        return {'root_ratio': 0.6, 'augmentation_ratio': 0.4}  # placeholder
    
    def _extract_root_info(self, result, word: str) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ø°Ø±"""
        return {'root': word[:3], 'type': 'Ø«Ù„Ø§Ø«ÙŠ', 'confidence': 0.7}  # placeholder
    
    def _identify_weak_letters(self, root: str) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ø¹ØªÙ„Ø©"""
        weak_letters = 'Ø§ÙˆÙŠ'
        return [char for char in root if char in weak_letters]
    
    def _get_derivation_family(self, root: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø§Ø¦Ù„Ø© Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚"""
        return []  # placeholder
    
    def _simple_root_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ø°Ø±"""
        return {
            'identified_root': word[:3] if len(word) >= 3 else word,
            'root_type': 'Ù…Ù‚Ø¯Ø±',
            'status': 'simple_analysis'
        }
    
    def _extract_affixes_info(self, result, word: str) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ÙˆØ§ØµÙ‚"""
        return {'prefixes': [], 'suffixes': []}  # placeholder
    
    def _determine_affixation_pattern(self, affixes_info: Dict) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„Ø¥Ù„ØµØ§Ù‚"""
        return 'Ø¨Ø³ÙŠØ·'  # placeholder
    
    def _simple_affixes_analysis(self, word: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„ÙˆØ§ØµÙ‚"""
        return {
            'prefixes': [],
            'suffixes': [],
            'total_affixes': 0,
            'status': 'simple_analysis'
        }
    
    def _derive_infinitive(self, word: str) -> str:
        """Ø§Ø´ØªÙ‚Ø§Ù‚ Ø§Ù„Ù…ØµØ¯Ø±"""
        return word  # placeholder
    
    def _extract_pure_form(self, word: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù…Ø¬Ø±Ø¯"""
        return self._remove_harakat(word)
    
    def _get_base_form(self, word: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        return word  # placeholder
    
    def _assess_derivational_level(self, word: str) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚"""
        return 'Ø£Ø³Ø§Ø³ÙŠ'  # placeholder
    
    def _calculate_morphological_complexity(self, word: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„ØµØ±ÙÙŠ"""
        return len(word) / 10.0  # placeholder
    
    def _get_canonical_form(self, word: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"""
        return word  # placeholder

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ù…ØªØªØ¨Ø¹
word_tracer = ArabicWordTracer()

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª
@app.route('/')
def index():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return render_template('word_tracer.html')

@app.route('/api/trace', methods=['POST'])
def trace_word():
    """API Ù„ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø©"""
    try:
        data = request.get_json()
        word = data.get('word', '').strip()
        
        if not word:
            return jsonify({'error': 'Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… ÙƒÙ„Ù…Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„'}), 400
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„ÙƒØ§Ù…Ù„
        result = word_tracer.trace_word_complete(word)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ API Ø§Ù„ØªØªØ¨Ø¹: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/stats')
def get_stats():
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    return jsonify(word_tracer.performance_stats)

@app.route('/api/engines')
def get_engines_status():
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª"""
    engines_status = {}
    for name, engine in word_tracer.engines.items():
        engines_status[name] = {
            'name': name,
            'status': 'active' if hasattr(engine, 'analyze') else 'inactive',
            'type': type(engine).__name__
        }
    
    return jsonify(engines_status)

@app.route('/static/<path:filename>')
def static_files(filename):
    """Ù…Ù„ÙØ§Øª Ø«Ø§Ø¨ØªØ©"""
    return send_from_directory('static', filename)

if __name__ == '__main__':
    print("ğŸ” Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...")
    print("ğŸŒ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù…ØªØ§Ø­Ø© Ø¹Ù„Ù‰: http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000)
