#!/usr/bin/env python3
"""
ğŸ”¥ Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø«Ø§Ù„Ø« Ù…Ù† Ù…Ù†ØµØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
====================================================
Ù†Ø¸Ø§Ù… Ø¥Ù†ØªØ§Ø¬ÙŠ Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Rule-Based Ùˆ Transformer
Arabic NLP Expert Engine v3.0.0
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


import_data asyncio
import_data os
import_data sys
import_data warnings
from pathlib import_data Path
from typing import_data Any, Dict, List

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
warnings.filterwarnings("ignore")
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import_data json
import_data time
from contextlib import_data asynccontextmanager
from datetime import_data datetime

import_data uvicorn

# FastAPI import_datas
from fastapi import_data FastAPI, HTTPException, Request
from fastapi.middleware.cors import_data CORSMiddleware
from fastapi.responses import_data JSONResponse
from pydantic import_data BaseModel

# Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
class TextInput(BaseModel):
    text: str
    analysis_level: str = "comprehensive"

class FeedbackInput(BaseModel):
    word: str
    correct_weight: str
    user_id: str = "anonymous"

class BatchTextInput(BaseModel):
    texts: List[str]
    analysis_level: str = "basic"

# Ù…Ø­Ø±Ùƒ Ù‡Ø¬ÙŠÙ† Ù…Ø¨Ø³Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
class TransformerHybridEngine:
    """ğŸ§  Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù‡Ø¬ÙŠÙ† - Rule-Based + Transformer"""
    
    def __init__(self, config_path=None):
        self.available_engines = [
            "phonology", "syllabic_unit", "morphology", "root_extraction",
            "verb_analysis", "pattern_analysis", "inflection", 
            "noun_plural", "diacritization", "weight_analysis",
            "particles", "derivation", "transformer_layer", "feedback_system"
        ]
        self.feedback_memory = []
        self.analysis_cache = {}
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.available_engines)} Ù…Ø­Ø±Ùƒ")
    
    async def analyze_text(self, text: str, level: str = "comprehensive"):
        """ğŸ” ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ"""
        begin_time = time.time()
        
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…
        analysis = {
            "input_text": text,
            "analysis_level": level,
            "timestamp": datetime.now().isoformat(),
            "processing_time_ms": 0,
            "engines_used": [],
            "results": {}
        }
        
        if level in ["basic", "comprehensive"]:
            # ØªØ­Ù„ÙŠÙ„ ØµÙˆØªÙŠ
            analysis["results"]["phonology"] = {
                "phonemes": self._extract_phonemes(text),
                "syllabic_units": self._extract_syllabic_units(text),
                "ipa_representation": self._to_ipa(text)
            }
            analysis["engines_used"].append("phonology")
        
        if level in ["intermediate", "comprehensive"]:
            # ØªØ­Ù„ÙŠÙ„ ØµØ±ÙÙŠ
            analysis["results"]["morphology"] = {
                "roots": self._extract_roots(text),
                "patterns": self._extract_patterns(text),
                "weights": self._extract_weights(text)
            }
            analysis["engines_used"].extend(["morphology", "weight_analysis"])
        
        if level == "comprehensive":
            # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…
            analysis["results"]["advanced"] = {
                "diacritized_text": self._diacritize(text),
                "verb_analysis": self._analyze_verbs(text),
                "noun_plural": self._analyze_nouns(text),
                "particles": self._extract_particles(text)
            }
            analysis["engines_used"].extend([
                "diacritization", "verb_analysis", "noun_plural", "particles"
            ])
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø°ÙƒÙŠØ©
        analysis["results"]["transformer_layer"] = {
            "confidence_scores": self._calculate_confidence(text),
            "context_analysis": self._analyze_context(text),
            "semantic_tags": self._extract_semantic_tags(text)
        }
        analysis["engines_used"].append("transformer_layer")
        
        processing_time = (time.time() - begin_time) * 1000
        analysis["processing_time_ms"] = round(processing_time, 2)
        
        return analysis
    
    def predict_diacritics(self, text: str):
        """ğŸ¯ ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ´ÙƒÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…
        words = text.split()
        diacritized_words = []
        
        for word in words:
            if word in ["ÙƒØªØ§Ø¨", "Ù…ÙƒØªØ¨Ø©", "Ø·Ø§Ù„Ø¨", "Ù…Ø¯Ø±Ø³Ø©"]:
                diacritized = {
                    "ÙƒØªØ§Ø¨": "ÙƒÙØªÙØ§Ø¨ÙŒ",
                    "Ù…ÙƒØªØ¨Ø©": "Ù…ÙÙƒÙ’ØªÙØ¨ÙØ©ÙŒ", 
                    "Ø·Ø§Ù„Ø¨": "Ø·ÙØ§Ù„ÙØ¨ÙŒ",
                    "Ù…Ø¯Ø±Ø³Ø©": "Ù…ÙØ¯Ù’Ø±ÙØ³ÙØ©ÙŒ"
                }.get(word, word)
            else:
                # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ´ÙƒÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ
                diacritized = self._add_basic_diacritics(word)
            
            diacritized_words.append(diacritized)
        
        return " ".join(diacritized_words)
    
    def predict_weight(self, word: str):
        """âš–ï¸ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
        weight_mapping = {
            "ÙƒØªØ§Ø¨": {"pattern": "ÙÙØ¹ÙØ§Ù„", "root": ["Ùƒ", "Øª", "Ø¨"], "type": "Ø§Ø³Ù…"},
            "ÙŠÙƒØªØ¨": {"pattern": "ÙŠÙÙÙ’Ø¹ÙÙ„", "root": ["Ùƒ", "Øª", "Ø¨"], "type": "ÙØ¹Ù„"},
            "Ù…ÙƒØªØ¨Ø©": {"pattern": "Ù…ÙÙÙ’Ø¹ÙÙ„ÙØ©", "root": ["Ùƒ", "Øª", "Ø¨"], "type": "Ø§Ø³Ù… Ù…ÙƒØ§Ù†"},
            "ÙƒØ§ØªØ¨": {"pattern": "ÙÙØ§Ø¹ÙÙ„", "root": ["Ùƒ", "Øª", "Ø¨"], "type": "Ø§Ø³Ù… ÙØ§Ø¹Ù„"}
        }
        
        if word in weight_mapping:
            return weight_mapping[word]
        else:
            # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ
            return {
                "pattern": "ÙÙØ¹ÙÙ„",
                "root": list(word[:3]) if len(word) >= 3 else list(word),
                "type": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "confidence": 0.7
            }
    
    # Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©
    def _extract_phonemes(self, text):
        return [{"char": c, "phoneme": c} for c in text if c.isalpha()]
    
    def _extract_syllabic_units(self, text):
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªÙ‚Ø³ÙŠÙ… Ù…Ù‚Ø§Ø·Ø¹
        return [text[i:i+2] for i in range(0, len(text), 2) if text[i:i+2].strip()]
    
    def _to_ipa(self, text):
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­ÙˆÙŠÙ„ IPA
        ipa_map = {"Ùƒ": "k", "Øª": "t", "Ø§": "a", "Ø¨": "b"}
        return "".join(ipa_map.get(c, c) for c in text)
    
    def _extract_roots(self, text):
        words = text.split()
        return [list(word[:3]) if len(word) >= 3 else list(word) for word in words]
    
    def _extract_patterns(self, text):
        return ["ÙØ¹Ù„" for _ in text.split()]
    
    def _extract_weights(self, text):
        return [self.predict_weight(word) for word in text.split()]
    
    def _diacritize(self, text):
        return self.predict_diacritics(text)
    
    def _analyze_verbs(self, text):
        return {"verb_count": len([w for w in text.split() if w.beginswith("ÙŠ")]),
                "tenses": ["Ù…Ø¶Ø§Ø±Ø¹"]}
    
    def _analyze_nouns(self, text):
        return {"noun_count": len(text.split()), "gender": "Ù…Ø°ÙƒØ±"}
    
    def _extract_particles(self, text):
        particles = ["ÙÙŠ", "Ø¹Ù„Ù‰", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ù…Ø¹"]
        found = [p for p in particles if p in text]
        return {"particles": found, "count": len(found)}
    
    def _calculate_confidence(self, text):
        return {"overall": 0.85, "diacritization": 0.90, "morphology": 0.80}
    
    def _analyze_context(self, text):
        return {"context_type": "formal", "domain": "general", "complexity": "medium"}
    
    def _extract_semantic_tags(self, text):
        return ["education", "arabic", "language"] if "Ù…Ø¯Ø±Ø³Ø©" in text else ["general"]
    
    def _add_basic_diacritics(self, word):
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ´ÙƒÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ
        return word + "ÙŒ"
    
    async def cleanup(self):
        """ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        print("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø­Ø±Ùƒ")

# Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
class PerformanceMonitor:
    def __init__(self):
        self.requests_count = 0
        self.total_time = 0.0
        self.error_count = 0
        
    async def record_request(self, endpoint, method, response_time, status_code):
        self.requests_count += 1
        self.total_time += response_time
        if status_code >= 400:
            self.error_count += 1

# Global instances
engine_instance = None
performance_monitor = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ğŸš€ Ø¯ÙˆØ±Ø© Ø­ÙŠØ§Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
    global engine_instance, performance_monitor
    
    print("ğŸ”¥ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ù…Ù†ØµØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ v3.0")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù‡Ø¬ÙŠÙ†
        engine_instance = TransformerHybridEngine()
        performance_monitor = PerformanceMonitor()
        
        app.state.engine = engine_instance
        app.state.monitor = performance_monitor
        
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø¨Ù†Ø¬Ø§Ø­")
        
        yield
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª: {e}")
        raise HTTPException(status_code=500, detail=f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
    
    finally:
        print("ğŸ”š Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªØ­Ø±ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯")
        if engine_instance:
            await engine_instance.cleanup()

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ FastAPI
app = FastAPI(
    title="ğŸ”¥ Arabic NLP Expert Engine v3.0",
    description="""
    Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø«Ø§Ù„Ø« Ù…Ù† Ù…Ù†ØµØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    
    Ù†Ø¸Ø§Ù… Ø¥Ù†ØªØ§Ø¬ÙŠ Ù‡Ø¬ÙŠÙ† Ù…ØªØ·ÙˆØ± ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ†:
    âœ… 14 Ù…Ø­Ø±Ùƒ Rule-Based Ù…ØªØ®ØµØµ
    âœ… Ø·Ø¨Ù‚Ø© Transformer Ø°ÙƒÙŠØ©  
    âœ… REST API Ù…ØªÙƒØ§Ù…Ù„Ø©
    âœ… ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© (Reinforcement Feedback)
    âœ… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    """,
    version="3.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Ø¥Ø¹Ø¯Ø§Ø¯ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
@app.get("/")
async def root():
    """ğŸ  Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return {
        "message": "ğŸ”¥ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ù†ØµØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ",
        "version": "3.0.0",
        "description": "Ù†Ø¸Ø§Ù… Ù‡Ø¬ÙŠÙ† Ù…ØªØ·ÙˆØ± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "features": [
            "14 Ù…Ø­Ø±Ùƒ Rule-Based Ù…ØªØ®ØµØµ",
            "Ø·Ø¨Ù‚Ø© Transformer Ø°ÙƒÙŠØ©",
            "ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
            "ØªØ´ÙƒÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©",
            "ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© ØªÙØ§Ø¹Ù„ÙŠØ©"
        ],
        "endpoints": {
            "analyze": "/api/analyze - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ",
            "diacritize": "/api/diacritize - ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ",
            "weight": "/api/weight - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†",
            "feedback": "/api/feedback - ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø©",
            "health": "/api/health - Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…",
            "stats": "/api/stats - Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"
        }
    }

@app.post("/api/analyze")
async def analyze_text(data: TextInput, request: Request):
    """ğŸ” ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    try:
        engine = request.app.state.engine
        result = await engine.analyze_text(data.text, data.analysis_level)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

@app.post("/api/diacritize")
async def diacritize_text(data: TextInput, request: Request):
    """ğŸ¯ ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    try:
        engine = request.app.state.engine
        diacritized = engine.predict_diacritics(data.text)
        return {
            "original_text": data.text,
            "diacritized_text": diacritized,
            "timestamp": datetime.now().isoformat(),
            "confidence": 0.88
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„: {str(e)}")

@app.post("/api/weight")
async def predict_weight(data: TextInput, request: Request):
    """âš–ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©"""
    try:
        engine = request.app.state.engine
        words = data.text.split()
        weights = []
        
        for word in words:
            weight_analysis = engine.predict_weight(word)
            weights.append({
                "word": word,
                "weight_analysis": weight_analysis,
                "hybrid_confidence": 0.85
            })
        
        return {
            "input_text": data.text,
            "weights": weights,
            "total_words": len(words),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†: {str(e)}")

@app.post("/api/feedback")
async def receive_feedback(data: FeedbackInput, request: Request):
    """ğŸ“ ØªÙ„Ù‚ÙŠ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©"""
    try:
        engine = request.app.state.engine
        
        # Ø­ÙØ¸ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        feedback_entry = {
            "word": data.word,
            "correct_weight": data.correct_weight,
            "user_id": data.user_id,
            "timestamp": datetime.now().isoformat()
        }
        
        engine.feedback_memory.append(feedback_entry)
        
        return {
            "status": "feedback recorded successfully",
            "feedback_id": len(engine.feedback_memory),
            "total_feedback": len(engine.feedback_memory),
            "message": "Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ! Ø³ØªØ³Ø§Ø¹Ø¯ Ù…Ù„Ø§Ø­Ø¸ØªÙƒ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©: {str(e)}")

@app.post("/api/batch")
async def batch_analyze(data: BatchTextInput, request: Request):
    """ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ"""
    try:
        engine = request.app.state.engine
        results = []
        
        for i, text in enumerate(data.texts):
            result = await engine.analyze_text(text, data.analysis_level)
            result["batch_index"] = i
            results.append(result)
        
        return {
            "batch_results": results,
            "total_texts": len(data.texts),
            "analysis_level": data.analysis_level,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {str(e)}")

@app.get("/api/health")
async def health_check():
    """ğŸ©º ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
    try:
        engine_status = "Ù…ØªØµÙ„" if engine_instance else "ØºÙŠØ± Ù…ØªØµÙ„"
        
        return {
            "status": "ØµØ­ÙŠ" if engine_instance else "Ø®Ø·Ø£",
            "timestamp": datetime.now().isoformat(),
            "engine_status": engine_status,
            "available_engines": len(engine_instance.available_engines) if engine_instance else 0,
            "feedback_count": len(engine_instance.feedback_memory) if engine_instance else 0,
            "version": "3.0.0",
            "uptime": "Ù…ØªØ§Ø­"
        }
    except Exception as e:
        return {
            "status": "Ø®Ø·Ø£",
            "error": str(e),
            "version": "3.0.0"
        }

@app.get("/api/stats")
async def get_statistics():
    """ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    try:
        monitor = performance_monitor
        avg_time = (monitor.total_time / monitor.requests_count) if monitor.requests_count > 0 else 0
        
        return {
            "total_requests": monitor.requests_count,
            "average_response_time": round(avg_time, 3),
            "error_count": monitor.error_count,
            "success_rate": round((monitor.requests_count - monitor.error_count) / max(monitor.requests_count, 1) * 100, 2),
            "feedback_received": len(engine_instance.feedback_memory) if engine_instance else 0,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}")

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
@app.exception_processr(HTTPException)
async def http_exception_processr(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "path": str(request.url.path),
            "method": request.method,
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… Arabic NLP Expert Engine v3.0")
    print("ğŸŒ Ø§Ù„Ø±Ø§Ø¨Ø·: http://localhost:5001")
    print("ğŸ“š Ø§Ù„ØªÙˆØ«ÙŠÙ‚: http://localhost:5001/docs")
    
    uvicorn.run(
        "arabic_nlp_v3_app:app",
        host="0.0.0.0",
        port=5001,
        reimport_data=True,
        log_level="info"
    )
