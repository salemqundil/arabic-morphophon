#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸŒ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
Advanced Web Interface for Enhanced Arabic Digital Vector System

ğŸ¯ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
- ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… ØªÙØ§Ø¹Ù„ÙŠØ© Ø­Ø¯ÙŠØ«Ø©
- ØªØ­Ù„ÙŠÙ„ ÙÙˆØ±ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
- Ø¹Ø±Ø¶ Ø¨ØµØ±ÙŠ Ù„Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
- Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
- ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø©
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too-long


from flask import Flask, render_template, request, jsonify, send_file
from flask_cors import CORS
import asyncio
import json
import io
import csv
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any
import base64
import numpy as np

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
from enhanced_system_architecture import EnhancedArabicVectorSystem

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
app = Flask(__name__)
CORS(app)

# Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ø¹Ø§Ù…
enhanced_system = EnhancedArabicVectorSystem()

# ============== Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==============


@app.route("/")
def index():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return render_template("index.html")


@app.route("/dashboard")
def dashboard():
    """Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
    return render_template("dashboard.html")


@app.route("/analysis")
def analysis():
    """ØµÙØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ"""
    return render_template("analysis.html")


@app.route("/batch")
def batch():
    """ØµÙØ­Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…Ø¹"""
    return render_template("batch.html")


# ============== API Endpoints ==============


@app.route("/api/analyze", methods=["POST"])
def api_analyze():
    """ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©"""
    try:
        data = request.json
        word = data.get("word", "").strip()
        optimization_level = data.get("optimization_level", "balanced")

        if not word:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… ÙƒÙ„Ù…Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„"}), 400

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                enhanced_system.analyze_word_enhanced(word, optimization_level)
            )

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ JSON
            response = {
                "word": word,
                "success": True,
                "digital_vector": result.digital_vector,
                "vector_dimensions": len(result.digital_vector),
                "quality_indicators": result.quality_indicators,
                "performance_metrics": {
                    "analysis_speed": result.performance_metrics.analysis_speed,
                    "accuracy_score": result.performance_metrics.accuracy_score,
                    "cache_hit_rate": result.performance_metrics.cache_hit_rate,
                    "parallel_efficiency": result.performance_metrics.parallel_efficiency,
                },
                "feature_importance": result.feature_importance,
                "linguistic_breakdown": result.linguistic_breakdown,
                "timestamp": datetime.now().isoformat(),
            }

            return jsonify(response)

        finally:
            loop.close()

    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}", "success": False}), 500


@app.route("/api/batch_analyze", methods=["POST"])
def api_batch_analyze():
    """ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    try:
        data = request.json
        words = data.get("words", [])
        optimization_level = data.get("optimization_level", "balanced")

        if not words or not isinstance(words, list):
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ù‚Ø§Ø¦Ù…Ø© ØµØ­ÙŠØ­Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª"}), 400

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        clean_words = [word.strip() for word in words if word.strip()]

        if not clean_words:
            return jsonify({"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„"}), 400

        # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…Ø¹
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            results = loop.run_until_complete(
                enhanced_system.batch_analyze(clean_words, optimization_level)
            )

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ JSON
            batch_response = {
                "total_words": len(clean_words),
                "successful_analyses": len(results),
                "success_rate": len(results) / len(clean_words) if clean_words else 0,
                "results": [],
                "summary": {
                    "average_accuracy": 0.0,
                    "average_vector_dimensions": 0,
                    "total_processing_time": 0.0,
                },
                "timestamp": datetime.now().isoformat(),
            }

            total_accuracy = 0.0
            total_dimensions = 0
            total_speed = 0.0

            for i, result in enumerate(results):
                word_result = {
                    "word": clean_words[i] if i < len(clean_words) else f"word_{i}",
                    "digital_vector": result.digital_vector,
                    "vector_dimensions": len(result.digital_vector),
                    "quality_indicators": result.quality_indicators,
                    "performance_metrics": {
                        "analysis_speed": result.performance_metrics.analysis_speed,
                        "accuracy_score": result.performance_metrics.accuracy_score,
                    },
                }

                batch_response["results"].append(word_result)

                # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                total_accuracy += result.quality_indicators.get("accuracy", 0.0)
                total_dimensions += len(result.digital_vector)
                total_speed += result.performance_metrics.analysis_speed

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
            if results:
                batch_response["summary"]["average_accuracy"] = total_accuracy / len(
                    results
                )
                batch_response["summary"]["average_vector_dimensions"] = (
                    total_dimensions // len(results)
                )
                batch_response["summary"]["average_processing_speed"] = (
                    total_speed / len(results)
                )

            return jsonify(batch_response)

        finally:
            loop.close()

    except Exception as e:
        return (
            jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…Ø¹: {str(e)}", "success": False}),
            500,
        )


@app.route("/api/statistics")
def api_statistics():
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
    try:
        stats = enhanced_system.get_system_statistics()
        return jsonify(
            {
                "success": True,
                "statistics": stats,
                "timestamp": datetime.now().isoformat(),
            }
        )
    except Exception as e:
        return (
            jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}", "success": False}),
            500,
        )


@app.route("/api/export/<format>")
def api_export(format):
    """ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    try:
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results_data = enhanced_system.results_database

        if not results_data:
            return jsonify({"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØµØ¯ÙŠØ±"}), 400

        if format.lower() == "csv":
            return export_to_csv(results_data)
        elif format.lower() == "json":
            return export_to_json(results_data)
        elif format.lower() == "excel":
            return export_to_excel(results_data)
        else:
            return jsonify({"error": "ØµÙŠØºØ© Ø§Ù„ØªØµØ¯ÙŠØ± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©"}), 400

    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµØ¯ÙŠØ±: {str(e)}", "success": False}), 500


@app.route("/api/visualization/vector/<word>")
def api_visualization_vector(word):
    """ØªØµÙˆØ± Ø§Ù„Ù…ØªØ¬Ù‡ Ù„ÙƒÙ„Ù…Ø© Ù…Ø¹ÙŠÙ†Ø©"""
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        word_result = None
        for entry in enhanced_system.results_database:
            if entry["word"] == word:
                word_result = entry["result"]
                break

        if not word_result:
            return jsonify({"error": "Ø§Ù„ÙƒÙ„Ù…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"}), 404

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Ø±Ø³Ù… Ø§Ù„Ù…ØªØ¬Ù‡
        vector = word_result.digital_vector
        ax1.plot(vector, marker="o", markersize=2)
        ax1.set_title(f"Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}", fontsize=14)
        ax1.set_xlabel("Ù…Ø¤Ø´Ø± Ø§Ù„Ù…ÙŠØ²Ø©")
        ax1.set_ylabel("Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ÙŠØ²Ø©")
        ax1.grid(True, alpha=0.3)

        # Ø±Ø³Ù… Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if word_result.feature_importance:
            features = list(word_result.feature_importance.keys())
            values = list(word_result.feature_importance.values())

            ax2.bar(features, values)
            ax2.set_title("Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©")
            ax2.set_ylabel("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©")
            ax2.tick_params(axis="x", rotation=45)

        plt.tight_layout()

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ base64
        img_buffer = io.BytesIO()
        plt.store_datafig(img_buffer, format="png", dpi=150, bbox_inches="tight")
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()

        return jsonify(
            {
                "success": True,
                "image": f"data:image/png;base64,{img_base64}",
                "word": word,
                "vector_info": {
                    "dimensions": len(vector),
                    "min_value": float(min(vector)),
                    "max_value": float(max(vector)),
                    "mean_value": float(np.mean(vector)),
                    "std_value": float(np.std(vector)),
                },
            }
        )

    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµÙˆØ±: {str(e)}", "success": False}), 500


@app.route("/api/comparison")
def api_comparison():
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø¹Ø¯Ø© ÙƒÙ„Ù…Ø§Øª"""
    try:
        words = request.args.getlist("words")

        if len(words) < 2:
            return jsonify({"error": "ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… ÙƒÙ„Ù…ØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"}), 400

        # Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words_data = []
        for word in words:
            for entry in enhanced_system.results_database:
                if entry["word"] == word:
                    words_data.append(
                        {
                            "word": word,
                            "vector": entry["result"].digital_vector,
                            "accuracy": entry["result"].quality_indicators.get(
                                "accuracy", 0.0
                            ),
                            "dimensions": len(entry["result"].digital_vector),
                        }
                    )
                    break

        if len(words_data) < 2:
            return jsonify({"error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"}), 400

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø§Øª
        comparison_matrix = []
        for i, data1 in enumerate(words_data):
            row = []
            for j, data2 in enumerate(words_data):
                if i == j:
                    similarity = 1.0
                else:
                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙˆØ³Ø§ÙŠÙ†ÙŠØ©
                    vec1 = np.array(data1["vector"])
                    vec2 = np.array(data2["vector"])

                    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø·ÙˆØ§Ù„
                    min_len = min(len(vec1), len(vec2))
                    vec1 = vec1[:min_len]
                    vec2 = vec2[:min_len]

                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    dot_product = np.dot(vec1, vec2)
                    magnitude1 = np.linalg.norm(vec1)
                    magnitude2 = np.linalg.norm(vec2)

                    if magnitude1 > 0 and magnitude2 > 0:
                        similarity = float(dot_product / (magnitude1 * magnitude2))
                    else:
                        similarity = 0.0

                row.append(similarity)
            comparison_matrix.append(row)

        return jsonify(
            {
                "success": True,
                "words": [data["word"] for data in words_data],
                "comparison_matrix": comparison_matrix,
                "word_details": words_data,
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: {str(e)}", "success": False}), 500


# ============== Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµØ¯ÙŠØ± ==============


def export_to_csv(results_data):
    """ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ CSV"""
    output = io.StringIO()
    writer = csv.writer(output)

    # Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    headers = [
        "Ø§Ù„ÙƒÙ„Ù…Ø©",
        "Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡",
        "Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ù‚Ø©",
        "Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„",
        "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©",
        "Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ",
    ]
    writer.writerow(headers)

    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    for entry in results_data:
        result = entry["result"]
        row = [
            entry["word"],
            len(result.digital_vector),
            result.quality_indicators.get("accuracy", 0.0),
            result.performance_metrics.analysis_speed,
            result.quality_indicators.get("reliability", 0.0),
            entry["timestamp"],
        ]
        writer.writerow(row)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    output.seek(0)
    return send_file(
        io.BytesIO(output.getvalue().encode()),
        mimetype="text/csv",
        as_attachment=True,
        downimport_data_name=f'arabic_vector_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
    )


def export_to_json(results_data):
    """ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ JSON"""
    export_data = {
        "export_info": {
            "total_entries": len(results_data),
            "export_timestamp": datetime.now().isoformat(),
            "system_version": "Enhanced Arabic Vector System v1.0",
        },
        "results": [],
    }

    for entry in results_data:
        result = entry["result"]
        export_data["results"].append(
            {
                "word": entry["word"],
                "timestamp": entry["timestamp"],
                "digital_vector": result.digital_vector,
                "vector_dimensions": len(result.digital_vector),
                "quality_indicators": result.quality_indicators,
                "performance_metrics": {
                    "analysis_speed": result.performance_metrics.analysis_speed,
                    "accuracy_score": result.performance_metrics.accuracy_score,
                    "cache_hit_rate": result.performance_metrics.cache_hit_rate,
                },
                "feature_importance": result.feature_importance,
            }
        )

    json_data = json.dumps(export_data, ensure_ascii=False, indent=2)

    return send_file(
        io.BytesIO(json_data.encode("utf-8")),
        mimetype="application/json",
        as_attachment=True,
        downimport_data_name=f'arabic_vector_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json',
    )


def export_to_excel(results_data):
    """ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ Excel"""
    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
    data_for_df = []

    for entry in results_data:
        result = entry["result"]
        data_for_df.append(
            {
                "Ø§Ù„ÙƒÙ„Ù…Ø©": entry["word"],
                "Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡": len(result.digital_vector),
                "Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ù‚Ø©": result.quality_indicators.get("accuracy", 0.0),
                "Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø§ØªØ³Ø§Ù‚": result.quality_indicators.get("consistency", 0.0),
                "Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©": result.quality_indicators.get("reliability", 0.0),
                "Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„": result.performance_metrics.analysis_speed,
                "ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©": result.performance_metrics.cache_hit_rate,
                "Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø²Ù…Ù†ÙŠ": entry["timestamp"],
            }
        )

    df = pd.DataFrame(data_for_df)

    # Ø­ÙØ¸ ÙÙŠ memory
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", index=False)

    output.seek(0)

    return send_file(
        output,
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        as_attachment=True,
        downimport_data_name=f'arabic_vector_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx',
    )


# ============== ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ==============

if __name__ == "__main__":
    print("ğŸŒ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†")
    print("=" * 60)
    print("ğŸ“ Ø§Ù„Ø±Ø§Ø¨Ø·: http://localhost:5000")
    print("ğŸ¯ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    print("   - ØªØ­Ù„ÙŠÙ„ ÙÙˆØ±ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("   - ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…Ø¹ Ù„Ù„ÙƒÙ„Ù…Ø§Øª")
    print("   - Ø¹Ø±Ø¶ Ø¨ØµØ±ÙŠ Ù„Ù„Ù…ØªØ¬Ù‡Ø§Øª")
    print("   - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡")
    print("   - ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    print("   - Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª")

    app.run(host="0.0.0.0", port=5000, debug=True)
