#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Phoneme to Word Decision Tree Generator
==============================================
Ù†Ø¸Ø§Ù… Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©,
    Decision tree logic flow visualization for Arabic text processing:
Phonemes â†’ Diacritics â†’ Syllables â†’ Final Words,
    Author: GitHub Copilot Arabic NLP Expert,
    Version: 1.0.0 - DECISION TREE LOGIC,
    Date: 2025-07-24,
    Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc
import json  # noqa: F401
import yaml  # noqa: F401
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict  # noqa: F401
from enum import Enum  # noqa: F401
import logging  # noqa: F401

# Configure logging,
    logging.basicConfig(
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DECISION TREE NODE TYPES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class NodeType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø¹Ù‚Ø¯ Ø§Ù„Ø´Ø¬Ø±Ø©"""

    ROOT = "root"
    PHONEME_LAYER = "phoneme_layer"
    DIACRITIC_LAYER = "diacritic_layer"
    SYLLABLE_LAYER = "syllable_layer"
    MORPHEME_LAYER = "morpheme_layer"
    WORD_LAYER = "word_layer"
    FINAL_OUTPUT = "final_output"


class DecisionCriteria(Enum):
    """Ù…Ø¹Ø§ÙŠÙŠØ± Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±"""

    PHONETIC_COMPATIBILITY = "phonetic_compatibility"
    MORPHOLOGICAL_VALIDITY = "morphological_validity"
    SYLLABLE_STRUCTURE = "syllable_structure"
    DIACRITIC_RULES = "diacritic_rules"
    PHONOTACTIC_CONSTRAINTS = "phonotactic_constraints"
    SEMANTIC_PLAUSIBILITY = "semantic_plausibility"


@dataclass,
    class DecisionNode:
    """Ø¹Ù‚Ø¯Ø© ÙÙŠ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª"""

    node_id: str,
    node_type: NodeType,
    input_data: List[str]
    decision_criteria: List[DecisionCriteria]
    processing_rules: Dict[str, Any]
    output_data: List[str]
    confidence_score: float,
    next_nodes: List[str]
    error_handling: Dict[str, str]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC PHONEME TO WORD DECISION TREE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPhonemeWordDecisionTree:
    """Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    def __init__(self):  # type: ignore[no-untyped def]
        """TODO: Add docstring."""
        self.decision_tree = {}
        self.processing_pipeline = []
        self.linguistic_rules = {}

        # Initialize decision tree structure,
    self._initialize_tree_structure()
        self._load_linguistic_rules()
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")

    def _initialize_tree_structure(self):  # type: ignore[no-untyped def]
        """ØªÙ‡ÙŠØ¦Ø© Ù‡ÙŠÙƒÙ„ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª"""

        # ROOT NODE - Input Processing,
    self.decision_tree["root"] = DecisionNode(
            node_id="root",
            node_type=NodeType.ROOT,
            input_data=["raw_arabic_text", "user_input"],
            decision_criteria=[DecisionCriteria.PHONETIC_COMPATIBILITY],
            processing_rules={
                "text_normalization": True,
                "character_validation": True,
                "encoding_check": "utf 8",
                "preprocessing_steps": [
                    "remove_extra_spaces",
                    "normalize_arabic_characters",
                    "validate_arabic_script",
                ],
            },
            output_data=["normalized_text", "character_stream"],
            confidence_score=1.0,
            next_nodes=["phoneme_analysis"],
            error_handling={
                "invalid_characters": "filter_and_log",
                "encoding_errors": "convert_to_utf8",
                "empty_input": "return_error",
            },
        )

        # LAYER 1: PHONEME ANALYSIS,
    self.decision_tree["phoneme_analysis"] = DecisionNode(
            node_id="phoneme_analysis",
            node_type=NodeType.PHONEME_LAYER,
            input_data=["character_stream"],
            decision_criteria=[
                DecisionCriteria.PHONETIC_COMPATIBILITY,
                DecisionCriteria.MORPHOLOGICAL_VALIDITY,
            ],
            processing_rules={
                "phoneme_classification": {
                    "consonants": [
                        "Ø¨",
                        "Øª",
                        "Ø«",
                        "Ø¬",
                        "Ø­",
                        "Ø®",
                        "Ø¯",
                        "Ø°",
                        "Ø±",
                        "Ø²",
                        "Ø³",
                        "Ø´",
                        "Øµ",
                        "Ø¶",
                        "Ø·",
                        "Ø¸",
                        "Ø¹",
                        "Øº",
                        "Ù",
                        "Ù‚",
                        "Ùƒ",
                        "Ù„",
                        "Ù…",
                        "Ù†",
                        "Ù‡",
                        "Ùˆ",
                        "ÙŠ",
                        "Ø¡",
                    ],
                    "vowels": ["Ù", "Ù", "Ù", "Ø§", "Ùˆ", "ÙŠ"],
                    "diacritics": ["Ù‹", "ÙŒ", "Ù", "Ù’", "Ù‘"],
                    "special": ["Ø¢", "Ø£", "Ø¥", "Ø¦", "Ø¤", "Ø©"],
                },
                "phonetic_features": {
                    "manner_of_articulation": ["stop", "fricative", "nasal", "liquid"],
                    "place_of_articulation": [
                        "bilabial",
                        "dental",
                        "velar",
                        "pharyngeal",
                    ],
                    "voicing": ["voiced", "voiceless"],
                    "emphasis": ["emphatic", "non_emphatic"],
                },
                "validation_rules": [
                    "check_arabic_phoneme_inventory",
                    "identify_phoneme_boundaries",
                    "extract_phonetic_features",
                ],
            },
            output_data=["phoneme_sequence", "phonetic_features", "phoneme_boundaries"],
            confidence_score=0.95,
            next_nodes=["diacritic_processing"],
            error_handling={
                "unknown_phoneme": "mark_as_foreign",
                "malformed_sequence": "attempt_correction",
                "feature_extraction_error": "use_default_features",
            },
        )

        # LAYER 2: DIACRITIC PROCESSING,
    self.decision_tree["diacritic_processing"] = DecisionNode(
            node_id="diacritic_processing",
            node_type=NodeType.DIACRITIC_LAYER,
            input_data=["phoneme_sequence", "phonetic_features"],
            decision_criteria=[
                DecisionCriteria.DIACRITIC_RULES,
                DecisionCriteria.MORPHOLOGICAL_VALIDITY,
            ],
            processing_rules={
                "diacritic_analysis": {
                    "short_vowels": {
                        "Ù": {"name": "fatha", "length": "short", "quality": "open"},
                        "Ù": {"name": "kasra", "length": "short", "quality": "front"},
                        "Ù": {"name": "damma", "length": "short", "quality": "back"},
                    },
                    "tanween": {
                        "Ù‹": {"type": "nunation_fath", "case": "accusative"},
                        "ÙŒ": {"type": "nunation_damm", "case": "nominative"},
                        "Ù": {"type": "nunation_kasr", "case": "genitive"},
                    },
                    "other_marks": {
                        "Ù’": {"type": "sukun", "function": "consonant_cluster"},
                        "Ù‘": {"type": "shadda", "function": "gemination"},
                    },
                },
                "vowelization_rules": [
                    "identify_explicit_vowels",
                    "predict_missing_vowels",
                    "apply_vowel_harmony",
                    "resolve_ambiguous_cases",
                ],
                "morphological_constraints": {
                    "root_pattern_matching": True,
                    "inflectional_analysis": True,
                    "derivational_analysis": True,
                },
            },
            output_data=[
                "vowelized_sequence",
                "morphological_analysis",
                "diacritic_features",
            ],
            confidence_score=0.88,
            next_nodes=["syllable_construction"],
            error_handling={
                "missing_vowels": "use_statistical_prediction",
                "conflicting_diacritics": "apply_priority_rules",
                "invalid_combinations": "suggest_corrections",
            },
        )

        # LAYER 3: SYLLABLE CONSTRUCTION,
    self.decision_tree["syllable_construction"] = DecisionNode(
            node_id="syllable_construction",
            node_type=NodeType.SYLLABLE_LAYER,
            input_data=["vowelized_sequence", "morphological_analysis"],
            decision_criteria=[
                DecisionCriteria.SYLLABLE_STRUCTURE,
                DecisionCriteria.PHONOTACTIC_CONSTRAINTS,
            ],
            processing_rules={
                "syllable_patterns": {
                    "CV": {"structure": "consonant + short_vowel", "weight": "light"},
                    "CVC": {
                        "structure": "consonant + short_vowel + consonant",
                        "weight": "heavy",
                    },
                    "CVV": {"structure": "consonant + long_vowel", "weight": "heavy"},
                    "CVVC": {
                        "structure": "consonant + long_vowel + consonant",
                        "weight": "super_heavy",
                    },
                    "CVCC": {
                        "structure": "consonant + short_vowel + consonant + consonant",
                        "weight": "super_heavy",
                    },
                },
                "syllabification_algorithm": [
                    "identify_syllable_nuclei",
                    "assign_consonants_to_syllables",
                    "apply_maximal_onset_principle",
                    "handle_consonant_clusters",
                    "resolve_ambiguous_boundaries",
                ],
                "phonotactic_constraints": {
                    "onset_constraints": [
                        "single_consonant_allowed",
                        "some_clusters_allowed",
                        "avoid_similar_place_clusters",
                    ],
                    "coda_constraints": [
                        "single_consonant_preferred",
                        "limited_clusters",
                        "word_final_exceptions",
                    ],
                    "nucleus_constraints": [
                        "exactly_one_vowel",
                        "long_vowel_exceptions",
                        "diphthong_handling",
                    ],
                },
            },
            output_data=["syllable_sequence", "prosodic_structure", "stress_pattern"],
            confidence_score=0.85,
            next_nodes=["morpheme_analysis"],
            error_handling={
                "invalid_syllable_structure": "apply_repair_strategies",
                "constraint_violations": "find_minimal_violations",
                "ambiguous_boundaries": "use_frequency_based_decisions",
            },
        )

        # LAYER 4: MORPHEME ANALYSIS,
    self.decision_tree["morpheme_analysis"] = DecisionNode(
            node_id="morpheme_analysis",
            node_type=NodeType.MORPHEME_LAYER,
            input_data=["syllable_sequence", "prosodic_structure"],
            decision_criteria=[
                DecisionCriteria.MORPHOLOGICAL_VALIDITY,
                DecisionCriteria.SEMANTIC_PLAUSIBILITY,
            ],
            processing_rules={
                "morpheme_identification": {
                    "root_extraction": {
                        "trilateral_roots": True,
                        "quadrilateral_roots": True,
                        "pattern_matching": ["ÙØ¹Ù„", "ÙØ§Ø¹Ù„", "Ù…ÙØ¹ÙˆÙ„", "ØªÙØ¹ÙŠÙ„"],
                    },
                    "affix_analysis": {
                        "prefixes": ["Ø§Ù„", "Ùˆ", "Ù", "Ø¨", "Ù„", "Ù…Ù†", "Ø¥Ù„Ù‰"],
                        "suffixes": ["Ø©", "Ø§Ù†", "ÙŠÙ†", "ÙˆÙ†", "Ù‡Ø§", "Ù‡Ù…", "Ù‡Ù†"],
                        "infixes": ["Øª", "Ù†", "Ø§"],
                    },
                    "stem_formation": [
                        "combine_root_and_pattern",
                        "apply_morphophonemic_rules",
                        "handle_weak_letters",
                        "process_gemination",
                    ],
                },
                "morphological_features": {
                    "word_class": ["noun", "verb", "adjective", "particle"],
                    "inflectional_features": {
                        "gender": ["masculine", "feminine"],
                        "number": ["singular", "dual", "plural"],
                        "case": ["nominative", "accusative", "genitive"],
                        "definiteness": ["definite", "indefinite"],
                    },
                    "derivational_features": {
                        "verbal_form": [
                            "I",
                            "II",
                            "III",
                            "IV",
                            "V",
                            "VI",
                            "VII",
                            "VIII",
                            "IX",
                            "X",
                        ],
                        "noun_type": ["concrete", "abstract", "agent", "patient"],
                        "adjective_type": ["qualitative", "relational", "intensive"],
                    },
                },
            },
            output_data=[
                "morpheme_structure",
                "grammatical_features",
                "semantic_roles",
            ],
            confidence_score=0.82,
            next_nodes=["word_formation"],
            error_handling={
                "unrecognized_morphemes": "consult_lexicon",
                "invalid_combinations": "check_morphological_rules",
                "ambiguous_analysis": "rank_by_frequency",
            },
        )

        # LAYER 5: WORD FORMATION,
    self.decision_tree["word_formation"] = DecisionNode(
            node_id="word_formation",
            node_type=NodeType.WORD_LAYER,
            input_data=["morpheme_structure", "grammatical_features"],
            decision_criteria=[
                DecisionCriteria.MORPHOLOGICAL_VALIDITY,
                DecisionCriteria.SEMANTIC_PLAUSIBILITY,
            ],
            processing_rules={
                "word_assembly": {
                    "morpheme_ordering": [
                        "prefix_attachment",
                        "stem_formation",
                        "suffix_attachment",
                        "phonological_adjustments",
                    ],
                    "allomorph_selection": {
                        "context_sensitive_rules": True,
                        "phonological_conditioning": True,
                        "lexical_conditioning": True,
                    },
                    "morphophonemic_processes": [
                        "vowel_assimilation",
                        "consonant_assimilation",
                        "epenthesis",
                        "deletion",
                        "metathesis",
                    ],
                },
                "word_validation": {
                    "lexicon_lookup": True,
                    "productivity_check": True,
                    "semantic_coherence": True,
                    "pragmatic_acceptability": True,
                },
                "output_formatting": {
                    "vocalization_level": ["none", "partial", "full"],
                    "orthographic_normalization": True,
                    "presentation_form": ["academic", "modern", "classical"],
                },
            },
            output_data=["formed_word", "confidence_metrics", "alternative_forms"],
            confidence_score=0.78,
            next_nodes=["final_output"],
            error_handling={
                "formation_failure": "provide_closest_valid_form",
                "low_confidence": "flag_for_manual_review",
                "multiple_valid_forms": "rank_by_frequency_and_context",
            },
        )

        # LAYER 6: FINAL OUTPUT,
    self.decision_tree["final_output"] = DecisionNode(
            node_id="final_output",
            node_type=NodeType.FINAL_OUTPUT,
            input_data=["formed_word", "confidence_metrics"],
            decision_criteria=[DecisionCriteria.SEMANTIC_PLAUSIBILITY],
            processing_rules={
                "quality_assessment": {
                    "linguistic_validity": True,
                    "confidence_threshold": 0.7,
                    "error_detection": True,
                },
                "output_enrichment": {
                    "add_linguistic_analysis": True,
                    "include_alternatives": True,
                    "provide_explanations": True,
                },
                "result_presentation": {
                    "format": "comprehensive_analysis",
                    "include_metadata": True,
                    "error_reporting": True,
                },
            },
            output_data=["final_word", "analysis_report", "processing_metadata"],
            confidence_score=0.75,
            next_nodes=[],
            error_handling={
                "below_threshold": "return_error_with_explanation",
                "processing_timeout": "return_partial_results",
                "system_error": "log_and_return_graceful_failure",
            },
        )

    def _load_linguistic_rules(self):  # type: ignore[no-untyped def]
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©"""

        self.linguistic_rules = {
            "phoneme_inventory": {
                "consonants": {
                    "stops": ["Ø¨", "Øª", "Ø¯", "Ø·", "Ùƒ", "Ù‚", "Ø¡"],
                    "fricatives": [
                        "Ù",
                        "Ø«",
                        "Ø°",
                        "Ø³",
                        "Ø²",
                        "Ø´",
                        "Øµ",
                        "Ø¶",
                        "Ø®",
                        "Øº",
                        "Ø­",
                        "Ø¹",
                        "Ù‡",
                    ],
                    "nasals": ["Ù…", "Ù†"],
                    "liquids": ["Ù„", "Ø±"],
                    "glides": ["Ùˆ", "ÙŠ"],
                },
                "vowels": {
                    "short": ["Ù", "Ù", "Ù"],
                    "long": ["Ø§", "ÙŠ", "Ùˆ"],
                    "tanween": ["Ù‹", "ÙŒ", "Ù"],
                },
            },
            "syllable_constraints": {
                "minimal_word": "CV",
                "maximal_syllable": "CVCC",
                "preferred_patterns": ["CV", "CVC", "CVV"],
                "stress_rules": {
                    "primary_stress": "final_heavy_syllable",
                    "secondary_stress": "alternating_pattern",
                    "stress_clash_resolution": "right_to_left",
                },
            },
            "morphological_patterns": {
                "trilateral_patterns": [
                    "ÙÙØ¹ÙÙ„",
                    "ÙÙØ¹ÙÙ„",
                    "ÙÙØ¹ÙÙ„",  # verbs
                    "ÙØ§Ø¹ÙÙ„",
                    "ÙÙØ¹Ù‘Ø§Ù„",
                    "Ù…ÙÙØ¹ÙˆÙ„",  # participles
                    "ÙÙØ¹Ù’Ù„",
                    "ÙÙØ¹Ù’Ù„",
                    "ÙÙØ¹Ù’Ù„",  # nouns
                ],
                "augmented_patterns": [
                    "Ø£ÙÙØ¹ÙÙ„",
                    "ÙÙØ¹ÙÙ‘Ù„",
                    "ÙØ§Ø¹ÙÙ„",  # Form II, III, IV
                    "ØªÙÙÙØ¹ÙÙ‘Ù„",
                    "ØªÙÙØ§Ø¹ÙÙ„",
                    "Ø§ÙÙ†ÙÙØ¹ÙÙ„",  # Form V, VI, VII
                ],
            },
        }

    def process_text_to_word(self, input_text: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""

        logger.info(f"Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: {input_text}")

        processing_log = []
        current_node = "root"
        current_data = input_text,
    try:
            while current_node and current_node in self.decision_tree:
                node = self.decision_tree[current_node]

                # Process current node,
    processing_result = self._process_node(node, current_data)
                processing_log.append(
                    {
                        "node": current_node,
                        "input": current_data,
                        "output": processing_result["output"],
                        "confidence": processing_result["confidence"],
                        "decisions_made": processing_result["decisions"],
                    }
                )

                # Move to next node,
    if node.next_nodes:
                    current_node = node.next_nodes[0]  # Follow primary path,
    current_data = processing_result["output"]
                else:
                    break

            # Compile final result,
    final_result = {
                "input_text": input_text,
                "final_word": current_data,
                "processing_pipeline": processing_log,
                "overall_confidence": self._calculate_overall_confidence(
                    processing_log
                ),
                "decision_path": [log["node"] for log in processing_log],
                "processing_success": True,
            }

            logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ÙƒØªÙ…Ù„Ø©: {input_text} â†’ {current_data}")
            return final_result,
    except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
            return {
                "input_text": input_text,
                "error": str(e),
                "processing_pipeline": processing_log,
                "processing_success": False,
            }

    def _process_node(self, node: DecisionNode, input_data: Any) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù‚Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø§Ù„Ø´Ø¬Ø±Ø©"""

        decisions_made = []

        # Apply processing rules based on node type,
    if node.node_type == NodeType.ROOT:
            output = self._process_root_node(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.PHONEME_LAYER:
            output = self._process_phoneme_layer(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.DIACRITIC_LAYER:
            output = self._process_diacritic_layer(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.SYLLABLE_LAYER:
            output = self._process_syllable_layer(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.MORPHEME_LAYER:
            output = self._process_morpheme_layer(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.WORD_LAYER:
            output = self._process_word_layer(
                input_data, node.processing_rules, decisions_made
            )
        elif node.node_type == NodeType.FINAL_OUTPUT:
            output = self._process_final_output(
                input_data, node.processing_rules, decisions_made
            )
        else:
            output = input_data  # Pass through,
    return {
            "output": output,
            "confidence": node.confidence_score,
            "decisions": decisions_made,
        }

    def _process_root_node(self, input_data: str, rules: Dict, decisions: List) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø¬Ø°Ø±"""
        decisions.append("text_normalization_applied")
        # Basic text normalization,
    normalized = input_data.strip()
        return normalized,
    def _process_phoneme_layer(
        self, input_data: str, rules: Dict, decisions: List
    ) -> List[str]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª"""
        decisions.append("phoneme_classification_completed")
        # Convert text to phoneme sequence,
    phonemes = list(input_data)
        return phonemes,
    def _process_diacritic_layer(
        self, input_data: List[str], rules: Dict, decisions: List
    ) -> List[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª"""
        decisions.append("diacritic_analysis_applied")
        # Add diacritic information,
    processed = []
        for phoneme in input_data:
            processed.append(
                {
                    "phoneme": phoneme,
                    "diacritics": self._identify_diacritics(phoneme),
                    "vowelization": self._predict_vowelization(phoneme),
                }
            )
        return processed,
    def _process_syllable_layer(
        self, input_data: List[Dict], rules: Dict, decisions: List
    ) -> List[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""
        decisions.append("syllable_construction_completed")
        # Construct syllables,
    syllables = self._construct_syllables(input_data)
        return syllables,
    def _process_morpheme_layer(
        self, input_data: List[Dict], rules: Dict, decisions: List
    ) -> Dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª"""
        decisions.append("morpheme_analysis_completed")
        # Analyze morphemes,
    return {
            "morphemes": input_data,
            "root": self._extract_root(input_data),
            "pattern": self._identify_pattern(input_data),
            "features": self._extract_features(input_data),
        }

    def _process_word_layer(
        self, input_data: Dict, rules: Dict, decisions: List
    ) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
        decisions.append("word_formation_completed")
        # Form final word,
    return self._form_word(input_data)

    def _process_final_output(
        self, input_data: str, rules: Dict, decisions: List
    ) -> Dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        decisions.append("final_output_prepared")
        return {"word": input_data, "analysis": "complete", "quality": "validated"}

    # Helper methods for processing,
    def _identify_diacritics(self, phoneme: str) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ù„Ù„ÙÙˆÙ†ÙŠÙ…"""
        diacritics = []
        if phoneme in ["Ù", "Ù", "Ù", "Ù‹", "ÙŒ", "Ù", "Ù’", "Ù‘"]:
            diacritics.append(phoneme)
        return diacritics,
    def _predict_vowelization(self, phoneme: str) -> str:
        """ØªÙ†Ø¨Ø¤ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„ÙÙˆÙ†ÙŠÙ…"""
        # Simplified vowelization prediction,
    consonants = ["Ø¨", "Øª", "Ø«", "Ø¬", "Ø­", "Ø®", "Ø¯", "Ø°", "Ø±", "Ø²", "Ø³", "Ø´"]
        if phoneme in consonants:
            return "Ù"  # Default fatha,
    return ""

    def _construct_syllables(self, phoneme_data: List[Dict]) -> List[Dict]:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª"""
        syllables = []
        current_syllable = []

        for item in phoneme_data:
            current_syllable.append(item)
            # Simplified syllable boundary detection,
    if len(current_syllable) >= 2:  # CV minimum,
    syllables.append(
                    {
                        "syllable": current_syllable,
                        "type": "CV" if len(current_syllable) == 2 else "CVC",
                        "weight": "light" if len(current_syllable) == 2 else "heavy",
                    }
                )
                current_syllable = []

        if current_syllable:
            syllables.append(
                {
                    "syllable": current_syllable,
                    "type": "incomplete",
                    "weight": "unknown",
                }
            )

        return syllables,
    def _extract_root(self, data: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # Simplified root extraction,
    consonants = []
        for syllable in data:
            for phoneme_data in syllable.get("syllable", []):
                phoneme = phoneme_data.get("phoneme", "")
                if phoneme not in ["Ù", "Ù", "Ù", "Ø§", "Ùˆ", "ÙŠ"]:
                    consonants.append(phoneme)

        return "".join(consonants[:3])  # Take first 3 consonants as root,
    def _identify_pattern(self, data: List[Dict]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
        # Simplified pattern identification,
    return "ÙÙØ¹ÙÙ„"  # Default pattern,
    def _extract_features(self, data: List[Dict]) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ø­ÙˆÙŠØ©"""
        return {"word_class": "noun", "gender": "masculine", "number": "singular"}

    def _form_word(self, morpheme_data: Dict) -> str:
        """ØªÙƒÙˆÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        # Simplified word formation,
    root = morpheme_data.get("root", "ÙƒØªØ¨")
        return root  # Return root as word for simplicity,
    def _calculate_overall_confidence(self, processing_log: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""
        if not processing_log:
            return 0.0,
    confidences = [log.get("confidence", 0.0) for log in processing_log]
        return sum(confidences) / len(confidences)

    def generate_decision_tree_diagram(self) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø®Ø·Ø· Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª"""

        diagram = """
# ğŸŒ³ Arabic Phoneme-to-Word Decision Tree Logic Flow
# Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª: Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

## ğŸ“Š DECISION TREE STRUCTURE

```
ROOT (Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù…)
 â”‚
 â”œâ”€ Text Normalization
 â”œâ”€ Character Validation
 â”œâ”€ Encoding Check
 â”‚
 â–¼
PHONEME LAYER (Ø·Ø¨Ù‚Ø© Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª)
 â”‚
 â”œâ”€ Consonant Classification
 â”‚   â”œâ”€ Stops: [Ø¨ØŒ ØªØŒ Ø¯ØŒ Ø·ØŒ ÙƒØŒ Ù‚ØŒ Ø¡]
 â”‚   â”œâ”€ Fricatives: [ÙØŒ Ø«ØŒ Ø°ØŒ Ø³ØŒ Ø²ØŒ Ø´ØŒ ØµØŒ Ø¶ØŒ Ø®ØŒ ØºØŒ Ø­ØŒ Ø¹ØŒ Ù‡]
 â”‚   â”œâ”€ Nasals: [Ù…ØŒ Ù†]
 â”‚   â”œâ”€ Liquids: [Ù„ØŒ Ø±]
 â”‚   â””â”€ Glides: [ÙˆØŒ ÙŠ]
 â”‚
 â”œâ”€ Vowel Identification
 â”‚   â”œâ”€ Short Vowels: [ÙØŒ ÙØŒ Ù]
 â”‚   â”œâ”€ Long Vowels: [Ø§ØŒ ÙŠØŒ Ùˆ]
 â”‚   â””â”€ Tanween: [Ù‹ØŒ ÙŒØŒ Ù]
 â”‚
 â””â”€ Feature Extraction
     â”œâ”€ Place of Articulation
     â”œâ”€ Manner of Articulation
     â”œâ”€ Voicing
     â””â”€ Emphasis
 â”‚
 â–¼
DIACRITIC LAYER (Ø·Ø¨Ù‚Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª)
 â”‚
 â”œâ”€ Explicit Diacritic Processing
 â”‚   â”œâ”€ Fatha (Ù) â†’ Open vowel
 â”‚   â”œâ”€ Kasra (Ù) â†’ Front vowel
 â”‚   â”œâ”€ Damma (Ù) â†’ Back vowel
 â”‚   â”œâ”€ Sukun (Ù’) â†’ No vowel
 â”‚   â””â”€ Shadda (Ù‘) â†’ Gemination
 â”‚
 â”œâ”€ Vowel Prediction (for missing diacritics)
 â”‚   â”œâ”€ Morphological Context
 â”‚   â”œâ”€ Phonological Patterns
 â”‚   â”œâ”€ Statistical Models
 â”‚   â””â”€ Rule-based Inference
 â”‚
 â””â”€ Morphophonemic Rules
     â”œâ”€ Vowel Harmony
     â”œâ”€ Assimilation Rules
     â””â”€ Vowel Epenthesis
 â”‚
 â–¼
SYLLABLE LAYER (Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹)
 â”‚
 â”œâ”€ Syllable Boundary Detection
 â”‚   â”œâ”€ Nucleus Identification (vowels)
 â”‚   â”œâ”€ Onset Assignment (consonants before nucleus)
 â”‚   â””â”€ Coda Assignment (consonants after nucleus)
 â”‚
 â”œâ”€ Syllable Structure Classification
 â”‚   â”œâ”€ CV (Light syllable)
 â”‚   â”œâ”€ CVC (Heavy syllable)
 â”‚   â”œâ”€ CVV (Heavy syllable - long vowel)
 â”‚   â”œâ”€ CVVC (Super-heavy syllable)
 â”‚   â””â”€ CVCC (Super-heavy syllable)
 â”‚
 â”œâ”€ Phonotactic Constraint Checking
 â”‚   â”œâ”€ Valid Onset Clusters
 â”‚   â”œâ”€ Valid Coda Clusters
 â”‚   â”œâ”€ Consonant Compatibility
 â”‚   â””â”€ Sonority Sequencing
 â”‚
 â””â”€ Prosodic Analysis
     â”œâ”€ Syllable Weight Calculation
     â”œâ”€ Stress Assignment Rules
     â””â”€ Rhythmic Pattern Detection
 â”‚
 â–¼
MORPHEME LAYER (Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª)
 â”‚
 â”œâ”€ Root Extraction
 â”‚   â”œâ”€ Trilateral Root Identification
 â”‚   â”‚   â”œâ”€ Strong Roots: ÙƒØªØ¨ØŒ Ø¯Ø±Ø³ØŒ Ø¹Ù„Ù…
 â”‚   â”‚   â”œâ”€ Weak Roots: Ù‚ÙˆÙ„ØŒ Ø¨ÙŠØ¹ØŒ ÙˆÙ‚Ù
 â”‚   â”‚   â””â”€ Doubled Roots: Ù…Ø¯Ø¯ØŒ Ø±Ø¯Ø¯ØŒ Ø¹Ø¯Ø¯
 â”‚   â”‚
 â”‚   â”œâ”€ Quadrilateral Root Identification
 â”‚   â”‚   â”œâ”€ Primary: Ø¯Ø­Ø±Ø¬ØŒ Ø²Ù„Ø²Ù„ØŒ ÙˆØ³ÙˆØ³
 â”‚   â”‚   â””â”€ Extended: ØªØ±Ø¬Ù…ØŒ ØªÙ„ÙØ²ØŒ ÙÙ„Ø³Ù
 â”‚   â”‚
 â”‚   â””â”€ Root Validation
 â”‚       â”œâ”€ Lexicon Lookup
 â”‚       â”œâ”€ Semantic Coherence
 â”‚       â””â”€ Frequency Analysis
 â”‚
 â”œâ”€ Pattern/Template Matching
 â”‚   â”œâ”€ Verbal Patterns
 â”‚   â”‚   â”œâ”€ Form I: ÙÙØ¹ÙÙ„ØŒ ÙÙØ¹ÙÙ„ØŒ ÙÙØ¹ÙÙ„
 â”‚   â”‚   â”œâ”€ Form II: ÙÙØ¹ÙÙ‘Ù„ (intensitive)
 â”‚   â”‚   â”œâ”€ Form III: ÙØ§Ø¹ÙÙ„ (associative)
 â”‚   â”‚   â”œâ”€ Form IV: Ø£ÙÙØ¹ÙÙ„ (causative)
 â”‚   â”‚   â””â”€ Forms V-X: ØªÙÙÙØ¹ÙÙ‘Ù„ØŒ ØªÙÙØ§Ø¹ÙÙ„ØŒ etc.
 â”‚   â”‚
 â”‚   â”œâ”€ Nominal Patterns
 â”‚   â”‚   â”œâ”€ Agent: ÙØ§Ø¹ÙÙ„ (ÙƒØ§ØªØ¨ØŒ Ù‚Ø§Ø±Ø¦)
 â”‚   â”‚   â”œâ”€ Patient: Ù…ÙÙØ¹ÙˆÙ„ (Ù…ÙƒØªÙˆØ¨ØŒ Ù…Ù‚Ø±ÙˆØ¡)
 â”‚   â”‚   â”œâ”€ Instrument: Ù…ÙÙØ¹Ø§Ù„ (Ù…ÙØªØ§Ø­ØŒ Ù…Ù‚Øµ)
 â”‚   â”‚   â”œâ”€ Place: Ù…ÙÙØ¹ÙÙ„ (Ù…ÙƒØªØ¨ØŒ Ù…Ø¯Ø±Ø³Ø©)
 â”‚   â”‚   â””â”€ Abstract: ÙÙØ¹Ù’Ù„ØŒ ÙÙØ¹Ù’Ù„ØŒ ÙÙØ¹Ù’Ù„
 â”‚   â”‚
 â”‚   â””â”€ Adjectival Patterns
 â”‚       â”œâ”€ Intensive: ÙÙØ¹Ù‘Ø§Ù„ (Ø¬Ù…ÙŠÙ„ØŒ ÙƒØ¨ÙŠØ±)
 â”‚       â”œâ”€ Comparative: Ø£ÙÙØ¹ÙÙ„ (Ø£ÙƒØ¨Ø±ØŒ Ø£Ø¬Ù…Ù„)
 â”‚       â””â”€ Elative: ÙÙØ¹Ù„Ø§Ù† (Ø¹Ø·Ø´Ø§Ù†ØŒ Ø¬ÙˆØ¹Ø§Ù†)
 â”‚
 â”œâ”€ Affix Analysis
 â”‚   â”œâ”€ Prefixes
 â”‚   â”‚   â”œâ”€ Definite Article: Ø§Ù„
 â”‚   â”‚   â”œâ”€ Prepositions: Ø¨ØŒ Ù„ØŒ ÙØŒ Ù…Ù†ØŒ Ø¥Ù„Ù‰
 â”‚   â”‚   â”œâ”€ Conjunctions: ÙˆØŒ Ù
 â”‚   â”‚   â””â”€ Verbal Prefixes: Ø£ØŒ Ù†ØŒ ØªØŒ ÙŠ
 â”‚   â”‚
 â”‚   â”œâ”€ Suffixes
 â”‚   â”‚   â”œâ”€ Pronominal: Ù‡ØŒ Ù‡Ø§ØŒ Ù‡Ù…ØŒ Ù‡Ù†ØŒ ÙŠØŒ Ùƒ
 â”‚   â”‚   â”œâ”€ Feminine: Ø©ØŒ Ø§Øª
 â”‚   â”‚   â”œâ”€ Plural: ÙˆÙ†ØŒ ÙŠÙ†ØŒ Ø§Øª
 â”‚   â”‚   â””â”€ Case Markers: Ù‹ØŒ ÙŒØŒ Ù
 â”‚   â”‚
 â”‚   â””â”€ Infixes (internal modifications)
 â”‚       â”œâ”€ Passive Voice: vowel pattern changes
 â”‚       â”œâ”€ Reflexive: Øª infix
 â”‚       â””â”€ Augmentation: various consonant insertions
 â”‚
 â””â”€ Morphological Feature Assignment
     â”œâ”€ Lexical Category: noun, verb, adjective, particle
     â”œâ”€ Inflectional Features
     â”‚   â”œâ”€ Gender: masculine, feminine
     â”‚   â”œâ”€ Number: singular, dual, plural
     â”‚   â”œâ”€ Case: nominative, accusative, genitive
     â”‚   â”œâ”€ Definiteness: definite, indefinite
     â”‚   â”œâ”€ Person: 1st, 2nd, 3rd
     â”‚   â””â”€ Tense/Aspect: perfective, imperfective, subjunctive
     â”‚
     â””â”€ Derivational Features
         â”œâ”€ Verbal Form: I-X
         â”œâ”€ Semantic Role: agent, patient, instrument, location
         â””â”€ Productivity: productive, semi-productive, lexicalized
 â”‚
 â–¼
WORD LAYER (Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª)
 â”‚
 â”œâ”€ Morpheme Assembly
 â”‚   â”œâ”€ Linear Ordering
 â”‚   â”‚   â”œâ”€ Prefix + Stem + Suffix ordering
 â”‚   â”‚   â”œâ”€ Multiple affix coordination
 â”‚   â”‚   â””â”€ Constraint satisfaction
 â”‚   â”‚
 â”‚   â”œâ”€ Allomorph Selection
 â”‚   â”‚   â”œâ”€ Phonologically conditioned variants
 â”‚   â”‚   â”œâ”€ Morphologically conditioned variants
 â”‚   â”‚   â””â”€ Lexically specified variants
 â”‚   â”‚
 â”‚   â””â”€ Interface Resolution
 â”‚       â”œâ”€ Morpheme boundary effects
 â”‚       â”œâ”€ Feature percolation
 â”‚       â””â”€ Blocking and competition
 â”‚
 â”œâ”€ Morphophonemic Processing
 â”‚   â”œâ”€ Phonological Rules Application
 â”‚   â”‚   â”œâ”€ Assimilation
 â”‚   â”‚   â”‚   â”œâ”€ Place assimilation: Ù†Ø¨ â†’ Ù…Ø¨
 â”‚   â”‚   â”‚   â”œâ”€ Voice assimilation: ØªØ¯ â†’ Ø¯Ø¯
 â”‚   â”‚   â”‚   â””â”€ Manner assimilation: Ù†Ù„ â†’ Ù„Ù„
 â”‚   â”‚   â”‚
 â”‚   â”‚   â”œâ”€ Epenthesis (vowel insertion)
 â”‚   â”‚   â”‚   â”œâ”€ Cluster breaking: ÙƒØªØ¨ â†’ ÙƒÙØªÙØ¨
 â”‚   â”‚   â”‚   â”œâ”€ Hiatus resolution: Ø§Ø§ â†’ Ø¢
 â”‚   â”‚   â”‚   â””â”€ Word-initial clusters: Ø³ØªØ¹Ù…Ù„ â†’ Ø§Ø³ØªØ¹Ù…Ù„
 â”‚   â”‚   â”‚
 â”‚   â”‚   â”œâ”€ Deletion
 â”‚   â”‚   â”‚   â”œâ”€ Vowel deletion in clusters
 â”‚   â”‚   â”‚   â”œâ”€ Consonant deletion in complex clusters
 â”‚   â”‚   â”‚   â””â”€ Final vowel deletion
 â”‚   â”‚   â”‚
 â”‚   â”‚   â””â”€ Metathesis (sound reordering)
 â”‚   â”‚       â”œâ”€ Øª + Øµ â†’ ØµØª
 â”‚   â”‚       â”œâ”€ Øª + Ø· â†’ Ø·Øª
 â”‚   â”‚       â””â”€ Øª + Ø² â†’ Ø²Øª
 â”‚   â”‚
 â”‚   â”œâ”€ Weak Letter Processing (Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ù„Ø©)
 â”‚   â”‚   â”œâ”€ Vowel-Consonant Alternations
 â”‚   â”‚   â”‚   â”œâ”€ Ù‚ÙˆÙ„: Ù‚Ø§Ù„ØŒ ÙŠÙ‚ÙˆÙ„ØŒ Ù‚Ø§Ø¦Ù„
 â”‚   â”‚   â”‚   â”œâ”€ Ø¨ÙŠØ¹: Ø¨Ø§Ø¹ØŒ ÙŠØ¨ÙŠØ¹ØŒ Ø¨Ø§Ø¦Ø¹
 â”‚   â”‚   â”‚   â””â”€ ÙˆÙ‚Ù: ÙˆÙ‚ÙØŒ ÙŠÙ‚ÙØŒ ÙˆØ§Ù‚Ù
 â”‚   â”‚   â”‚
 â”‚   â”‚   â”œâ”€ Vowel Length Adjustments
 â”‚   â”‚   â”‚   â”œâ”€ Long vowel shortening
 â”‚   â”‚   â”‚   â”œâ”€ Short vowel lengthening
 â”‚   â”‚   â”‚   â””â”€ Vowel quality changes
 â”‚   â”‚   â”‚
 â”‚   â”‚   â””â”€ Glide Formation
 â”‚   â”‚       â”œâ”€ i â†’ y / _V
 â”‚   â”‚       â”œâ”€ u â†’ w / _V
 â”‚   â”‚       â””â”€ Vowel hiatus resolution
 â”‚   â”‚
 â”‚   â””â”€ Gemination Processing (Ø§Ù„ØªØ¶Ø¹ÙŠÙ)
 â”‚       â”œâ”€ Morphological gemination
 â”‚       â”œâ”€ Phonological gemination
 â”‚       â””â”€ Gemination simplification rules
 â”‚
 â”œâ”€ Lexical Validation
 â”‚   â”œâ”€ Dictionary Lookup
 â”‚   â”‚   â”œâ”€ Root-based lexicon search
 â”‚   â”‚   â”œâ”€ Full-form lexicon search
 â”‚   â”‚   â””â”€ Compound analysis
 â”‚   â”‚
 â”‚   â”œâ”€ Productivity Assessment
 â”‚   â”‚   â”œâ”€ Pattern productivity metrics
 â”‚   â”‚   â”œâ”€ Neologism detection
 â”‚   â”‚   â””â”€ Borrowing identification
 â”‚   â”‚
 â”‚   â””â”€ Semantic Validation
 â”‚       â”œâ”€ Compositional semantics
 â”‚       â”œâ”€ Selectional restrictions
 â”‚       â””â”€ Pragmatic plausibility
 â”‚
 â””â”€ Output Generation
     â”œâ”€ Orthographic Realization
     â”‚   â”œâ”€ Script selection (Arabic script)
     â”‚   â”œâ”€ Diacritic level (none/partial/full)
     â”‚   â””â”€ Presentation forms
     â”‚
     â”œâ”€ Alternative Forms Generation
     â”‚   â”œâ”€ Morphological variants
     â”‚   â”œâ”€ Phonological variants
     â”‚   â””â”€ Dialectal variants
     â”‚
     â””â”€ Confidence Scoring
         â”œâ”€ Individual component scores
         â”œâ”€ Aggregate confidence
         â””â”€ Uncertainty quantification
 â”‚
 â–¼
FINAL OUTPUT (Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ)
 â”‚
 â”œâ”€ Quality Assessment
 â”‚   â”œâ”€ Linguistic Validity Check
 â”‚   â”œâ”€ Confidence Threshold Check
 â”‚   â””â”€ Error Detection
 â”‚
 â”œâ”€ Result Enrichment
 â”‚   â”œâ”€ Linguistic Analysis Metadata
 â”‚   â”‚   â”œâ”€ Morphological breakdown
 â”‚   â”‚   â”œâ”€ Phonological structure
 â”‚   â”‚   â”œâ”€ Syntactic features
 â”‚   â”‚   â””â”€ Semantic information
 â”‚   â”‚
 â”‚   â”œâ”€ Processing Information
 â”‚   â”‚   â”œâ”€ Decision path taken
 â”‚   â”‚   â”œâ”€ Alternative analyses
 â”‚   â”‚   â”œâ”€ Confidence scores
 â”‚   â”‚   â””â”€ Processing time
 â”‚   â”‚
 â”‚   â””â”€ Error Reporting
 â”‚       â”œâ”€ Failed analyses
 â”‚       â”œâ”€ Ambiguous cases
 â”‚       â”œâ”€ Low confidence warnings
 â”‚       â””â”€ Suggested improvements
 â”‚
 â””â”€ Final Word + Comprehensive Analysis
     â”œâ”€ Primary Result: Final Arabic Word
     â”œâ”€ Analysis Report: Complete linguistic breakdown
     â”œâ”€ Metadata: Processing details and statistics
     â””â”€ Alternatives: Other possible analyses
```

## ğŸ”„ DECISION CRITERIA AT EACH LAYER

### Phoneme Layer Decisions:
- âœ… Is character in Arabic phoneme inventory?
- âœ… What are the phonetic features?
- âœ… Are there phoneme boundary ambiguities?
- âœ… Are there foreign/borrowed elements?

### Diacritic Layer Decisions:
- âœ… Are diacritics explicitly marked?
- âœ… Can missing vowels be predicted?
- âœ… Do vowel patterns follow morphological rules?
- âœ… Are there diacritic conflicts to resolve?

### Syllable Layer Decisions:
- âœ… Where are syllable boundaries?
- âœ… What syllable types are present?
- âœ… Are phonotactic constraints satisfied?
- âœ… How should stress be assigned?

### Morpheme Layer Decisions:
- âœ… Can a valid root be extracted?
- âœ… Does the word match known patterns?
- âœ… Are affixes properly identified?
- âœ… Do morphological features cohere?

### Word Layer Decisions:
- âœ… How should morphemes be assembled?
- âœ… Which phonological rules apply?
- âœ… Is the result lexically valid?
- âœ… What is the confidence level?

### Final Output Decisions:
- âœ… Does the result meet quality thresholds?
- âœ… Should alternatives be provided?
- âœ… How much analysis detail to include?
- âœ… Are there errors to report?

## ğŸ“Š CONFIDENCE SCORING ALGORITHM,
    Each layer contributes to overall confidence:
- Phoneme Layer: 0.95 (high - deterministic)
- Diacritic Layer: 0.88 (good - some prediction involved)
- Syllable Layer: 0.85 (good - structural constraints)
- Morpheme Layer: 0.82 (fair - lexical knowledge required)
- Word Layer: 0.78 (fair - complex interactions)
- Final Output: 0.75 (fair - overall assessment)

**Overall Confidence = Î (individual_confidences) / number_of_layers**

## ğŸš¨ ERROR HANDLING STRATEGIES

- **Graceful Degradation**: Continue processing with reduced confidence
- **Alternative Paths**: Explore multiple analysis options
- **Fallback Methods**: Use simpler algorithms when complex ones fail
- **Human-in-the-Loop**: Flag difficult cases for manual review
- **Learning from Errors**: Update models based on correction feedback

---

**ğŸ¯ This decision tree represents the complete logic flow from individual Arabic phonemes to fully formed and analyzed Arabic words.**
        """

        return diagram,
    def export_tree_structure(self, filename: str = "arabic_decision_tree.json"):  # type: ignore[no-untyped def]
        """ØªØµØ¯ÙŠØ± Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø´Ø¬Ø±Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù JSON"""

        exportable_tree = {}
        for node_id, node in self.decision_tree.items():
            exportable_tree[node_id] = asdict(node)
            # Convert enums to strings for JSON serialization,
    exportable_tree[node_id]["node_type"] = node.node_type.value,
    exportable_tree[node_id]["decision_criteria"] = [
                criteria.value for criteria in node.decision_criteria
            ]

        with open(filename, 'w', encoding='utf 8') as f:
            json.dump(exportable_tree, f, ensure_ascii=False, indent=2)

        logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø´Ø¬Ø±Ø© Ø¥Ù„Ù‰ {filename}")

    def export_linguistic_rules(self, filename: str = "arabic_linguistic_rules.yaml"):  # type: ignore[no-untyped def]
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø¥Ù„Ù‰ Ù…Ù„Ù YAML"""

        with open(filename, 'w', encoding='utf 8') as f:
            yaml.dump(
                self.linguistic_rules, f, allow_unicode=True, default_flow_style=False
            )

        logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø¥Ù„Ù‰ {filename}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION AND DEMONSTRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():  # type: ignore[no-untyped def]
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ¥Ø¸Ù‡Ø§Ø± Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª"""

    print("ğŸŒ³ Arabic Phoneme-to Word Decision Tree System")
    print("=" * 60)

    # Create decision tree system,
    decision_tree = ArabicPhonemeWordDecisionTree()

    # Generate and display decision tree diagram,
    print("\nğŸ“Š Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©:")
    diagram = decision_tree.generate_decision_tree_diagram()
    print(diagram)

    # Test the system with sample text,
    test_cases = ["ÙƒØªØ§Ø¨", "Ù…Ø¯Ø±Ø³Ø©", "ÙŠÙƒØªØ¨", "Ù…ÙƒØªÙˆØ¨"]

    print("\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…:")
    for test_text in test_cases:
        print(f"\n   ğŸ“ Ø§Ù„Ù†Øµ: {test_text}")
        result = decision_tree.process_text_to_word(test_text)

        if result["processing_success"]:
            print(f"   âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result['final_word']}")
            print(f"   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: {result['overall_confidence']:.2f}")
            print(f"   ğŸ›¤ï¸  Ø§Ù„Ù…Ø³Ø§Ø±: {'} â†’ '.join(result['decision_path'])}")
        else:
            print(f"   âŒ Ø®Ø·Ø£: {result['error']}")

    # Export tree structure and rules,
    decision_tree.export_tree_structure()
    decision_tree.export_linguistic_rules()

    print("\nâœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø¹Ø±Ø¶ Ù†Ø¸Ø§Ù… Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª!")
    print("ğŸ“„ ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª:")
    print("   - arabic_decision_tree.json")
    print("   - arabic_linguistic_rules.yaml")


if __name__ == "__main__":
    main()
