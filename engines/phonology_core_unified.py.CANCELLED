#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""""
ğŸ¯ HIERARCHICAL ARABIC WORD TRACING ENGINE ğŸ¯
Zero Layer Phonology Core - Unified Foundation System,
    ÙÙˆÙ†ÙŠÙ… â†’ Ø­Ø±ÙƒØ© â†’ Ù…Ù‚Ø·Ø¹ â†’ Ø¬Ø°Ø± â†’ ÙˆØ²Ù† â†’ Ø§Ø´ØªÙ‚Ø§Ù‚ â†’ ØªØ±ÙƒÙŠØ¨ ØµØ±ÙÙŠ â†’ ÙˆØ²Ù† â†’ ØªØ±ÙƒÙŠØ¨ Ù†Ø­ÙˆÙŠ,
    Author: Arabic NLP Expert Team,
    Version: 3.0.0,
    Date: 2025-07 23,
    License: MIT,
    ZERO LAYER ARCHITECTURE (Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙØ±):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ PHONEME + HARAKAT COMBINATIONS â†’ ALL HIGHER LINGUISTIC LAYERS                  â•‘
â•‘ ğŸ”¤ 28 Arabic Consonants + Pharyngeals + Glottals                               â•‘
â•‘ ğŸµ Short Vowels: ÙØªØ­Ø©ØŒ ÙƒØ³Ø±Ø©ØŒ Ø¶Ù…Ø© + variations + sukun + shadda + tanween      â•‘
â•‘ ğŸ—ï¸ Foundation: Phonology â†’ Syllables â†’ Roots â†’ Patterns â†’ Morphology â†’ Syntax â•‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""""

import logging
    from typing import Dict, List, Any, Tuple
    from dataclasses import dataclass
    from enum import Enum
    from collections import defaultdict

# Configure comprehensive logging,
    logging.basicConfig()
    logging.basicConfig(level=logging.INFO,)
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s','
    handlers=[
        logging.FileHandler('hierarchical_arabic_tracing.log', encoding='utf 8'),'
        logging.StreamHandler(),
    ])
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ZERO LAYER: PHONOLOGICAL FEATURE CLASSIFICATION SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPhonemeType(Enum):
    """ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""

    CONSONANT = "consonant"  # ØµØ§Ù…Øª"
    VOWEL_SHORT = "vowel_short"  # Ø­Ø±ÙƒØ© Ù‚ØµÙŠØ±Ø©"
    VOWEL_LONG = "vowel_long"  # Ø­Ø±ÙƒØ© Ø·ÙˆÙŠÙ„Ø©"
    DIACRITIC = "diacritic"  # ØªØ´ÙƒÙŠÙ„"
    SUKUN = "sukun"  # Ø³ÙƒÙˆÙ†"
    SHADDA = "shadda"  # Ø´Ø¯Ø©"
    TANWEEN = "tanween"  # ØªÙ†ÙˆÙŠÙ†"


class ArabicConsonantFeatures(Enum):
    """Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„ØµØ§Ù…ØªØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""

    # Place of Articulation - Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ø·Ù‚,
    BILABIAL = "bilabial"  # Ø´ÙÙˆÙŠ"
    LABIODENTAL = "labiodental"  # Ø´ÙÙˆÙŠ Ø£Ø³Ù†Ø§Ù†ÙŠ"
    DENTAL = "dental"  # Ø£Ø³Ù†Ø§Ù†ÙŠ"
    ALVEOLAR = "alveolar"  # Ù„Ø«ÙˆÙŠ"
    POSTALVEOLAR = "postalveolar"  # Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ù„Ø«ÙˆÙŠ"
    PALATAL = "palatal"  # ØºØ§Ø±ÙŠ"
    VELAR = "velar"  # Ø·Ø¨Ù‚ÙŠ"
    UVULAR = "uvular"  # Ù„Ù‡ÙˆÙŠ"
    PHARYNGEAL = "pharyngeal"  # Ø­Ù„Ù‚ÙŠ"
    GLOTTAL = "glottal"  # Ø­Ù†Ø¬Ø±ÙŠ"

    # Manner of Articulation - Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù†Ø·Ù‚,
    STOP = "stop"  # ÙˆÙ‚ÙØ©"
    FRICATIVE = "fricative"  # Ø§Ø­ØªÙƒØ§ÙƒÙŠ"
    NASAL = "nasal"  # Ø£Ù†ÙÙŠ"
    LIQUID = "liquid"  # Ø³Ø§Ø¦Ù„"
    TRILL = "trill"  # Ø±Ø¹Ø´Ø©"
    APPROXIMANT = "approximant"  # ØªÙ‚Ø±ÙŠØ¨ÙŠ"

    # Voicing - Ø§Ù„Ù‡Ù…Ø³ ÙˆØ§Ù„Ø¬Ù‡Ø±,
    VOICED = "voiced"  # Ù…Ø¬Ù‡ÙˆØ±"
    VOICELESS = "voiceless"  # Ù…Ù‡Ù…ÙˆØ³"

    # Emphasis - Ø§Ù„ØªÙØ®ÙŠÙ…,
    PLAIN = "plain"  # Ù…Ø±Ù‚Ù‚"
    EMPHATIC = "emphatic"  # Ù…ÙØ®Ù…"
    PHARYNGEALIZED = "pharyngealized"  # Ù…ÙØ·Ø¨Ù‚"


class ShortVowelType(Enum):
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""

    FATHA = "fatha"  # ÙØªØ­Ø© - a"
    KASRA = "kasra"  # ÙƒØ³Ø±Ø© - i"
    DAMMA = "damma"  # Ø¶Ù…Ø© - u"
    FATHATAN = "fathatan"  # ÙØªØ­ØªØ§Ù† - an"
    KASRATAN = "kasratan"  # ÙƒØ³Ø±ØªØ§Ù† - in"
    DAMMATAN = "dammatan"  # Ø¶Ù…ØªØ§Ù† - un"


@dataclass,
    class PhonemeVector:
    """Ù…ØªØ¬Ù‡ Ø§Ù„ÙÙˆÙ†ÙŠÙ… - Phoneme Vector Representation""""

    symbol: str  # Ø§Ù„Ø±Ù…Ø²,
    arabic_letter: str  # Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ,
    ipa: str  # Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©,
    phoneme_type: ArabicPhonemeType  # Ù†ÙˆØ¹ Ø§Ù„ÙÙˆÙ†ÙŠÙ…,
    features: List[str]  # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©,
    frequency: float  # ØªÙƒØ±Ø§Ø± Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…,
    vector_representation: List[float]  # Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠ,
    morphological_weight: float  # Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ,
    syllable_position: List[str]  # Ù…ÙˆØ§Ø¶Ø¹ ÙÙŠ Ø§Ù„Ù…Ù‚Ø·Ø¹,
    phonological_rules: List[str]  # Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©


@dataclass,
    class HarakatVector:
    """Ù…ØªØ¬Ù‡ Ø§Ù„Ø­Ø±ÙƒØ© - Harakat Vector Representation""""

    symbol: str  # Ø±Ù…Ø² Ø§Ù„Ø­Ø±ÙƒØ©,
    arabic_diacritic: str  # Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¨ÙŠ,
    vowel_type: ShortVowelType  # Ù†ÙˆØ¹ Ø§Ù„Ø­Ø±ÙƒØ©,
    ipa: str  # Ø§Ù„Ø±Ù…Ø² Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ,
    morphological_function: List[str]  # Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„ØµØ±ÙÙŠØ©,
    syntactic_function: List[str]  # Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù†Ø­ÙˆÙŠØ©,
    syllable_weight: float  # ÙˆØ²Ù† Ø§Ù„Ù…Ù‚Ø·Ø¹,
    vector_representation: List[float]  # Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠ,
    frequency: float  # ØªÙƒØ±Ø§Ø± Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…,
    contextual_rules: List[str]  # Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©


@dataclass,
    class CVSegment:
    """Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠ - CV Segment Representation""""

    onset: List[str]  # Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©,
    nucleus: List[str]  # Ø§Ù„Ù†ÙˆØ§Ø©,
    coda: List[str]  # Ø§Ù„Ù†Ù‡Ø§ÙŠØ©,
    cv_pattern: str  # Ù†Ù…Ø· CV,
    syllable_weight: str  # ÙˆØ²Ù† Ø§Ù„Ù…Ù‚Ø·Ø¹: Ø®ÙÙŠÙ/Ø«Ù‚ÙŠÙ„,
    stress: bool  # Ø§Ù„Ù†Ø¨Ø±,
    position_in_word: str  # Ù…ÙˆØ¶Ø¹ ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©,
    vector_representation: List[float]  # Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø±Ù‚Ù…ÙŠ


@dataclass,
    class VectorTrace:
    """ØªØªØ¨Ø¹ Ù…ØªØ¬Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø© - Hierarchical Vector Trace""""

    word: str  # Ø§Ù„ÙƒÙ„Ù…Ø©,
    phonemes: List[PhonemeVector]  # Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª,
    harakat: List[HarakatVector]  # Ø§Ù„Ø­Ø±ÙƒØ§Øª,
    syllables: List[CVSegment]  # Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹,
    root: Tuple[str, ...]  # Ø§Ù„Ø¬Ø°Ø±,
    pattern: str  # Ø§Ù„ÙˆØ²Ù†/Ø§Ù„Ø¨Ø­Ø±,
    derivation_type: str  # Ù†ÙˆØ¹ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚,
    morphological_status: str  # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØµØ±ÙÙŠØ©,
    syntactic_features: Dict[str, str]  # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ø­ÙˆÙŠØ©,
    confidence: float  # Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©,
    final_vector: List[float]  # Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ,
    tracing_steps: List[Dict[str, Any]]  # Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØªØ¨Ø¹


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHONOLOGY CORE ENGINE - UNIFIED FOUNDATION SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PhonologyCoreEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø§Ù„Ù…ÙˆØ­Ø¯ - Zero Layer Foundation""""

    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…ÙˆØ­Ø¯""""
        self.logger = logging.getLogger('PhonologyCoreEngine')'
        self._setup_logging()

        # Initialize phoneme inventory,
    self.phoneme_inventory = self._initialize_phoneme_inventory()
        self.harakat_inventory = self._initialize_harakat_inventory()

        # Phonological rules,
    self.phonological_rules = self._initialize_phonological_rules()

        # CV patterns and syllable templates,
    self.cv_patterns = self._initialize_cv_patterns()

        # Integration with existing engines,
    self.engine_integration = self._setup_engine_integration()

        self.logger.info()
            "ğŸ¯ PhonologyCoreEngine initialized - Zero Layer Foundation Ready""
        )

    def _setup_logging(self) -> None:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª""""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter()
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s''
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

    def _initialize_phoneme_inventory(self) -> Dict[str, PhonemeVector]:
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø®Ø²ÙˆÙ† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""
        phonemes = {}

        # Arabic Consonants - Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„ØµØ§Ù…ØªØ©,
    consonant_data = [
            # [symbol, arabic, ipa, features, frequency]
            ('b', 'Ø¨', 'b', ['bilabial', 'stop', 'voiced'], 0.089),'
            ('t', 'Øª', 't', ['dental', 'stop', 'voiceless'], 0.156),'
            ('th', 'Ø«', 'Î¸', ['dental', 'fricative', 'voiceless'], 0.012),'
            ('j', 'Ø¬', 'Ê¤', ['postalveolar', 'affricate', 'voiced'], 0.056),'
            ('H', 'Ø­', 'Ä§', ['pharyngeal', 'fricative', 'voiceless'], 0.078),'
            ('kh', 'Ø®', 'x', ['uvular', 'fricative', 'voiceless'], 0.034),'
            ('d', 'Ø¯', 'd', ['dental', 'stop', 'voiced'], 0.067),'
            ('dh', 'Ø°', 'Ã°', ['dental', 'fricative', 'voiced'], 0.019),'
            ('r', 'Ø±', 'r', ['alveolar', 'trill', 'voiced'], 0.145),'
            ('z', 'Ø²', 'z', ['alveolar', 'fricative', 'voiced'], 0.023),'
            ('s', 'Ø³', 's', ['alveolar', 'fricative', 'voiceless'], 0.098),'
            ('sh', 'Ø´', 'Êƒ', ['postalveolar', 'fricative', 'voiceless'], 0.045),'
            ('S', 'Øµ', 'sË¤', ['alveolar', 'fricative', 'voiceless', 'emphatic'], 0.067),'
            ('D', 'Ø¶', 'dË¤', ['dental', 'stop', 'voiced', 'emphatic'], 0.023),'
            ('T', 'Ø·', 'tË¤', ['dental', 'stop', 'voiceless', 'emphatic'], 0.034),'
            ('Z', 'Ø¸', 'Ã°Ë¤', ['dental', 'fricative', 'voiced', 'emphatic'], 0.008),'
            ('c', 'Ø¹', 'Ê•', ['pharyngeal', 'fricative', 'voiced'], 0.091),'
            ('gh', 'Øº', 'É£', ['uvular', 'fricative', 'voiced'], 0.028),'
            ('f', 'Ù', 'f', ['labiodental', 'fricative', 'voiceless'], 0.045),'
            ('q', 'Ù‚', 'q', ['uvular', 'stop', 'voiceless'], 0.067),'
            ('k', 'Ùƒ', 'k', ['velar', 'stop', 'voiceless'], 0.078),'
            ('l', 'Ù„', 'l', ['alveolar', 'liquid', 'voiced'], 0.134),'
            ('m', 'Ù…', 'm', ['bilabial', 'nasal', 'voiced'], 0.123),'
            ('n', 'Ù†', 'n', ['alveolar', 'nasal', 'voiced'], 0.167),'
            ('h', 'Ù‡', 'h', ['glottal', 'fricative', 'voiceless'], 0.089),'
            ('w', 'Ùˆ', 'w', ['labial', 'approximant', 'voiced'], 0.098),'
            ('y', 'ÙŠ', 'j', ['palatal', 'approximant', 'voiced'], 0.134),'
            ('hamza', 'Ø¡', 'Ê”', ['glottal', 'stop', 'voiceless'], 0.045),'
        ]

        for symbol, arabic, ipa, features, freq in consonant_data:
            vector_rep = self._create_phoneme_vector(features, freq)
            syllable_pos = ()
                ['onset', 'coda']'
                if symbol not in ['w', 'y']'
                else ['onset', 'nucleus', 'coda']'
            )

            phonemes[arabic] = PhonemeVector()
                symbol=symbol,
                arabic_letter=arabic,
                ipa=ipa,
                phoneme_type=ArabicPhonemeType.CONSONANT,
                features=features,
                frequency=freq,
                vector_representation=vector_rep,
                morphological_weight=self._calculate_morphological_weight(features),
                syllable_position=syllable_pos,
                phonological_rules=self._get_phonological_rules(symbol))

        return phonemes,
    def _initialize_harakat_inventory(self) -> Dict[str, HarakatVector]:
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""
        harakat = {}

        # Short Vowels Data - Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©,
    harakat_data = [
            # [symbol, diacritic, vowel_type, ipa, morph_func, synt_func, weight, freq]
            ()
                'a','
                'Ù','
                ShortVowelType.FATHA,
                'a','
                ['past_verb', 'verbal_noun'],'
                ['accusative'],'
                1.0,
                0.234),
            ()
                'i','
                'Ù','
                ShortVowelType.KASRA,
                'i','
                ['feminine', 'active_participle'],'
                ['genitive'],'
                1.0,
                0.189),
            ()
                'u','
                'Ù','
                ShortVowelType.DAMMA,
                'u','
                ['present_verb', 'passive_participle'],'
                ['nominative'],'
                1.0,
                0.145),
            ()
                'an','
                'Ù‹','
                ShortVowelType.FATHATAN,
                'an','
                ['indefinite'],'
                ['nunation', 'accusative'],'
                1.5,
                0.023),
            ()
                'in','
                'Ù','
                ShortVowelType.KASRATAN,
                'in','
                ['indefinite'],'
                ['nunation', 'genitive'],'
                1.5,
                0.015),
            ()
                'un','
                'ÙŒ','
                ShortVowelType.DAMMATAN,
                'un','
                ['indefinite'],'
                ['nunation', 'nominative'],'
                1.5,
                0.012),
            ()
                'sukun','
                'Ù’','
                ShortVowelType.FATHA,
                '','
                ['consonant_cluster'],'
                ['cluster_formation'],'
                0.0,
                0.078),
            ()
                'shadda','
                'Ù‘','
                ShortVowelType.FATHA,
                '','
                ['gemination', 'emphasis'],'
                ['intensification'],'
                2.0,
                0.034),
        ]

        for ()
            symbol,
            diacritic,
            vowel_type,
            ipa,
            morph_func,
            synt_func,
            weight,
            freq) in harakat_data:
            vector_rep = self._create_harakat_vector(vowel_type, weight, freq)
            contextual_rules = self._get_harakat_rules(symbol)

            harakat[diacritic] = HarakatVector()
                symbol=symbol,
                arabic_diacritic=diacritic,
                vowel_type=vowel_type,
                ipa=ipa,
                morphological_function=morph_func,
                syntactic_function=synt_func,
                syllable_weight=weight,
                vector_representation=vector_rep,
                frequency=freq,
                contextual_rules=contextual_rules)

        return harakat,
    def _initialize_phonological_rules(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©""""
        return {
            'assimilation': {'
                'solar_assimilation': {'
                    'rule': 'al + solar_consonant â†’ a + solar_consonant + solar_consonant','
                    'solar_letters': ['
                        'Øª','
                        'Ø«','
                        'Ø¯','
                        'Ø°','
                        'Ø±','
                        'Ø²','
                        'Ø³','
                        'Ø´','
                        'Øµ','
                        'Ø¶','
                        'Ø·','
                        'Ø¸','
                        'Ù„','
                        'Ù†','
                    ],
                    'examples': {'Ø§Ù„Ø´Ù…Ø³': 'Ø§Ø´Ù‘Ù…Ø³', 'Ø§Ù„ØªØ±Ø§Ø¨': 'Ø§ØªÙ‘Ø±Ø§Ø¨'},'
                    'confidence': 0.98,'
                },
                'nasal_assimilation': {'
                    'rule': 'n + consonant â†’ consonant assimilation','
                    'examples': {'Ù…Ù†Ø¨Ø±': 'Ù…Ù…Ø¨Ø±', 'Ø¥Ù†ÙƒØ§Ø±': 'Ø¥ÙƒÙƒØ§Ø±'},'
                    'confidence': 0.85,'
                },
            },
            'deletion': {'
                'vowel_deletion': {'
                    'rule': 'short_vowel â†’ âˆ… / unstressed','
                    'examples': {'ÙƒØ§ØªØ¨': 'ÙƒØ§ØªØ¨', 'Ù…Ø¯Ø±Ø³Ø©': 'Ù…Ø¯Ø±Ø³Ø©'},'
                    'confidence': 0.75,'
                }
            },
            'insertion': {'
                'epenthesis': {'
                    'rule': 'âˆ… â†’ vowel / consonant_cluster','
                    'examples': {'ÙƒØªØ¨': 'ÙƒÙØªÙØ¨', 'Ù‚Ù„Ù…': 'Ù‚ÙÙ„ÙÙ…'},'
                    'confidence': 0.80,'
                }
            },
            'emphasis_spreading': {'
                'rule': '[+emphatic] â†’ [+emphatic] / syllable_domain','
                'emphatic_consonants': ['Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ù‚'],'
                'examples': {'Ø·Ø§Ù„Ø¨': 'tË¤É‘ËlË¤ibË¤', 'ØµØ§Ø¨Ø±': 'sË¤É‘ËbË¤irË¤'},'
                'confidence': 0.92,'
            },
        }

    def _initialize_cv_patterns(self) -> Dict[str, Dict[str, Any]]:
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ù†Ù…Ø§Ø· CV ÙˆØ§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©""""
        return {
            'cv_types': {'
                'V': {'weight': 'light', 'frequency': 0.05, 'examples': ['Ø£', 'Ø¥']},'
                'CV': {'weight': 'light', 'frequency': 0.40, 'examples': ['Ù…Ø§', 'Ù„Ø§']},'
                'CVC': {'weight': 'heavy', 'frequency': 0.30, 'examples': ['Ù…Ù†', 'Ø¥Ù†']},'
                'CVV': {'
                    'weight': 'heavy','
                    'frequency': 0.15,'
                    'examples': ['Ø¬Ø§Ø¡', 'Ù‚Ø§Ù„'],'
                },
                'CVVC': {'
                    'weight': 'superheavy','
                    'frequency': 0.08,'
                    'examples': ['Ø¨Ø§Ø¨', 'Ù†ÙˆØ±'],'
                },
                'CVCC': {'
                    'weight': 'superheavy','
                    'frequency': 0.02,'
                    'examples': ['Ø¨Ù†Øª', 'ÙƒØªØ¨'],'
                },
            },
            'syllable_templates': {'
                'monosyllabic': ['CV', 'CVC', 'CVV'],'
                'disyllabic': ['CV.CV', 'CV.CVC', 'CVC.CV'],'
                'trisyllabic': ['CV.CV.CV', 'CV.CVC.CV', 'CVC.CV.CV'],'
            },
            'stress_rules': {'
                'primary_stress': 'penultimate','
                'secondary_stress': 'initial','
                'weight_sensitive': True,'
            },
        }

    def _setup_engine_integration(self) -> Dict[str, str]:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©""""
        return {
            'syllable_engine': 'nlp.syllable.engine.SyllableEngine','
            'derivation_engine': 'nlp.derivation.engine.DerivationEngine','
            'morphology_engine': 'nlp.morphology.engine.MorphologyEngine','
            'phonological_engine': 'nlp.phonological.engine.PhonologicalEngine','
            'weight_engine': 'nlp.weight.engine.WeightEngine','
            'root_engine': 'nlp.frozen_root.engine.FrozenRootEngine','
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CORE WORD TRACING FUNCTIONALITY - Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ù„ØªØªØ¨Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def trace_word(self, word: str) -> VectorTrace:
        """"
        ØªØªØ¨Ø¹ Ù‡Ø±Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ… Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰,
    Hierarchical Arabic word tracing from phoneme to meaning,
    Args:
            word: Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„,
    Returns:
            VectorTrace: ØªØªØ¨Ø¹ Ù…ØªØ¬Ù‡ Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø©
        """"
        try:
            self.logger.info(f"ğŸ” Starting hierarchical tracing for word: {word}")"

            # Step 1: Phoneme Extraction - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª,
    phonemes = self._extract_phonemes(word)

            # Step 2: Harakat Analysis - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙƒØ§Øª,
    harakat = self._extract_harakat(word)

            # Step 3: CV Segmentation - ØªÙ‚Ø·ÙŠØ¹ CV,
    syllables = self._segment_syllables(word, phonemes, harakat)

            # Step 4: Root Extraction - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±,
    root = self._extract_root(word, phonemes)

            # Step 5: Pattern Recognition - Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ²Ù†,
    pattern = self._recognize_pattern(word, root, syllables)

            # Step 6: Derivation Analysis - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚,
    derivation_type = self._analyze_derivation(word, root, pattern)

            # Step 7: Morphological Status - Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØµØ±ÙÙŠØ©,
    morphological_status = self._determine_morphological_status(word, pattern)

            # Step 8: Syntactic Features - Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ø­ÙˆÙŠØ©,
    syntactic_features = self._extract_syntactic_features(word, harakat)

            # Step 9: Confidence Calculation - Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©,
    confidence = self._calculate_confidence(phonemes, harakat, syllables, root)

            # Step 10: Final Vector Generation - ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ,
    final_vector = self._generate_final_vector()
                phonemes, harakat, syllables, root, pattern
            )

            # Create trace steps,
    tracing_steps = self._create_tracing_steps()
                word, phonemes, harakat, syllables, root, pattern
            )

            trace = VectorTrace()
                word=word,
                phonemes=phonemes,
                harakat=harakat,
                syllables=syllables,
                root=root,
                pattern=pattern,
                derivation_type=derivation_type,
                morphological_status=morphological_status,
                syntactic_features=syntactic_features,
                confidence=confidence,
                final_vector=final_vector,
                tracing_steps=tracing_steps)

            self.logger.info(f"âœ… Hierarchical tracing completed for: {word}")"
            return trace,
    except Exception as e:
            self.logger.error(f"âŒ Error in word tracing: {e}")"
            return self._create_error_trace(word, str(e))

    def _extract_phonemes(self, word: str) -> List[PhonemeVector]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©""""
        phonemes = []

        for char in word:
            if char in self.phoneme_inventory:
                phonemes.append(self.phoneme_inventory[char])
            elif char not in ['Ù', 'Ù', 'Ù', 'Ù‹', 'ÙŒ', 'Ù', 'Ù’', 'Ù‘']:  # Skip diacritics'
                # Handle unknown characters,
    unknown_phoneme = PhonemeVector()
                    symbol=char,
                    arabic_letter=char,
                    ipa=char,
                    phoneme_type=ArabicPhonemeType.CONSONANT,
                    features=['unknown'],'
                    frequency=0.001,
                    vector_representation=[0.0] * 10,
                    morphological_weight=1.0,
                    syllable_position=['onset'],'
                    phonological_rules=[])
                phonemes.append(unknown_phoneme)

        return phonemes,
    def _extract_harakat(self, word: str) -> List[HarakatVector]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©""""
        harakat = []

        for char in word:
            if char in self.harakat_inventory:
                harakat.append(self.harakat_inventory[char])

        # If no diacritics found, infer basic vowels,
    if not harakat:
            # Simple vowel inference based on syllable structure,
    inferred_harakat = self._infer_harakat(word)
            harakat.extend(inferred_harakat)

        return harakat,
    def _segment_syllables()
        self, word: str, phonemes: List[PhonemeVector], harakat: List[HarakatVector]
    ) -> List[CVSegment]:
        """ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ CV""""
        syllables = []

        # Simple CV segmentation algorithm,
    cv_string = self._create_cv_string(phonemes, harakat)
        cv_segments = self._segment_cv_string(cv_string)

        for i, cv_pattern in enumerate(cv_segments):
            weight = self._calculate_syllable_weight(cv_pattern)
            stress = self._determine_stress(i, len(cv_segments), weight)
            position = self._determine_position(i, len(cv_segments))

            # Extract actual phonemes for this syllable,
    onset, nucleus, coda = self._extract_syllable_phonemes()
                cv_pattern, phonemes, harakat, i
            )

            syllable = CVSegment()
                onset=onset,
                nucleus=nucleus,
                coda=coda,
                cv_pattern=cv_pattern,
                syllable_weight=weight,
                stress=stress,
                position_in_word=position,
                vector_representation=self._create_syllable_vector()
                    cv_pattern, weight, stress
                ))
            syllables.append(syllable)

        return syllables,
    def _extract_root()
        self, word: str, phonemes: List[PhonemeVector]
    ) -> Tuple[str, ...]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©""""
        # Simple root extraction (trilateral focus)
        consonants = [
            p.arabic_letter,
    for p in phonemes,
    if p.phoneme_type == ArabicPhonemeType.CONSONANT
        ]

        # Remove common prefixes and suffixes,
    filtered_consonants = self._filter_root_consonants(consonants, word)

        # Take first 3 consonants as potential root,
    if len(filtered_consonants) >= 3:
            return tuple(filtered_consonants[:3])
        elif len(filtered_consonants) == 2:
            return tuple(filtered_consonants + [''])  # Defective root'
        else:
            return tuple(consonants[:3] if len(consonants) >= 3 else consonants)

    def _recognize_pattern()
        self, word: str, root: Tuple[str, ...], syllables: List[CVSegment]
    ) -> str:
        """Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ""""
        # Create CV pattern string,
    cv_pattern = ''.join([s.cv_pattern for s in syllables])'

        # Common Arabic patterns,
    pattern_map = {
            'CVC': 'ÙÙØ¹Ù’','
            'CVCV': 'ÙÙØ¹ÙÙ„Ù','
            'CVCVC': 'ÙÙØ§Ø¹ÙÙ„','
            'CVCVCV': 'ÙÙØ¹ÙÙ‘Ø§Ù„ÙØ©','
            'CVCVCVC': 'Ù…ÙÙÙ’Ø¹ÙÙˆÙ„','
            'CVCVCVCV': 'ÙÙØ¹ÙÙ‘Ø§Ù„ÙÙŠÙÙ‘Ø©','
        }

        return pattern_map.get(cv_pattern, f'Pattern_{cv_pattern}')'

    def _analyze_derivation()
        self, word: str, root: Tuple[str, ...], pattern: str
    ) -> str:
        """ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚""""
        if pattern.startswith('ÙÙØ¹ÙÙ„'):'
            return 'basic_verb''
        elif 'ÙÙØ§Ø¹ÙÙ„' in pattern:'
            return 'active_participle''
        elif 'Ù…ÙÙÙ’Ø¹ÙÙˆÙ„' in pattern:'
            return 'passive_participle''
        elif pattern.startswith('Ù…Ù'):'
            return 'participial_noun''
        elif pattern.endswith('Ø©'):'
            return 'feminine_noun''
        else:
            return 'derived_form''

    def _determine_morphological_status(self, word: str, pattern: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØµØ±ÙÙŠØ© (Ù…Ø¹Ø±Ø¨/Ù…Ø¨Ù†ÙŠ)""""
        # Simple heuristics,
    if word.endswith(('Øª', 'Ù†', 'ÙˆØ§', 'ØªÙ…', 'ØªÙ†')):'
            return 'mabni'  # Ù…Ø¨Ù†ÙŠ'
        elif any(word.endswith(ending) for ending in ['ÙŒ', 'Ù‹', 'Ù', 'Ù', 'Ù', 'Ù']):'
            return 'murab'  # Ù…Ø¹Ø±Ø¨'
        else:
            return 'murab'  # Default to murab'

    def _extract_syntactic_features()
        self, word: str, harakat: List[HarakatVector]
    ) -> Dict[str, str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ø­ÙˆÙŠØ©""""
        features = {}

        # Gender determination,
    if word.endswith(('Ø©', 'Ø§Ø¡', 'Ù‰')):'
            features['gender'] = 'feminine''
        else:
            features['gender'] = 'masculine''

        # Number determination,
    if word.endswith(('ÙˆÙ†', 'ÙŠÙ†')):'
            features['number'] = 'plural''
        elif word.endswith(('Ø§Ù†', 'ÙŠÙ†')):'
            features['number'] = 'dual''
        else:
            features['number'] = 'singular''

        # Case determination from harakat,
    for h in harakat:
            if 'nominative' in h.syntactic_function:'
                features['case'] = 'nominative''
            elif 'accusative' in h.syntactic_function:'
                features['case'] = 'accusative''
            elif 'genitive' in h.syntactic_function:'
                features['case'] = 'genitive''

        # Definiteness,
    if word.startswith('Ø§Ù„'):'
            features['definiteness'] = 'definite''
        else:
            features['definiteness'] = 'indefinite''

        return features

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # UTILITY METHODS - Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _create_phoneme_vector()
        self, features: List[str], frequency: float
    ) -> List[float]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡ Ø§Ù„ÙÙˆÙ†ÙŠÙ…""""
        # Create 10-dimensional vector based on features and frequency,
    vector = [0.0] * 10

        # Feature encoding,
    feature_map = {
            'voiced': 0,'
            'voiceless': 1,'
            'stop': 2,'
            'fricative': 3,'
            'nasal': 4,'
            'liquid': 5,'
            'emphatic': 6,'
            'pharyngeal': 7,'
            'bilabial': 8,'
            'dental': 9,'
        }

        for feature in features:
            if feature in feature_map:
                vector[feature_map[feature]] = 1.0

        # Add frequency as weight,
    vector = [v * frequency for v in vector]

        return vector,
    def _create_harakat_vector()
        self, vowel_type: ShortVowelType, weight: float, frequency: float
    ) -> List[float]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡ Ø§Ù„Ø­Ø±ÙƒØ©""""
        vector = [0.0] * 8

        # Vowel type encoding,
    if vowel_type == ShortVowelType.FATHA:
            vector[0] = 1.0,
    elif vowel_type == ShortVowelType.KASRA:
            vector[1] = 1.0,
    elif vowel_type == ShortVowelType.DAMMA:
            vector[2] = 1.0,
    elif vowel_type == ShortVowelType.FATHATAN:
            vector[3] = 1.0,
    elif vowel_type == ShortVowelType.KASRATAN:
            vector[4] = 1.0,
    elif vowel_type == ShortVowelType.DAMMATAN:
            vector[5] = 1.0

        # Weight and frequency,
    vector[6] = weight,
    vector[7] = frequency,
    return vector,
    def _calculate_morphological_weight(self, features: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ Ù„Ù„ÙÙˆÙ†ÙŠÙ…""""
        weight = 1.0,
    if 'emphatic' in features:'
            weight += 0.5,
    if 'pharyngeal' in features:'
            weight += 0.3,
    if 'liquid' in features:'
            weight += 0.2,
    return weight,
    def _get_phonological_rules(self, symbol: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„ÙÙˆÙ†ÙŠÙ…""""
        rules = []

        if symbol in [
            't','
            'th','
            'd','
            'dh','
            'r','
            'z','
            's','
            'sh','
            'S','
            'D','
            'T','
            'Z','
            'l','
            'n','
        ]:
            rules.append('solar_assimilation')'

        if symbol in ['S', 'D', 'T', 'Z', 'q']:'
            rules.append('emphasis_spreading')'

        if symbol in ['w', 'y']:'
            rules.append('glide_formation')'

        return rules,
    def _get_harakat_rules(self, symbol: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø­Ø±ÙƒØ©""""
        rules = []

        if symbol in ['an', 'in', 'un']:'
            rules.append('nunation')'
            rules.append('case_marking')'

        if symbol == 'sukun':'
            rules.append('cluster_formation')'
            rules.append('syllable_closure')'

        if symbol == 'shadda':'
            rules.append('gemination')'
            rules.append('emphasis')'

        return rules,
    def _create_cv_string()
        self, phonemes: List[PhonemeVector], harakat: List[HarakatVector]
    ) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© CV Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª ÙˆØ§Ù„Ø­Ø±ÙƒØ§Øª""""
        cv_string = """

        for phoneme in phonemes:
            if phoneme.phoneme_type == ArabicPhonemeType.CONSONANT:
                cv_string += "C""

        # Add vowels based on harakat,
    for harakat_item in harakat:
            if harakat_item.symbol not in ['sukun', 'shadda']:'
                cv_string += "V""

        return cv_string,
    def _segment_cv_string(self, cv_string: str) -> List[str]:
        """ØªÙ‚Ø·ÙŠØ¹ Ø³Ù„Ø³Ù„Ø© CV Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹""""
        segments = []
        current_segment = """

        i = 0,
    while i < len(cv_string):
            if cv_string[i] == 'C':'
                current_segment += 'C''
            elif cv_string[i] == 'V':'
                current_segment += 'V''
                # End segment after vowel (simple rule)
                segments.append(current_segment)
                current_segment = """

            i += 1,
    if current_segment:
            segments.append(current_segment)

        return segments if segments else ['CV']'

    def _calculate_syllable_weight(self, cv_pattern: str) -> str:
        """Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„Ù…Ù‚Ø·Ø¹""""
        if cv_pattern in ['V', 'CV']:'
            return 'light''
        elif cv_pattern in ['CVC', 'CVV']:'
            return 'heavy''
        else:
            return 'superheavy''

    def _determine_stress()
        self, position: int, total_syllables: int, weight: str
    ) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø¨Ø±""""
        # Penultimate stress rule,
    if total_syllables > 1 and position == total_syllables - 2:
            return True,
    elif total_syllables == 1:
            return True,
    else:
            return False,
    def _determine_position(self, index: int, total: int) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù…Ù‚Ø·Ø¹ ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©""""
        if index == 0:
            return 'initial''
        elif index == total - 1:
            return 'final''
        else:
            return 'medial''

    def _extract_syllable_phonemes()
        self,
        cv_pattern: str,
        phonemes: List[PhonemeVector],
        harakat: List[HarakatVector],
        syllable_index: int) -> Tuple[List[str], List[str], List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ù„Ù…Ù‚Ø·Ø¹ Ù…Ø­Ø¯Ø¯""""
        # Simplified extraction,
    onset = []
        nucleus = []
        coda = []

        # Basic segmentation based on CV pattern,
    if 'C' in cv_pattern:'
            if syllable_index < len(phonemes):
                onset.append(phonemes[syllable_index].arabic_letter)

        if 'V' in cv_pattern:'
            if syllable_index < len(harakat):
                nucleus.append(harakat[syllable_index].symbol)

        return onset, nucleus, coda,
    def _create_syllable_vector()
        self, cv_pattern: str, weight: str, stress: bool
    ) -> List[float]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡ Ø§Ù„Ù…Ù‚Ø·Ø¹""""
        vector = [0.0] * 6

        # CV pattern encoding,
    if cv_pattern == 'CV':'
            vector[0] = 1.0,
    elif cv_pattern == 'CVC':'
            vector[1] = 1.0,
    elif cv_pattern == 'CVV':'
            vector[2] = 1.0

        # Weight encoding,
    if weight == 'light':'
            vector[3] = 1.0,
    elif weight == 'heavy':'
            vector[4] = 2.0

        # Stress encoding,
    if stress:
            vector[5] = 1.0,
    return vector,
    def _filter_root_consonants(self, consonants: List[str], word: str) -> List[str]:
        """ØªØµÙÙŠØ© Ø§Ù„ØµÙˆØ§Ù…Øª Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±""""
        # Remove common prefixes and suffixes,
    filtered = consonants.copy()

        # Remove definite article,
    if word.startswith('Ø§Ù„') and len(len(filtered)  > 2) > 2:'
            filtered = filtered[1:]  # Remove the 'Ù„' from 'Ø§Ù„''

        # Remove common prefixes,
    prefixes = ['Ù…', 'Øª', 'Ø³', 'Ø£', 'ÙŠ', 'Ù†']'
        if filtered and filtered[0] in prefixes and len(len(filtered)  > 3) > 3:
            filtered = filtered[1:]

        # Remove common suffixes,
    suffixes = ['Øª', 'Ù‡', 'Ø©', 'Ù†', 'ÙˆÙ†', 'ÙŠÙ†']'
        if filtered and len(len(filtered)  > 3) > 3:
            for suffix in suffixes:
                if word.endswith(suffix):
                    filtered = filtered[:-1]
                    break,
    return filtered,
    def _infer_harakat(self, word: str) -> List[HarakatVector]:
        """Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©""""
        # Simple harakat inference,
    inferred = []

        # Add basic fatha for each consonant (simplified)
        consonant_count = len([c for c in word if c in self.phoneme_inventory])

        for _ in range(max(1, consonant_count - 1)):
            if 'Ù' in self.harakat_inventory:'
                inferred.append(self.harakat_inventory['Ù'])'

        return inferred,
    def _calculate_confidence()
        self,
        phonemes: List[PhonemeVector],
        harakat: List[HarakatVector],
        syllables: List[CVSegment],
        root: Tuple[str, ...]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©""""
        confidence = 0.0

        # Phoneme confidence,
    if phonemes:
            phoneme_conf = sum(p.frequency for p in phonemes) / len(phonemes)
            confidence += phoneme_conf * 0.3

        # Harakat confidence,
    if harakat:
            harakat_conf = sum(h.frequency for h in harakat) / len(harakat)
            confidence += harakat_conf * 0.2

        # Syllable confidence,
    if syllables:
            confidence += 0.3

        # Root confidence,
    if root and len([r for r in root if r]):
            confidence += 0.2,
    return min(confidence, 1.0)

    def _generate_final_vector()
        self,
        phonemes: List[PhonemeVector],
        harakat: List[HarakatVector],
        syllables: List[CVSegment],
        root: Tuple[str, ...],
        pattern: str) -> List[float]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ""""
        final_vector = []

        # Concatenate phoneme vectors,
    for phoneme in phonemes[:5]:  # Limit to first 5 phonemes,
    final_vector.extend(phoneme.vector_representation)

        # Pad to ensure fixed size,
    while len(final_vector) < 50:
            final_vector.append(0.0)

        # Add harakat vectors,
    for harakat_item in harakat[:3]:  # Limit to first 3 harakat,
    final_vector.extend(harakat_item.vector_representation)

        # Pad to ensure fixed size,
    while len(final_vector) < 74:
            final_vector.append(0.0)

        # Add syllable information,
    final_vector.extend([len(syllables), len(root), len(pattern)])

        return final_vector[:100]  # Fixed size vector,
    def _create_tracing_steps()
        self,
        word: str,
        phonemes: List[PhonemeVector],
        harakat: List[HarakatVector],
        syllables: List[CVSegment],
        root: Tuple[str, ...],
        pattern: str) -> List[Dict[str, Any]]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØªØ¨Ø¹""""
        steps = [
            {
                'step': 1,'
                'name': 'phoneme_extraction','
                'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª','
                'input': word,'
                'output': [p.arabic_letter for p in phonemes],'
                'confidence': 0.95,'
            },
            {
                'step': 2,'
                'name': 'harakat_analysis','
                'description': 'ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙƒØ§Øª','
                'input': word,'
                'output': [h.arabic_diacritic for h in harakat],'
                'confidence': 0.85,'
            },
            {
                'step': 3,'
                'name': 'syllable_segmentation','
                'description': 'ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹','
                'input': [p.arabic_letter for p in phonemes],'
                'output': [s.cv_pattern for s in syllables],'
                'confidence': 0.90,'
            },
            {
                'step': 4,'
                'name': 'root_extraction','
                'description': 'Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±','
                'input': [p.arabic_letter for p in phonemes],'
                'output': list(root),'
                'confidence': 0.80,'
            },
            {
                'step': 5,'
                'name': 'pattern_recognition','
                'description': 'Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ²Ù†','
                'input': list(root),'
                'output': pattern,'
                'confidence': 0.75,'
            },
        ]

        return steps,
    def _create_error_trace(self, word: str, error: str) -> VectorTrace:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØªØ¨Ø¹ Ø®Ø·Ø£""""
        return VectorTrace()
            word=word,
            phonemes=[],
            harakat=[],
            syllables=[],
            root=('', '', ''),'
            pattern='ERROR','
            derivation_type='error','
            morphological_status='error','
            syntactic_features={'error': error},'
            confidence=0.0,
            final_vector=[0.0] * 100,
            tracing_steps=[
                {
                    'step': 0,'
                    'name': 'error','
                    'description': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {error}','
                    'input': word,'
                    'output': 'ERROR','
                    'confidence': 0.0,'
                }
            ])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # API INTERFACE METHODS - Ø·Ø±Ù‚ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def batch_trace_words(self, words: List[str]) -> List[VectorTrace]:
        """ØªØªØ¨Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª""""
        traces = []

        for word in words:
            trace = self.trace_word(word)
            traces.append(trace)

        return traces,
    def get_phoneme_inventory(self) -> Dict[str, PhonemeVector]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª""""
        return self.phoneme_inventory,
    def get_harakat_inventory(self) -> Dict[str, HarakatVector]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø­Ø±ÙƒØ§Øª""""
        return self.harakat_inventory,
    def get_phonological_rules(self) -> Dict[str, Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©""""
        return self.phonological_rules,
    def analyze_text_hierarchy(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù‡Ø±Ù…ÙŠ Ù„Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„""""
        words = text.split()
        word_traces = self.batch_trace_words(words)

        return {
            'text': text,'
            'word_count': len(words),'
            'word_traces': word_traces,'
            'overall_confidence': ()'
                sum(trace.confidence for trace in word_traces) / len(word_traces)
                if word_traces,
    else 0.0
            ),
            'phoneme_distribution': self._analyze_phoneme_distribution(word_traces),'
            'syllable_patterns': self._analyze_syllable_patterns(word_traces),'
            'root_families': self._analyze_root_families(word_traces),'
        }

    def _analyze_phoneme_distribution()
        self, traces: List[VectorTrace]
    ) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª""""
        distribution = defaultdict(int)

        for trace in traces:
            for phoneme in trace.phonemes:
                distribution[phoneme.arabic_letter] += 1,
    return dict(distribution)

    def _analyze_syllable_patterns(self, traces: List[VectorTrace]) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹""""
        patterns = defaultdict(int)

        for trace in traces:
            for syllable in trace.syllables:
                patterns[syllable.cv_pattern] += 1,
    return dict(patterns)

    def _analyze_root_families()
        self, traces: List[VectorTrace]
    ) -> Dict[Tuple[str, ...], List[str]]:
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ø¦Ù„Ø§Øª Ø§Ù„Ø¬Ø°ÙˆØ±""""
        families = defaultdict(list)

        for trace in traces:
            if trace.root and any(trace.root):
                families[trace.root].append(trace.word)

        return dict(families)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTING AND DEMONSTRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØ­Ø¯""""
    print("ğŸ¯ HIERARCHICAL ARABIC WORD TRACING ENGINE")"
    print("=" * 70)"

    # Initialize the engine,
    engine = PhonologyCoreEngine()

    # Test words,
    test_words = [
        "ÙƒØªØ¨",  # wrote"
        "Ù…Ø¯Ø±Ø³Ø©",  # school"
        "Ø§Ù„Ø·Ø§Ù„Ø¨",  # the student"
        "ÙŠØ¯Ø±Ø³ÙˆÙ†",  # they study"
        "Ø§Ù„ÙƒØªØ§Ø¨",  # the book"
        "Ù…ÙƒØªØ¨Ø©",  # library"
    ]

    print("\nğŸ“Š TESTING HIERARCHICAL WORD TRACING:")"
    print(" " * 50)"

    for word in test_words:
        print(f"\nğŸ” Analyzing: {word}")"
        trace = engine.trace_word(word)

        print(f"   ğŸ“± Phonemes: {[p.arabic_letter for p} in trace.phonemes]}")"
        print(f"   ğŸµ Harakat: {[h.arabic_diacritic for h} in trace.harakat]}")"
        print(f"   ğŸ—ï¸ Syllables: {[s.cv_pattern for s} in trace.syllables]}")"
        print(f"   ğŸŒ± Root: {trace.root}")"
        print(f"   âš–ï¸ Pattern: {trace.pattern}")"
        print(f"   ğŸ”„ Derivation: {trace.derivation_type}")"
        print(f"   ğŸ“ˆ Confidence: {trace.confidence:.2f}")"

    # Test text analysis,
    test_text = "ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©""
    print(f"\nğŸ“ TESTING TEXT HIERARCHY: {test_text}")"
    print(" " * 50)"

    analysis = engine.analyze_text_hierarchy(test_text)
    print(f"   ğŸ“Š Word Count: {analysis['word_count']}")'"
    print(f"   ğŸ“ˆ Overall Confidence: {analysis['overall_confidence']:.2f}")'"
    print()
        f"   ğŸ”¤ Phoneme Distribution: {dict(list(analysis['phoneme_distribution'].items())[:5])}"'"
    )
    print(f"   ğŸ—ï¸ Syllable Patterns: {analysis['syllable_patterns']}")'"
    print(f"   ğŸŒ± Root Families: {analysis['root_families']}")'"

    print("\nâœ… HIERARCHICAL ARABIC WORD TRACING ENGINE - COMPLETE!")"
    print("=" * 70)"


if __name__ == "__main__":"
    main()

