#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Relative Pronouns Deep Learning Models - Simplified
========================================================
Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Ù…Ø¨Ø³Ø·,
    Simplified deep learning models for Arabic relative pronouns classification,
    without external audio processing dependencies.

Author: Arabic NLP Expert Team - GitHub Copilot,
    Version: 1.0.0 - SIMPLIFIED DEEP LEARNING,
    Date: 2025-07-24,
    Encoding: UTF 8
"""

import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader
    import numpy as np
    import logging
    from typing import Dict, List, Any, Tuple,
    logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RELATIVE PRONOUNS CLASSIFICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©,
    RELATIVE_PRONOUNS = {
    0: "Ø§Ù„Ø°ÙŠ",
    1: "Ø§Ù„ØªÙŠ",
    2: "Ø§Ù„Ø°Ù‰",
    3: "Ø§Ù„Ù„ØªÙŠ",
    4: "Ø§Ù„Ù„Ø°Ø§Ù†",
    5: "Ø§Ù„Ù„Ø°ÙŠÙ†",
    6: "Ø§Ù„Ù„ØªØ§Ù†",
    7: "Ø§Ù„Ù„ØªÙŠÙ†",
    8: "Ø§Ù„Ø°ÙŠÙ†",
    9: "Ø§Ù„Ù„Ø§ØªÙŠ",
    10: "Ø§Ù„Ù„Ø§Ø¦ÙŠ",
    11: "Ø§Ù„Ù„ÙˆØ§ØªÙŠ",
    12: "Ù…ÙÙ†",
    13: "Ù…Ø§",
    14: "Ø£ÙŠ",
    15: "Ø°Ùˆ",
    16: "Ø°Ø§Øª",
}

# Ø¹ÙƒØ³ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ø¨Ø­Ø«,
    PRONOUNS_TO_ID = {v: k for k, v in RELATIVE_PRONOUNS.items()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHONETIC PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounPhoneticProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__(self):

        # Ù…Ø®Ø²ÙˆÙ† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©,
    self.phonemes = [
            # Ø£ØµÙˆØ§Øª ØµØ§Ù…ØªØ©
    'b',
    't',
    'th',
    'j',
    'h',
    'kh',
    'd',
    'dh',
    'r',
    'z',
    's',
    'sh',
    'á¹£',
    'á¸',
    'á¹­',
    'áº“',
    'Ê•',
    'gh',
    'f',
    'q',
    'k',
    'l',
    'm',
    'n',
    'w',
    'y',
            # Ø£ØµÙˆØ§Øª ØµØ§Ø¦ØªØ©
    'a',
    'i',
    'u',
    'aa',
    'ii',
    'uu',
    'ay',
    'aw',
            # Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
    'Ê”',
    '<PAD>',
    '<UNK>',
    ]

    self.phoneme_to_idx = {p: idx for idx, p in enumerate(self.phonemes)}
    self.idx_to_phoneme = {idx: p for idx, p in enumerate(self.phonemes)}
    self.vocab_size = len(self.phonemes)

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª,
    self.arabic_char_to_phoneme = {
    'Ø§': 'aa',
    'Ø£': 'a',
    'Ø¥': 'i',
    'Ø¢': 'aa',
    'Ø¨': 'b',
    'Øª': 't',
    'Ø«': 'th',
    'Ø¬': 'j',
    'Ø­': 'h',
    'Ø®': 'kh',
    'Ø¯': 'd',
    'Ø°': 'dh',
    'Ø±': 'r',
    'Ø²': 'z',
    'Ø³': 's',
    'Ø´': 'sh',
    'Øµ': 'á¹£',
    'Ø¶': 'á¸',
    'Ø·': 'á¹­',
    'Ø¸': 'áº“',
    'Ø¹': 'Ê•',
    'Øº': 'gh',
    'Ù': 'f',
    'Ù‚': 'q',
    'Ùƒ': 'k',
    'Ù„': 'l',
    'Ù…': 'm',
    'Ù†': 'n',
    'Ù‡': 'h',
    'Ùˆ': 'w',
    'ÙŠ': 'y',
    'Ù‰': 'aa',
    'Ø©': 'h',
    'Ø¡': 'Ê”',
    'Ù': 'a',
    'Ù': 'i',
    'Ù': 'u',
    'Ù‹': 'an',
    'Ù': 'in',
    'ÙŒ': 'un',
    'Ù’': '',  # Ø³ÙƒÙˆÙ†
    }

    logger.info(f"ğŸ“š ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª: {self.vocab_size} ÙÙˆÙ†ÙŠÙ…")

    def syllables_to_phonemes(self, syllables: List[str]) -> List[str]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª"""

    phonemes = []

        for syllable in syllables:
            for char in syllable:
                if char in self.arabic_char_to_phoneme:
    phoneme = self.arabic_char_to_phoneme[char]
                    if phoneme:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙØ§Ø±ØºØ©,
    phonemes.append(phoneme)

    return phonemes,
    def encode_phonemes(self, phonemes: List[str]) -> List[int]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…"""

    return [
    self.phoneme_to_idx.get(p, self.phoneme_to_idx['<UNK>']) for p in phonemes
    ]

    def pad_sequence(self, sequence: List[int], max_length: int) -> List[int]:
    """Ø¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ Ù„Ù„ØªØ³Ù„Ø³Ù„"""

    pad_token = self.phoneme_to_idx['<PAD>']

        if len(sequence) >= max_length:
    return sequence[:max_length]
        else:
    return sequence + [pad_token] * (max_length - len(sequence))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATASET CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    data: List[Tuple[List[str], int]],
    processor: RelativePronounPhoneticProcessor, max_length: int = 20):
    self.data = data,
    self.processor = processor,
    self.max_length = max_length,
    def __len__(self):
    def __len__(self):
    return len(self.data)

    def __getitem__(self, idx):
    def __getitem__(self, idx):
    syllables, label = self.data[idx]

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª,
    phonemes = self.processor.syllables_to_phonemes(syllables)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙˆØ¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ,
    encoded = self.processor.encode_phonemes(phonemes)
    padded = self.processor.pad_sequence(encoded, self.max_length)

    return torch.tensor(padded, dtype=torch.long), torch.tensor()
    label, dtype=torch.long
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEP LEARNING MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounLSTM(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    vocab_size: int,
    embed_dim: int,
    hidden_size: int,
    num_classes: int,
    num_layers: int = 2, dropout: float = 0.1):
    super().__init__()

    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
    self.lstm = nn.LSTM()
    embed_dim,
    hidden_size,
    num_layers,
    batch_first=True,
    bidirectional=True,
    dropout=dropout)
    self.dropout = nn.Dropout(dropout)
    self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
    def forward(self, x):
        # x: (batch_size, seq_len)
    embedded = self.embedding(x)

        # LSTM,
    lstm_out, (hidden, cell) = self.lstm(embedded)

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ø®Ø± Ø¥Ø®Ø±Ø§Ø¬,
    final_output = lstm_out[:  1, :]

        # Ø§Ù„ØªØµÙ†ÙŠÙ,
    output = self.dropout(final_output)
    output = self.fc(output)

    return output,
    class RelativePronounGRU(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ GRU Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    vocab_size: int,
    embed_dim: int,
    hidden_size: int,
    num_classes: int,
    num_layers: int = 2, dropout: float = 0.1):
    super().__init__()

    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
    self.gru = nn.GRU()
    embed_dim,
    hidden_size,
    num_layers,
    batch_first=True,
    bidirectional=True,
    dropout=dropout)
    self.dropout = nn.Dropout(dropout)
    self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
    def forward(self, x):
    embedded = self.embedding(x)
    gru_out, hidden = self.gru(embedded)

        # Ø¢Ø®Ø± Ø¥Ø®Ø±Ø§Ø¬,
    final_output = gru_out[:  1, :]

    output = self.dropout(final_output)
    output = self.fc(output)

    return output,
    class RelativePronounTransformer(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Transformer Ù…Ø¨Ø³Ø· Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    vocab_size: int,
    d_model: int,
    nhead: int,
    num_encoder_layers: int,
    num_classes: int,
    dim_feedforward: int = 2048, dropout: float = 0.1):
    super().__init__()

    self.d_model = d_model,
    self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
    self.pos_encoding = PositionalEncoding(d_model, dropout)

    encoder_layer = nn.TransformerEncoderLayer()
    d_model=d_model,
    nhead=nhead,
    dim_feedforward=dim_feedforward,
    dropout=dropout,
    batch_first=True)

    self.transformer = nn.TransformerEncoder(encoder_layer, num_encoder_layers)
    self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, x):
    def forward(self, x):
        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª,
    embedded = self.embedding(x) * np.sqrt(self.d_model)
    embedded = self.pos_encoding(embedded)

        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ù„Ù„Ø­Ø´Ùˆ,
    padding_mask = x == 0

        # Transformer,
    transformed = self.transformer(embedded, src_key_padding_mask=padding_mask)

        # ØªØ¬Ù…ÙŠØ¹ (Ù…ØªÙˆØ³Ø· Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø´Ùˆ)
    mask = (~padding_mask).float().unsqueeze( 1)
    pooled = (transformed * mask).sum(dim=1) / mask.sum(dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ,
    output = self.classifier(pooled)

    return output,
    class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ Ù„Ù„Ù€ Transformer"""

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):

    super().__init__()
    self.dropout = nn.Dropout(p=dropout)

    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp()
    torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
    )

    pe[: 0::2] = torch.sin(position * div_term)
    pe[: 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0).transpose(0, 1)

    self.register_buffer('pe', pe)

    def forward(self, x):
    def forward(self, x):
    x = x + self.pe[: x.size(1), :].transpose(0, 1)
    return self.dropout(x)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounTrainer:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__(self, model, device='cpu', learning_rate=0.001):

    self.model = model.to(device)
    self.device = device,
    self.criterion = nn.CrossEntropyLoss()
    self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    self.train_losses = []
    self.train_accuracies = []

    def train_epoch(self, dataloader):
    """ØªØ¯Ø±ÙŠØ¨ Ø­Ù‚Ø¨Ø© ÙˆØ§Ø­Ø¯Ø©"""

    self.model.train()
    total_loss = 0,
    correct = 0,
    total = 0,
    for inputs, labels in dataloader:
    inputs, labels = inputs.to(self.device), labels.to(self.device)

    self.optimizer.zero_grad()
    outputs = self.model(inputs)
    loss = self.criterion(outputs, labels)
    loss.backward()
    self.optimizer.step()

    total_loss += loss.item()
    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / len(dataloader)
    accuracy = 100.0 * correct / total,
    self.train_losses.append(avg_loss)
    self.train_accuracies.append(accuracy)

    return avg_loss, accuracy,
    def evaluate(self, dataloader):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

    self.model.eval()
    correct = 0,
    total = 0,
    predictions = []
    true_labels = []

        with torch.no_grad():
            for inputs, labels in dataloader:
    inputs, labels = inputs.to(self.device), labels.to(self.device)
    outputs = self.model(inputs)
    _, predicted = outputs.max(1)

    total += labels.size(0)
    correct += predicted.eq(labels).sum().item()

    predictions.extend(predicted.cpu().numpy())
    true_labels.extend(labels.cpu().numpy())

    accuracy = 100.0 * correct / total,
    return accuracy, predictions, true_labels,
    def train(self, train_dataloader, epochs=20):
    """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„"""

    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù€ {epochs} Ø­Ù‚Ø¨Ø©")

        for epoch in range(epochs):
    train_loss, train_acc = self.train_epoch(train_dataloader)

            if epoch % 5 == 0:
    logger.info()
    f"Ø­Ù‚Ø¨Ø© {epoch+1}/{epochs}: Loss={train_loss:.4f}, Accuracy={train_acc:.2f}%"
    )

    logger.info("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    return train_acc


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFERENCE ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounInference:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self, model, processor: RelativePronounPhoneticProcessor, device='cpu'
    ):
    self.model = model.to(device)
    self.processor = processor,
    self.device = device,
    self.model.eval()

    def predict(self, syllables: List[str], max_length: int = 20) -> Dict[str, Any]:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª,
    phonemes = self.processor.syllables_to_phonemes(syllables)
    encoded = self.processor.encode_phonemes(phonemes)
    padded = self.processor.pad_sequence(encoded, max_length)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ tensor,
    input_tensor = torch.tensor([padded], dtype=torch.long).to(self.device)

        with torch.no_grad():
    outputs = self.model(input_tensor)
    probabilities = F.softmax(outputs, dim=1)
    predicted_class = outputs.argmax(1).item()
    confidence = probabilities.max().item()

    return {
    'predicted_pronoun': RELATIVE_PRONOUNS[predicted_class],
    'predicted_id': predicted_class,
    'confidence': confidence,
    'input_syllables': syllables,
    'phonemes': phonemes,
    'top_3_predictions': self._get_top_predictions(probabilities[0], 3),
    }

    def _get_top_predictions(self, probabilities, k=3):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ k ØªÙ†Ø¨Ø¤Ø§Øª"""

    top_k = torch.topk(probabilities, k)
    predictions = []

        for i in range(k):
    idx = top_k.indices[i].item()
    prob = top_k.values[i].item()
    predictions.append({'pronoun': RELATIVE_PRONOUNS[idx], 'probability': prob})

    return predictions


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION FUNCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def demonstrate_relative_pronoun_models():
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""

    print("ğŸ§  Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 60)

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª,
    processor = RelativePronounPhoneticProcessor()

    # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ (Ù…Ù‚Ø§Ø·Ø¹ØŒ Ù…Ø¹Ø±Ù Ø§Ù„ÙØ¦Ø©)
    training_data = [
        # ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ"], 0),  # Ø§Ù„Ø°ÙŠ
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ"], 0),  # ØªÙƒØ±Ø§Ø±
    (["Ø§Ù„Ù’", "ØªÙÙŠ"], 1),  # Ø§Ù„ØªÙŠ
    (["Ø§Ù„Ù’", "ØªÙÙŠ"], 1),  # ØªÙƒØ±Ø§Ø±
    (["Ø§Ù„Ù’", "Ø°ÙÙ‰"], 2),  # Ø§Ù„Ø°Ù‰
    (["Ø§Ù„Ù’", "Ù„Ù", "ØªÙÙŠ"], 3),  # Ø§Ù„Ù„ØªÙŠ
    (["Ø§Ù„Ù’", "Ù„Ù", "Ø°ÙØ§", "Ù†Ù"], 4),  # Ø§Ù„Ù„Ø°Ø§Ù†
    (["Ø§Ù„Ù’", "Ù„Ù", "Ø°ÙÙŠÙ’", "Ù†Ù"], 5),  # Ø§Ù„Ù„Ø°ÙŠÙ†
    (["Ø§Ù„Ù’", "Ù„Ù", "ØªÙØ§", "Ù†Ù"], 6),  # Ø§Ù„Ù„ØªØ§Ù†
    (["Ø§Ù„Ù’", "Ù„Ù", "ØªÙÙŠÙ’", "Ù†Ù"], 7),  # Ø§Ù„Ù„ØªÙŠÙ†
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ", "Ù†Ù"], 8),  # Ø§Ù„Ø°ÙŠÙ†
    (["Ø§Ù„Ù’", "Ù„ÙØ§", "ØªÙÙŠ"], 9),  # Ø§Ù„Ù„Ø§ØªÙŠ
    (["Ø§Ù„Ù’", "Ù„ÙØ§Ø¦ÙÙŠ"], 10),  # Ø§Ù„Ù„Ø§Ø¦ÙŠ
    (["Ø§Ù„Ù’", "Ù„Ù", "ÙˆÙØ§", "ØªÙÙŠ"], 11),  # Ø§Ù„Ù„ÙˆØ§ØªÙŠ
    (["Ù…ÙÙ†Ù’"], 12),  # Ù…ÙÙ†
    (["Ù…ÙØ§"], 13),  # Ù…Ø§
    (["Ø£ÙÙŠÙ‘"], 14),  # Ø£ÙŠ
    (["Ø°ÙÙˆ"], 15),  # Ø°Ùˆ
    (["Ø°ÙØ§ØªÙ"], 16),  # Ø°Ø§Øª
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ"], 0),
    (["Ø§Ù„Ù’", "ØªÙÙŠ"], 1),
    (["Ù…ÙÙ†Ù’"], 12),
    (["Ù…ÙØ§"], 13),
    ]

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª,
    dataset = RelativePronounDataset(training_data, processor, max_length=15)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±,
    models = {
    "LSTM": RelativePronounLSTM()
    vocab_size=processor.vocab_size,
    embed_dim=64,
    hidden_size=128,
    num_classes=len(RELATIVE_PRONOUNS),
    num_layers=2),
    "GRU": RelativePronounGRU()
    vocab_size=processor.vocab_size,
    embed_dim=64,
    hidden_size=128,
    num_classes=len(RELATIVE_PRONOUNS),
    num_layers=2),
    "Transformer": RelativePronounTransformer()
    vocab_size=processor.vocab_size,
    d_model=128,
    nhead=8,
    num_encoder_layers=3,
    num_classes=len(RELATIVE_PRONOUNS),
    dim_feedforward=512),
    }

    # ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬,
    results = {}

    for model_name, model in models.items():
    print(f"\nğŸ”¬ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ {model_name}")
    print(" " * 40)

        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨,
    trainer = RelativePronounTrainer(model, learning_rate=0.001)
    final_accuracy = trainer.train(dataloader, epochs=20)

        # Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±,
    inference_engine = RelativePronounInference(model, processor)

    test_cases = [
    ["Ø§Ù„Ù’", "Ø°ÙÙŠ"],  # Ø§Ù„Ø°ÙŠ
    ["Ø§Ù„Ù’", "ØªÙÙŠ"],  # Ø§Ù„ØªÙŠ
    ["Ø§Ù„Ù’", "Ù„Ù", "Ø°ÙØ§", "Ù†Ù"],  # Ø§Ù„Ù„Ø°Ø§Ù†
    ["Ù…ÙÙ†Ù’"],  # Ù…ÙÙ†
    ["Ù…ÙØ§"],  # Ù…Ø§
    ["Ø£ÙÙŠÙ‘"],  # Ø£ÙŠ
    ]

    print("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:")
    correct_predictions = 0,
    for syllables in test_cases:
    result = inference_engine.predict(syllables)
    print()
    f"   {syllables} â†’ {result['predicted_pronoun']} (Ø«Ù‚Ø©: {result['confidence']:.2f})"
    )

            # ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ (Ø¨Ø³ÙŠØ·)
    expected_pronouns = ["Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ù„Ø°Ø§Ù†", "Ù…ÙÙ†", "Ù…Ø§", "Ø£ÙŠ"]
    expected_idx = test_cases.index(syllables)
            if result['predicted_pronoun'] == expected_pronouns[expected_idx]:
    correct_predictions += 1,
    test_accuracy = (correct_predictions / len(test_cases)) * 100,
    results[model_name] = {
    'training_accuracy': final_accuracy,
    'test_accuracy': test_accuracy,
    }

    print(f"   Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_accuracy:.1f}%")

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©,
    print("\nğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print("=" * 40)
    for model_name, metrics in results.items():
    print(f"{model_name}:")
    print(f"   Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {metrics['training_accuracy']:.1f}%")
    print(f"   Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {metrics['test_accuracy']:.1f}%")

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©!")


if __name__ == "__main__":
    demonstrate_relative_pronoun_models()

