import json
import ast
import os
import re
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any

class PythonSystemAnalyzer:
    def __init__(self, project_directory: str):

        self.project_dir = project_directory
        self.python_files = []
        self.functions = {}
        self.classes = {}
        self.imports = {}
        self.complexity_metrics = {}
        self.redundancy_analysis = {}
        self.integration_analysis = {}

    def analyze_complete_system(self):
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…...")

        # 1. Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø§ÙŠØ«ÙˆÙ†
        self._collect_python_files()

        # 2. ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
        for file_path in self.python_files:
            self._analyze_file(file_path)

        # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±
        self._analyze_redundancy()

        # 4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù…Ù„
        self._analyze_integration()

        # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        self._analyze_complexity()

        # 6. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
        return self._generate_comprehensive_report()

    def _collect_python_files(self):
        """Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ø§ÙŠØ«ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        for root, dirs, files in os.walk(self.project_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    self.python_files.append(file_path)

        print(f"ğŸ“ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(self.python_files)} Ù…Ù„Ù Ø¨Ø§ÙŠØ«ÙˆÙ†")

    def _analyze_file(self, file_path: str):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø¨Ø§ÙŠØ«ÙˆÙ† ÙˆØ§Ø­Ø¯"""
        try:
            with open(file_path, 'r', encoding='utf 8') as f:
                content = f.read()

            # ØªØ­Ù„ÙŠÙ„ AST
            tree = ast.parse(content)

            file_name = os.path.basename(file_path)
            self.functions[file_name] = []
            self.classes[file_name] = []
            self.imports[file_name] = []

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø¯
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    self._analyze_function(node, file_name)
                elif isinstance(node, ast.ClassDef):
                    self._analyze_class(node, file_name)
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    self._analyze_import(node, file_name)

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path: {e}}")

    def _analyze_function(self, node: ast.FunctionDef, file_name: str):
        """ØªØ­Ù„ÙŠÙ„ Ø¯Ø§Ù„Ø©"""
        func_info = {
            'name': node.name,
            'line_number': node.lineno,
            'args_count': len(node.args.args),
            'decorators': [d.id for d in node.decorator_list if isinstance(d, ast.Name)],
            'docstring': ast.get_docstring(node),
            'complexity': self._calculate_cyclomatic_complexity(node),
            'calls_made': self._extract_function_calls(node),
            'variables_used': self._extract_variables(node),
            'is_private': node.name.startswith('_'),
            'is_property': any(d.id == 'property' for d in node.decorator_list if isinstance(d, ast.Name)),
            'return_statements': self._count_return_statements(node)
        }

        self.functions[file_name].append(func_info)

    def _analyze_class(self, node: ast.ClassDef, file_name: str):
        """ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ø§Ø³"""
        methods = []
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(item.name)

        class_info = {
            'name': node.name,
            'line_number': node.lineno,
            'methods': methods,
            'method_count': len(methods),
            'inheritance': [base.id for base in node.bases if isinstance(base, ast.Name)],
            'docstring': ast.get_docstring(node),
            'is_abstract': any('ABC' in str(base) for base in node.bases)
        }

        self.classes[file_name].append(class_info)

    def _analyze_import(self, node, file_name: str):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª"""
        if isinstance(node, ast.Import):
            for alias in node.names:
                self.imports[file_name].append({
                    'type': 'import',
                    'module': alias.name,
                    'alias': alias.asname
                })
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                self.imports[file_name].append({
                    'type': 'from_import',
                    'module': node.module,
                    'name': alias.name,
                    'alias': alias.asname
                })

    def _calculate_cyclomatic_complexity(self, node) -> int:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ±ÙŠ"""
        complexity = 1  # Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    def _extract_function_calls(self, node) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„"""
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(f"{child.func.value.id if isinstance(child.func.value, ast.Name) else '?'.{child.func.attr}}")
        return calls

    def _extract_variables(self, node) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"""
        variables = []
        for child in ast.walk(node):
            if isinstance(child, ast.Name) and isinstance(child.ctx, ast.Load):
                variables.append(child.id)
        return list(set(variables))

    def _count_return_statements(self, node) -> int:
        """Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹"""
        return sum(1 for child in ast.walk(node) if isinstance(child, ast.Return))

    def _analyze_redundancy(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        print("ğŸ”„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±...")

        # ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        all_function_names = []
        for file_functions in self.functions.values():
            all_function_names.extend([f['name'] for f in file_functions])

        function_name_counts = Counter(all_function_names)
        duplicated_function_names = {name: count for name, count in function_name_counts.items() if count > 1}

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØªÙƒØ±Ø±
        all_function_calls = []
        for file_functions in self.functions.values():
            for func in file_functions:
                all_function_calls.extend(func['calls_made'])

        call_patterns = Counter(all_function_calls)
        common_patterns = {pattern: count for pattern, count in call_patterns.most_common(10)}

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        all_imports = []
        for file_imports in self.imports.values():
            all_imports.extend([imp['module'] for imp in file_imports])

        import_counts = Counter(all_imports)
        common_imports = {module: count for module, count in import_counts.most_common(10)}

        self.redundancy_analysis = {
            'duplicated_function_names': duplicated_function_names,
            'common_call_patterns': common_patterns,
            'common_imports': common_imports,
            'redundancy_score': len(duplicated_function_names) / len(set(all_function_names)) if all_function_names else 0
        }

    def _analyze_integration(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
        print("ğŸ”— ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù…Ù„...")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª
        file_dependencies = defaultdict(set)

        for file_name, imports in self.imports.items():
            for imp in imports:
                if imp['module']:
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
                    if any(imp['module'] in other_file for other_file in self.python_files):
                        file_dependencies[file_name].add(imp['module'])

        # ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª
        cross_file_calls = defaultdict(list)

        for file_name, functions in self.functions.items():
            for func in functions:
                for call in func['calls_made']:
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¯Ø§Ù„Ø© ÙÙŠ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰
                    for other_file, other_functions in self.functions.items():
                        if other_file != file_name:
                            if any(call == other_func['name'] for other_func in other_functions):
                                cross_file_calls[file_name].append({
                                    'caller': func['name'],
                                    'called': call,
                                    'target_file': other_file
                                })

        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙ…Ø§Ø³Ùƒ
        total_functions = sum(len(funcs) for funcs in self.functions.values())
        total_cross_calls = sum(len(calls) for calls in cross_file_calls.values())
        cohesion_index = total_cross_calls / total_functions if total_functions > 0 else 0

        self.integration_analysis = {
            'file_dependencies': dict(file_dependencies),
            'cross_file_calls': dict(cross_file_calls),
            'cohesion_index': cohesion_index,
            'total_dependencies': len(file_dependencies),
            'avg_dependencies_per_file': sum(len(deps) for deps in file_dependencies.values()) / len(file_dependencies) if file_dependencies else 0
        }

    def _analyze_complexity(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯...")

        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ§Ù„
        all_complexities = []
        file_complexities = {}

        for file_name, functions in self.functions.items():
            file_complexity = sum(func['complexity'] for func in functions)
            file_complexities[file_name] = {
                'total_complexity': file_complexity,
                'function_count': len(functions),
                'avg_complexity': file_complexity / len(functions) if functions else 0,
                'max_complexity': max((func['complexity'] for func in functions), default=0),
                'high_complexity_functions': [func['name'] for func in functions if func['complexity'] > 10]
            }
            all_complexities.extend([func['complexity'] for func in functions])

        # ØªØ­Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª
        file_sizes = {}
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf 8') as f:
                    lines = f.readlines()
                    file_name = os.path.basename(file_path)
                    file_sizes[file_name] = {
                        'total_lines': len(lines),
                        'code_lines': len([line for line in lines if line.strip() and not line.strip().startswith('#')]),
                        'comment_lines': len([line for line in lines if line.strip().startswith('#')]),
                        'blank_lines': len([line for line in lines if not line.strip()])
                    }
            except:
                pass

        self.complexity_metrics = {
            'file_complexities': file_complexities,
            'file_sizes': file_sizes,
            'overall_complexity': sum(all_complexities),
            'avg_function_complexity': sum(all_complexities) / len(all_complexities) if all_complexities else 0,
            'high_complexity_threshold': 10,
            'total_high_complexity_functions': sum(len(fc['high_complexity_functions']) for fc in file_complexities.values())
        }

    def _classify_functions(self) -> Dict[str, List[str]]:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©"""
        classifications = {
            'generators': [],
            'analyzers': [],
            'validators': [],
            'utilities': [],
            'initializers': [],
            'processors': [],
            'exporters': [],
            'loaders': [],
            'testers': [],
            'helpers': []
        }

        for file_name, functions in self.functions.items():
            for func in functions:
                name = func['name'].lower()

                if any(keyword in name for keyword in ['generate', 'create', 'build', 'construct']):
                    classifications['generators'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['analyze', 'examine', 'evaluate', 'assess']):
                    classifications['analyzers'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['validate', 'check', 'verify', 'ensure']):
                    classifications['validators'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['load', 'read', 'import', 'fetch']):
                    classifications['loaders'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['save', 'write', 'export', 'output']):
                    classifications['exporters'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['process', 'transform', 'convert', 'parse']):
                    classifications['processors'].append(f"{file_name:{func['name']}}")
                elif any(keyword in name for keyword in ['test', 'demo', 'example']):
                    classifications['testers'].append(f"{file_name:{func['name']}}")
                elif name.startswith('__init__') or 'init' in name:
                    classifications['initializers'].append(f"{file_name:{func['name']}}")
                elif name.startswith('_') or any(keyword in name for keyword in ['helper', 'util', 'tool']):
                    classifications['helpers'].append(f"{file_name:{func['name']}}")
                else:
                    classifications['utilities'].append(f"{file_name:{func['name']}}")

        return classifications

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„"""
        print("ğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„...")

        function_classifications = self._classify_functions()

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        total_functions = sum(len(funcs) for funcs in self.functions.values())
        total_classes = sum(len(classes) for classes in self.classes.values())
        total_lines = sum(size['total_lines'] for size in self.complexity_metrics['file_sizes'].values())

        report = {
            'system_overview': {
                'total_files': len(self.python_files),
                'total_functions': total_functions,
                'total_classes': total_classes,
                'total_lines_of_code': total_lines,
                'analysis_timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'N/A'
            },

            'function_analysis': {
                'classification': function_classifications,
                'complexity_distribution': self.complexity_metrics,
                'most_complex_functions': self._get_most_complex_functions(5)
            },

            'redundancy_analysis': self.redundancy_analysis,

            'integration_analysis': self.integration_analysis,

            'quality_metrics': {
                'code_coverage': self._estimate_code_coverage(),
                'documentation_coverage': self._calculate_documentation_coverage(),
                'maintainability_index': self._calculate_maintainability_index()
            },

            'recommendations': self._generate_recommendations()
        }

        return report

    def _get_most_complex_functions(self, count: int) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ø§Ù„Ø¯ÙˆØ§Ù„ ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹"""
        all_functions = []
        for file_name, functions in self.functions.items():
            for func in functions:
                all_functions.append({
                    'file': file_name,
                    'name': func['name'],
                    'complexity': func['complexity'],
                    'lines': func.get('line_number', 0)
                })

        return sorted(all_functions, key=lambda x: x['complexity'], reverse=True)[:count]

    def _estimate_code_coverage(self) -> float:
        """ØªÙ‚Ø¯ÙŠØ± ØªØºØ·ÙŠØ© Ø§Ù„ÙƒÙˆØ¯"""
        test_functions = 0
        total_functions = 0

        for file_name, functions in self.functions.items():
            if 'test' in file_name.lower():
                test_functions += len(functions)
            total_functions += len(functions)

        return (test_functions / total_functions * 100) if total_functions > 0 else 0

    def _calculate_documentation_coverage(self) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØºØ·ÙŠØ© Ø§Ù„ØªÙˆØ«ÙŠÙ‚"""
        documented_functions = 0
        total_functions = 0

        for functions in self.functions.values():
            for func in functions:
                total_functions += 1
                if func['docstring']:
                    documented_functions += 1

        return (documented_functions / total_functions * 100) if total_functions > 0 else 0

    def _calculate_maintainability_index(self) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØµÙŠØ§Ù†Ø©"""
        # Ù…Ø¤Ø´Ø± Ù…Ø¨Ø³Ø· ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„ØªÙˆØ«ÙŠÙ‚ ÙˆØ§Ù„Ø­Ø¬Ù…
        avg_complexity = self.complexity_metrics['avg_function_complexity']
        doc_coverage = self._calculate_documentation_coverage()
        redundancy_score = self.redundancy_analysis['redundancy_score']

        # Ù…Ø¤Ø´Ø± Ù…Ù† 0 Ø¥Ù„Ù‰ 100 (Ø£Ø¹Ù„Ù‰ = Ø£ÙØ¶Ù„)
        maintainability = 100 - (avg_complexity * 5) + (doc_coverage * 0.3) - (redundancy_score * 20)
        return max(0, min(100, maintainability))

    def _generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        recommendations = []

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if self.complexity_metrics['avg_function_complexity'] > 8:
            recommendations.append("âš ï¸ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ø±ØªÙØ¹ - ÙÙƒØ± ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©")

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±
        if self.redundancy_analysis['redundancy_score'] > 0.2:
            recommendations.append("ğŸ”„ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ø§Ù„ÙŠØ© - ÙÙƒØ± ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ÙƒÙˆØ¯")

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        if self._calculate_documentation_coverage() < 50:
            recommendations.append("ğŸ“š ØªØºØ·ÙŠØ© Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ù…Ù†Ø®ÙØ¶Ø© - Ø£Ø¶Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙˆØ«ÙŠÙ‚")

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ§Ù…Ù„
        if self.integration_analysis['cohesion_index'] < 0.1:
            recommendations.append("ğŸ”— Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙ…Ø§Ø³Ùƒ Ù…Ù†Ø®ÙØ¶ - Ø­Ø³Ù† Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª")

        if not recommendations:
            recommendations.append("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©!")

        return recommendations


def run_comprehensive_analysis():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…"""
    current_dir = os.getcwd()
    analyzer = PythonSystemAnalyzer(current_dir)

    print("=" * 60)
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø§ÙŠØ«ÙˆÙ†")
    print("=" * 60)

    report = analyzer.analyze_complete_system()

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    with open('COMPREHENSIVE_SYSTEM_ANALYSIS.json', 'w', encoding='utf 8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ
    print("\n" + "=" * 60)
    print("ğŸ“Š Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    print("=" * 60)

    overview = report['system_overview']
    print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {overview['total_files']}")
    print(f"ğŸ”§ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¯ÙˆØ§Ù„: {overview['total_functions']}")
    print(f"ğŸ—ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª: {overview['total_classes']}")
    print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {overview['total_lines_of_code']}")

    print(f"\nğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {report['function_analysis']['complexity_distribution']['avg_function_complexity']:.2f}")
    print(f"ğŸ“š ØªØºØ·ÙŠØ© Ø§Ù„ØªÙˆØ«ÙŠÙ‚: {report['quality_metrics']['documentation_coverage']:.1f}%")
    print(f"ğŸ§ª ØªÙ‚Ø¯ÙŠØ± ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {report['quality_metrics']['code_coverage']:.1f}%")
    print(f"ğŸ”§ Ù…Ø¤Ø´Ø± Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØµÙŠØ§Ù†Ø©: {report['quality_metrics']['maintainability_index']:.1f/100}")

    print(f"\nğŸ”„ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±: {report['redundancy_analysis']['redundancy_score']:.2%}")
    print(f"ğŸ”— Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙ…Ø§Ø³Ùƒ: {report['integration_analysis']['cohesion_index']:.3f}")

    print("\nğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
    for rec in report['recommendations']:
        print(f"  {rec}")

    print("\nğŸ“‹ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯ÙˆØ§Ù„:")
    for category, functions in report['function_analysis']['classification'].items():
        if functions:
            print(f"  {category}: {len(functions) Ø¯Ø§Ù„Ø©}")

    print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„ ÙÙŠ: COMPREHENSIVE_SYSTEM_ANALYSIS.json")
    print("=" * 60)

    return report

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
if __name__ == "__main__":
    try:
        analysis_report = run_comprehensive_analysis()
        print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")#!/usr/bin/env python3
"""
 MODULAR NLP ENGINE IMPLEMENTATION
Core Base Engine Abstract Class

This module provides the foundation for all NLP engines in the modular architecture.
Each engine inherits from BaseNLPEngine and implements the required methods.
"""

# Global suppressions for WinSurf IDE
# pylint: disable=broad-except,unused-variable,unused-argument,too-many-arguments
# pylint: disable=invalid-name,too-few-public-methods,missing-docstring
# pylint: disable=too-many-locals,too-many-branches,too-many statements
# noqa: E501,F401,F403,E722,A001,F821


import logging
import hashlib
import json
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
import sqlite3
import os
import time

logger = logging.getLogger(__name__)



# =============================================================================
# DatabaseManager Class Implementation
# ØªÙ†ÙÙŠØ° ÙØ¦Ø© DatabaseManager
# =============================================================================

class DatabaseManager:
    """Manages SQLite databases for engines"""

    def __init__(self, engine_name: str, use_shared: bool = False):

        self.engine_name = engine_name
        self.use_shared = use_shared
        self.db_path = self._get_db_path()
        self._init_database()


# -----------------------------------------------------------------------------
# _get_db_path Method - Ø·Ø±ÙŠÙ‚Ø© _get_db_path
# -----------------------------------------------------------------------------

    def _get_db_path(self) -> str:
        """Get database path for engine"""
        if self.use_shared:
            return "database/shared.db"
        else:
            return f"database/engines/{self.engine_name}.db"


# -----------------------------------------------------------------------------
# _init_database Method - Ø·Ø±ÙŠÙ‚Ø© _init_database
# -----------------------------------------------------------------------------

    def _init_database(self):
        """Initialize database with basic tables"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)

        conn = sqlite3.connect(self.db_path)
        conn.run_command(
            """
            CREATE TABLE IF NOT EXISTS engine_metadata (
                key TEXT PRIMARY KEY,
                value TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        conn.run_command(
            """
            CREATE TABLE IF NOT EXISTS analysis_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                input_hash TEXT UNIQUE,
                input_text TEXT,
                parameters TEXT,
                result TEXT,
                processing_time REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        conn.run_command(
            """
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                operation TEXT,
                processing_time REAL,
                memory_usage REAL,
                input_size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """
        )

        conn.commit()
        conn.close()


# -----------------------------------------------------------------------------
# cache_result Method - Ø·Ø±ÙŠÙ‚Ø© cache_result
# -----------------------------------------------------------------------------

    def cache_result():
    """
Process cache_result operation
Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù…Ù„ÙŠØ© cache_result

Args:
    param (type): Description of parameter

Returns:
    type: Description of return value

Raises:
    ValueError: If invalid input provided

Example:
    >>> result = cache_result(param)
    >>> print(result)
"""
        self, input_text: str, parameters: Dict, result: Dict, processing_time: float
    ):
        """Cache analysis result"""
        input_hash = hashlib.md5(
            f"{input_text:{json.dumps(parameters, sort_keys=True)}}".encode()
        ).hexdigest()

        conn = sqlite3.connect(self.db_path)
        conn.run_command(
            """
            INSERT OR REPLACE INTO analysis_cache
            (input_hash, input_text, parameters, result, processing_time)
            VALUES (?, ?, ?, ?, ?)
        """,
            (
                input_hash,
                input_text,
                json.dumps(parameters),
                json.dumps(result),
                processing_time))
        conn.commit()
        conn.close()


# -----------------------------------------------------------------------------
# get_cached_result Method - Ø·Ø±ÙŠÙ‚Ø© get_cached_result
# -----------------------------------------------------------------------------

    def get_cached_result(self, input_text: str, parameters: Dict) -> Optional[Dict]:
        """Get cached analysis result"""
        input_hash = hashlib.md5(
            f"{input_text:{json.dumps(parameters, sort_keys=True)}}".encode()
        ).hexdigest()

        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        row = conn.run_command(
            """
            SELECT result FROM analysis_cache
            WHERE input_hash = ?
        """,
            (input_hash)).fetchone()
        conn.close()

        return json.import_datas(row["result"]) if row else None


# -----------------------------------------------------------------------------
# log_performance Method - Ø·Ø±ÙŠÙ‚Ø© log_performance
# -----------------------------------------------------------------------------

    def log_performance():
    """
Process log_performance operation
Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù…Ù„ÙŠØ© log_performance

Args:
    param (type): Description of parameter

Returns:
    type: Description of return value

Raises:
    ValueError: If invalid input provided

Example:
    >>> result = log_performance(param)
    >>> print(result)
"""
        self,
        operation: str,
        processing_time: float,
        memory_usage: float,
        input_size: int):
        """Log performance metrics"""
        conn = sqlite3.connect(self.db_path)
        conn.run_command(
            """
            INSERT INTO performance_metrics
            (operation, processing_time, memory_usage, input_size)
            VALUES (?, ?, ?, ?)
        """,
            (operation, processing_time, memory_usage, input_size))
        conn.commit()
        conn.close()



# =============================================================================
# BaseNLPEngine Class Implementation
# ØªÙ†ÙÙŠØ° ÙØ¦Ø© BaseNLPEngine
# =============================================================================

class BaseNLPEngine(ABC):
    """Abstract base class for all NLP engines"""

    def __init__(self, engine_name: str, config: Dict[str, Any]):

        self.engine_name = engine_name
        self.config = config
        self.db_manager = DatabaseManager(
            engine_name, config.get("use_shared_dbf", False)
        )
        self.models = {}
        self.is_initialized = False
        self.version = config.get("version", "1.0.0")
        self.description = config.get("description", f"{engine_name NLP Engine}")

        # Performance tracking
        self.total_requests = 0
        self.total_processing_time = 0.0
        self.cache_hits = 0
        self.cache_misses = 0

        logger.info(f"Initializing %s engine v{self.version}", self.engine_name)

    @abstractmethod

# -----------------------------------------------------------------------------
# analyze Method - Ø·Ø±ÙŠÙ‚Ø© analyze
# -----------------------------------------------------------------------------

    def analyze(self, text: str, **kwargs) -> Dict[str, Any]:
        """
        Main analysis method - must be implemented by each engine

        Args:
            text: Input text to analyze
            **kwargs: Additional parameters specific to the engine

        Returns:
            Dict containing analysis results
        """

    @abstractmethod

# -----------------------------------------------------------------------------
# import_data_models Method - Ø·Ø±ÙŠÙ‚Ø© import_data_models
# -----------------------------------------------------------------------------

    def import_data_models(self) -> bool:
        """
        Import engine specific models

        Returns:
            True if models import_dataed successfully, False otherwise
        """

    @abstractmethod

# -----------------------------------------------------------------------------
# validate_input Method - Ø·Ø±ÙŠÙ‚Ø© validate_input
# -----------------------------------------------------------------------------

    def validate_input(self, text: str) -> bool:
        """
        Validate input parameters

        Args:
            text: Input text to validate

        Returns:
            True if input is valid, False otherwise
        """
        raise NotImplementedError("Subclasses must implement validate_input method")


# -----------------------------------------------------------------------------
# initialize Method - Ø·Ø±ÙŠÙ‚Ø© initialize
# -----------------------------------------------------------------------------

    def initialize(self) -> bool:
        """Initialize the engine"""
        try:
            logger.info("Importing models for %s...", self.engine_name)
            if self.import_data_models():
                self.is_initialized = True
                logger.info("%s engine initialized successfully", self.engine_name)
                return True
            else:
                logger.error("Failed to import models for %s", self.engine_name)
                return False
        except (ImportError, AttributeError, OSError, ValueError) as e:
            logger.error("Failed to initialize %s: %s", self.engine_name, e)
            return False


# -----------------------------------------------------------------------------
# process_with_cache Method - Ø·Ø±ÙŠÙ‚Ø© process_with_cache
# -----------------------------------------------------------------------------

    def process_with_cache(self, text: str, **kwargs) -> Dict[str, Any]:
        """Process text with caching support"""

        begin_time = time.time()

        # Check cache if enabled
        if self.config.get("enable_cachef", True):
            if cached_result := self.db_manager.get_cached_result(text, kwargs):
                self.cache_hits += 1
                processing_time = time.time() - begin_time

                return {
                    "success": True,
                    "result": cached_result,
                    "cached": True,
                    "processing_time": processing_time,
                    "engine": self.engine_name,
                    "version": self.version,
                }

        # Increment cache miss if we reach here
        if self.config.get("enable_cachef", True):
            self.cache_misses += 1

        # Validate input
        if not self.validate_input(text, **kwargs):
            return {
                "success": False,
                "error": "Invalid input parameters",
                "engine": self.engine_name,
            }

        try:
            # Perform analysis
            result = self.analyze(text, **kwargs)
            processing_time = time.time() - begin_time

            # Update statistics
            self.total_requests += 1
            self.total_processing_time += processing_time

            # Cache result if enabled
            if self.config.get("enable_cache", True):
                self.db_manager.cache_result(text, kwargs, result, processing_time)

            # Log performance
            self.db_manager.log_performance("analyzef", processing_time, 0.0, len(text))

            return {
                "success": True,
                "result": result,
                "cached": False,
                "processing_time": processing_time,
                "engine": self.engine_name,
                "version": self.version,
            }

        except (ValueError, TypeError, RuntimeError) as e:
            logger.error("Analysis error in %s: %sf", self.engine_name, e)
            return {"success": False, "error": str(e), "engine": self.engine_name}


# -----------------------------------------------------------------------------
# get_info Method - Ø·Ø±ÙŠÙ‚Ø© get_info
# -----------------------------------------------------------------------------

    def get_info(self) -> Dict[str, Any]:
        """Return engine informationf"
        avg_processing_time = (
            self.total_processing_time / self.total_requests
            if self.total_requests > 0
            else 0.0
        )

        cache_hit_rate = (
            self.cache_hits / self.total_requests if self.total_requests > 0 else 0.0
        )

        return {
            "name": self.engine_name,
            "version": self.version,
            "description": self.description,
            "initialized": self.is_initialized,
            "models_import_dataed": list(self.models.keys()),
            "config": {
                "cache_enabled": self.config.get("enable_cache", True),
                "shared_db": self.config.get("use_shared_db", False),
            },
            "statisticsf": {
                "total_requests": self.total_requests,
                "cache_hits": self.cache_hits,
                "cache_hit_rate": cache_hit_rate,
                "average_processing_time": avg_processing_time,
            },
            "capabilities": self.get_capabilities(),
        }


# -----------------------------------------------------------------------------
# get_capabilities Method - Ø·Ø±ÙŠÙ‚Ø© get_capabilities
# -----------------------------------------------------------------------------

    def get_capabilities(self) -> List[str]:
        """Return list of engine capabilities"""
        return getattr(self, "CAPABILITIES", ["analyze"])


# -----------------------------------------------------------------------------
# get_cache_stats Method - Ø·Ø±ÙŠÙ‚Ø© get_cache_stats
# -----------------------------------------------------------------------------

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statisticsf"
        return {
            "hits": self.cache_hits,
            "misses": self.cache_misses,
            "size": self.cache_hits + self.cache_misses,  # Total cache operations
            "hit_rate": (
                self.cache_hits / (self.cache_hits + self.cache_misses)
                if (self.cache_hits + self.cache_misses) -> 0
                else 0.0
            ),
        }


# -----------------------------------------------------------------------------
# reimport_data_models Method - Ø·Ø±ÙŠÙ‚Ø© reimport_data_models
# -----------------------------------------------------------------------------

    def reimport_data_models(self) -> bool:
        """Reimport engine models"""
        try:
            logger.info("Reimport_dataing models for %s...", self.engine_name)
            self.models.clear()
            return self.import_data_models()
        except (ImportError, AttributeError, OSError, ValueError) as e:
            logger.error("Failed to reimport models for %s: %s", self.engine_name, e)
            return False


# -----------------------------------------------------------------------------
# health_check Method - Ø·Ø±ÙŠÙ‚Ø© health_check
# -----------------------------------------------------------------------------

    def health_check(self) -> Dict[str, Any]:
        """Perform health check on the engine"""
        try:
            # Test basic functionality
            test_result = self.validate_input("testf")

            return {
                "status": "healthy" if self.is_initialized else "unhealthy",
                "initialized": self.is_initialized,
                "models_import_dataed": len(self.models),
                "database_accessible": os.path.exists(self.db_manager.db_path),
                "test_validation": test_result,
            }
        except (ImportError, AttributeError, OSError, ValueError) as e:
            return {"status": "unhealthy", "error": str(e)}


# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…
analysis_report =

