#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Pronouns Generator from Syllables - Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©
===============================================================================

This module generates Arabic pronouns (detached and attached) from syllabic patterns
using deep learning and phonological analysis. It covers both:
1. Detached pronouns (Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø©): Ø£Ù†Ø§ØŒ Ø£Ù†ØªØŒ Ù‡ÙˆØŒ Ù‡ÙŠØŒ Ù†Ø­Ù†ØŒ Ø¥Ù„Ø®
2. Attached pronouns (Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©): Ù€Ù†ÙŠØŒ Ù€ÙƒØŒ Ù€Ù‡ØŒ Ù€Ù‡Ø§ØŒ Ø¥Ù„Ø®

Ù†Ø¸Ø§Ù… Ù…ØªØ·ÙˆØ± ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - ARABIC PRONOUNS FROM SYLLABLES
Date: 2025-07-24
Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc


import json  # noqa: F401
import logging  # noqa: F401
import numpy as np  # noqa: F401
import sys  # noqa: F401
from pathlib import Path  # noqa: F401
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field  # noqa: F401
from enum import Enum  # noqa: F401
import re  # noqa: F401

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
    logging.FileHandler('arabic_pronouns_generator.log', encoding='utf 8'),
    logging.StreamHandler(sys.stdout),
    ],
)

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC PRONOUNS CLASSIFICATION SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class PronounType(Enum):
    """ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    DETACHED = "Ù…Ù†ÙØµÙ„"  # Detached pronouns
    ATTACHED = "Ù…ØªØµÙ„"  # Attached pronouns


class PronounPerson(Enum):
    """ØªØµÙ†ÙŠÙ Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    FIRST = "Ù…ØªÙƒÙ„Ù…"  # First person
    SECOND = "Ù…Ø®Ø§Ø·Ø¨"  # Second person
    THIRD = "ØºØ§Ø¦Ø¨"  # Third person


class PronounNumber(Enum):
    """ØªØµÙ†ÙŠÙ Ø¹Ø¯Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    SINGULAR = "Ù…ÙØ±Ø¯"  # Singular
    DUAL = "Ù…Ø«Ù†Ù‰"  # Dual
    PLURAL = "Ø¬Ù…Ø¹"  # Plural


class PronounGender(Enum):
    """ØªØµÙ†ÙŠÙ Ø¬Ù†Ø³ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    MASCULINE = "Ù…Ø°ÙƒØ±"  # Masculine
    FEMININE = "Ù…Ø¤Ù†Ø«"  # Feminine
    NEUTRAL = "Ù…Ø­Ø§ÙŠØ¯"  # Neutral (for first person)


@dataclass
class PronounEntry:
    """ÙƒÙŠØ§Ù† Ø§Ù„Ø¶Ù…ÙŠØ± Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ©"""

    text: str  # Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¶Ù…ÙŠØ±
    pronoun_type: PronounType  # Ù†ÙˆØ¹ Ø§Ù„Ø¶Ù…ÙŠØ± (Ù…ØªØµÙ„/Ù…Ù†ÙØµÙ„)
    person: PronounPerson  # Ø§Ù„Ø´Ø®Øµ (Ù…ØªÙƒÙ„Ù…/Ù…Ø®Ø§Ø·Ø¨/ØºØ§Ø¦Ø¨)
    number: PronounNumber  # Ø§Ù„Ø¹Ø¯Ø¯ (Ù…ÙØ±Ø¯/Ù…Ø«Ù†Ù‰/Ø¬Ù…Ø¹)
    gender: PronounGender  # Ø§Ù„Ø¬Ù†Ø³ (Ù…Ø°ÙƒØ±/Ù…Ø¤Ù†Ø«/Ù…Ø­Ø§ÙŠØ¯)
    syllable_pattern: str  # Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠ
    phonetic_features: Dict[str, Any] = field(default_factory=dict)
    usage_contexts: List[str] = field(default_factory=list)
    frequency_score: float = 0.0
    class_id: int = 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC PRONOUNS DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPronounsDatabase:
    """Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    def __init__(self):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.pronouns: List[PronounEntry] = []
    self.class_mapping: Dict[int, str] = {}
    self.syllable_patterns: Dict[str, List[str]] = {}
    self._initialize_pronouns_database()

    def _initialize_pronouns_database(self):  # type: ignore[no-untyped def]
    """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""

        # Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø© - Detached Pronouns
    detached_pronouns = [
            # First Person - Ø§Ù„Ù…ØªÙƒÙ„Ù…
    PronounEntry(
    "Ø£Ù†Ø§",
    PronounType.DETACHED,
    PronounPerson.FIRST,
    PronounNumber.SINGULAR,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.95,
                class_id=0,
    ),
    PronounEntry(
    "Ù†Ø­Ù†",
    PronounType.DETACHED,
    PronounPerson.FIRST,
    PronounNumber.PLURAL,
    PronounGender.NEUTRAL,
    "CV CVC",
    frequency_score=0.85,
                class_id=1,
    ),
            # Second Person - Ø§Ù„Ù…Ø®Ø§Ø·Ø¨
    PronounEntry(
    "Ø£Ù†Øª",
    PronounType.DETACHED,
    PronounPerson.SECOND,
    PronounNumber.SINGULAR,
    PronounGender.MASCULINE,
    "CV CVC",
    frequency_score=0.90,
                class_id=2,
    ),
    PronounEntry(
    "Ø£Ù†ØªÙ",
    PronounType.DETACHED,
    PronounPerson.SECOND,
    PronounNumber.SINGULAR,
    PronounGender.FEMININE,
    "CV CV",
    frequency_score=0.88,
                class_id=3,
    ),
    PronounEntry(
    "Ø£Ù†ØªÙ…Ø§",
    PronounType.DETACHED,
    PronounPerson.SECOND,
    PronounNumber.DUAL,
    PronounGender.NEUTRAL,
    "CV-CV CV",
    frequency_score=0.40,
                class_id=4,
    ),
    PronounEntry(
    "Ø£Ù†ØªÙ…",
    PronounType.DETACHED,
    PronounPerson.SECOND,
    PronounNumber.PLURAL,
    PronounGender.MASCULINE,
    "CV CVC",
    frequency_score=0.75,
                class_id=5,
    ),
    PronounEntry(
    "Ø£Ù†ØªÙ†",
    PronounType.DETACHED,
    PronounPerson.SECOND,
    PronounNumber.PLURAL,
    PronounGender.FEMININE,
    "CV CVC",
    frequency_score=0.65,
                class_id=6,
    ),
            # Third Person - Ø§Ù„ØºØ§Ø¦Ø¨
    PronounEntry(
    "Ù‡Ùˆ",
    PronounType.DETACHED,
    PronounPerson.THIRD,
    PronounNumber.SINGULAR,
    PronounGender.MASCULINE,
    "CV CV",
    frequency_score=0.95,
                class_id=7,
    ),
    PronounEntry(
    "Ù‡ÙŠ",
    PronounType.DETACHED,
    PronounPerson.THIRD,
    PronounNumber.SINGULAR,
    PronounGender.FEMININE,
    "CV CV",
    frequency_score=0.93,
                class_id=8,
    ),
    PronounEntry(
    "Ù‡Ù…Ø§",
    PronounType.DETACHED,
    PronounPerson.THIRD,
    PronounNumber.DUAL,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.45,
                class_id=9,
    ),
    PronounEntry(
    "Ù‡Ù…",
    PronounType.DETACHED,
    PronounPerson.THIRD,
    PronounNumber.PLURAL,
    PronounGender.MASCULINE,
    "CVC",
    frequency_score=0.80,
                class_id=10,
    ),
    PronounEntry(
    "Ù‡Ù†",
    PronounType.DETACHED,
    PronounPerson.THIRD,
    PronounNumber.PLURAL,
    PronounGender.FEMININE,
    "CVC",
    frequency_score=0.70,
                class_id=11,
    ),
    ]

        # Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø© - Attached Pronouns
    attached_pronouns = [
            # First Person - Ø§Ù„Ù…ØªÙƒÙ„Ù…
    PronounEntry(
    "Ù€Ù†ÙŠ",
    PronounType.ATTACHED,
    PronounPerson.FIRST,
    PronounNumber.SINGULAR,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.85,
                class_id=12,
    ),
    PronounEntry(
    "Ù€ÙŠ",
    PronounType.ATTACHED,
    PronounPerson.FIRST,
    PronounNumber.SINGULAR,
    PronounGender.NEUTRAL,
    "CV",
    frequency_score=0.90,
                class_id=13,
    ),
    PronounEntry(
    "Ù€Ù†Ø§",
    PronounType.ATTACHED,
    PronounPerson.FIRST,
    PronounNumber.PLURAL,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.88,
                class_id=14,
    ),
            # Second Person - Ø§Ù„Ù…Ø®Ø§Ø·Ø¨
    PronounEntry(
    "Ù€Ùƒ",
    PronounType.ATTACHED,
    PronounPerson.SECOND,
    PronounNumber.SINGULAR,
    PronounGender.MASCULINE,
    "CVC",
    frequency_score=0.92,
                class_id=15,
    ),
    PronounEntry(
    "Ù€ÙƒÙ",
    PronounType.ATTACHED,
    PronounPerson.SECOND,
    PronounNumber.SINGULAR,
    PronounGender.FEMININE,
    "CV",
    frequency_score=0.85,
                class_id=16,
    ),
    PronounEntry(
    "Ù€ÙƒÙ…Ø§",
    PronounType.ATTACHED,
    PronounPerson.SECOND,
    PronounNumber.DUAL,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.35,
                class_id=17,
    ),
    PronounEntry(
    "Ù€ÙƒÙ…",
    PronounType.ATTACHED,
    PronounPerson.SECOND,
    PronounNumber.PLURAL,
    PronounGender.MASCULINE,
    "CVC",
    frequency_score=0.70,
                class_id=18,
    ),
    PronounEntry(
    "Ù€ÙƒÙ†",
    PronounType.ATTACHED,
    PronounPerson.SECOND,
    PronounNumber.PLURAL,
    PronounGender.FEMININE,
    "CVC",
    frequency_score=0.60,
                class_id=19,
    ),
            # Third Person - Ø§Ù„ØºØ§Ø¦Ø¨
    PronounEntry(
    "Ù€Ù‡",
    PronounType.ATTACHED,
    PronounPerson.THIRD,
    PronounNumber.SINGULAR,
    PronounGender.MASCULINE,
    "CV",
    frequency_score=0.95,
                class_id=20,
    ),
    PronounEntry(
    "Ù€Ù‡Ø§",
    PronounType.ATTACHED,
    PronounPerson.THIRD,
    PronounNumber.SINGULAR,
    PronounGender.FEMININE,
    "CV CV",
    frequency_score=0.90,
                class_id=21,
    ),
    PronounEntry(
    "Ù€Ù‡Ù…Ø§",
    PronounType.ATTACHED,
    PronounPerson.THIRD,
    PronounNumber.DUAL,
    PronounGender.NEUTRAL,
    "CV CV",
    frequency_score=0.40,
                class_id=22,
    ),
    PronounEntry(
    "Ù€Ù‡Ù…",
    PronounType.ATTACHED,
    PronounPerson.THIRD,
    PronounNumber.PLURAL,
    PronounGender.MASCULINE,
    "CVC",
    frequency_score=0.75,
                class_id=23,
    ),
    PronounEntry(
    "Ù€Ù‡Ù†",
    PronounType.ATTACHED,
    PronounPerson.THIRD,
    PronounNumber.PLURAL,
    PronounGender.FEMININE,
    "CVC",
    frequency_score=0.65,
                class_id=24,
    ),
    ]

        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±
    self.pronouns = detached_pronouns + attached_pronouns

        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
    self.class_mapping = {p.class_id: p.text for p in self.pronouns}

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©
    self._group_syllable_patterns()

    logger.info(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±: {len(self.pronouns)} Ø¶Ù…ÙŠØ±")

    def _group_syllable_patterns(self):  # type: ignore[no-untyped def]
    """ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø­Ø³Ø¨ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©"""

        for pronoun in self.pronouns:
    pattern = pronoun.syllable_pattern
            if pattern not in self.syllable_patterns:
    self.syllable_patterns[pattern] = []
    self.syllable_patterns[pattern].append(pronoun.text)

    logger.info(f"ğŸ“Š Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©: {len(self.syllable_patterns)} Ù†Ù…Ø·")

    def get_pronoun_by_id(self, class_id: int) -> Optional[PronounEntry]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¶Ù…ÙŠØ± Ø¨Ù…Ø¹Ø±Ù Ø§Ù„ÙØ¦Ø©"""
        for pronoun in self.pronouns:
            if pronoun.class_id == class_id:
    return pronoun
    return None

    def get_pronouns_by_pattern(self, pattern: str) -> List[PronounEntry]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠ"""
    return [p for p in self.pronouns if p.syllable_pattern == pattern]

    def get_statistics(self) -> Dict[str, Any]:
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""

    stats = {
    'total_pronouns': len(self.pronouns),
    'detached_count': len(
    [p for p in self.pronouns if p.pronoun_type == PronounType.DETACHED]
    ),
    'attached_count': len(
    [p for p in self.pronouns if p.pronoun_type == PronounType.ATTACHED]
    ),
    'patterns_distribution': {},
    'person_distribution': {},
    'number_distribution': {},
    'gender_distribution': {},
    }

        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        for pattern, pronouns in self.syllable_patterns.items():
    stats['patterns_distribution'][pattern] = len(pronouns)

        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ
        for person in PronounPerson:
    count = len([p for p in self.pronouns if p.person == person])
    stats['person_distribution'][person.value] = count

        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø¯Ø¯
        for number in PronounNumber:
    count = len([p for p in self.pronouns if p.number == number])
    stats['number_distribution'][number.value] = count

        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬Ù†Ø³
        for gender in PronounGender:
    count = len([p for p in self.pronouns if p.gender == gender])
    stats['gender_distribution'][gender.value] = count

    return stats


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYLLABLE-TO-PRONOUN PATTERN ANALYZER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class SyllablePatternAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ© Ù„Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    def __init__(self, syllables_database_path: str):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.syllables_db = self._load_syllables_database(syllables_database_path)
    self.pattern_mappings: Dict[str, List[str]] = {}
    self._analyze_pronoun_patterns()

    def _load_syllables_database(self, path: str) -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""
        try:
            with open(path, 'r', encoding='utf 8') as f:
    return json.load(f)
        except FileNotFoundError:
    logger.error(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {path}")
    return {}

    def _analyze_pronoun_patterns(self):  # type: ignore[no-untyped def]
    """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©"""

        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø©
    detached_patterns = {
    'CV CV': ['Ø£Ù†Ø§', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ù‡Ù…Ø§'],  # Ø£ÙÙ†ÙØ§ØŒ Ù‡ÙÙˆÙØŒ Ù‡ÙÙŠÙØŒ Ù‡ÙÙ…ÙØ§
    'CV CVC': ['Ù†Ø­Ù†', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ†'],  # Ù†ÙØ­Ù’Ù†ÙØŒ Ø£ÙÙ†Ù’ØªÙØŒ Ø£ÙÙ†Ù’ØªÙÙ…Ù’ØŒ Ø£ÙÙ†Ù’ØªÙÙ†ÙÙ‘
    'CV-CV CV': ['Ø£Ù†ØªÙ…Ø§'],  # Ø£ÙÙ†Ù’ØªÙÙ…ÙØ§
    'CVC': ['Ù‡Ù…', 'Ù‡Ù†'],  # Ù‡ÙÙ…Ù’ØŒ Ù‡ÙÙ†ÙÙ‘
    }

        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©
    attached_patterns = {
    'CV': ['Ù€ÙŠ', 'Ù€ÙƒÙ', 'Ù€Ù‡'],  # Ù€ÙÙŠØŒ Ù€ÙÙƒÙØŒ Ù€ÙÙ‡
    'CV CV': ['Ù€Ù†ÙŠ', 'Ù€Ù†Ø§', 'Ù€Ù‡Ø§', 'Ù€Ù‡Ù…Ø§', 'Ù€ÙƒÙ…Ø§'],  # Ù€ÙÙ†ÙÙŠØŒ Ù€ÙÙ†ÙØ§ØŒ Ù€ÙÙ‡ÙØ§ØŒ Ù€ÙÙ‡ÙÙ…ÙØ§ØŒ Ù€ÙÙƒÙÙ…ÙØ§
    'CVC': ['Ù€Ùƒ', 'Ù€ÙƒÙ…', 'Ù€ÙƒÙ†', 'Ù€Ù‡Ù…', 'Ù€Ù‡Ù†'],  # Ù€ÙÙƒÙØŒ Ù€ÙÙƒÙÙ…Ù’ØŒ Ù€ÙÙƒÙÙ†ÙÙ‘ØŒ Ù€ÙÙ‡ÙÙ…Ù’ØŒ Ù€ÙÙ‡ÙÙ†ÙÙ‘
    }

    self.pattern_mappings = {**detached_patterns, **attached_patterns}

    logger.info(f"ğŸ“ ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(self.pattern_mappings)} Ù†Ù…Ø· Ù„Ù„Ø¶Ù…Ø§Ø¦Ø±")

    def map_syllables_to_pronoun(self, syllables: List[str]) -> List[str]:
    """Ø±Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¨Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©"""

        if not syllables:
    return []

        # ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    pattern = self._determine_syllable_pattern(syllables)

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
    matching_pronouns = self.pattern_mappings.get(pattern, [])

    return matching_pronouns

    def _determine_syllable_pattern(self, syllables: List[str]) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        if not syllables:
    return ""

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù‚Ø·Ø¹ Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹Ù‡
    pattern_parts = []

        for syllable in syllables:
    syllable_type = self._classify_syllable_type(syllable)
    pattern_parts.append(syllable_type)

    return ' '.join(pattern_parts)

    def _classify_syllable_type(self, syllable: str) -> str:
    """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ÙˆØ§Ø­Ø¯"""

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„ØªØ´ÙƒÙŠÙ„ Ù„Ù„ØªØ­Ù„ÙŠÙ„
    clean_syllable = re.sub(r'[ÙÙÙÙ‘Ù’]', '', syllable)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    consonants = re.findall(r'[Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ]', clean_syllable)
    vowels = re.findall(r'[Ø§ÙˆÙŠ]|[ÙÙÙ]', syllable)

    consonant_count = len(consonants)
    vowel_count = len(vowels)

        # ØªØµÙ†ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…Ø·
        if consonant_count == 1 and vowel_count == 1:
    return "CV"
        elif consonant_count == 2 and vowel_count == 1:
    return "CVC"
        elif consonant_count == 1 and vowel_count >= 2:
    return "CVV"
        elif consonant_count >= 2 and vowel_count >= 2:
    return "CVVC"
        else:
    return "CV"  # Ø§ÙØªØ±Ø§Ø¶ÙŠ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEP LEARNING MODEL FOR PRONOUN CLASSIFICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class ModelConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚"""

    input_size: int = 40  # Ø­Ø¬Ù… Ù…ÙŠØ²Ø§Øª MFCC
    hidden_size: int = 128  # Ø­Ø¬Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
    num_layers: int = 2  # Ø¹Ø¯Ø¯ Ø·Ø¨Ù‚Ø§Øª LSTM
    num_classes: int = 25  # Ø¹Ø¯Ø¯ ÙØ¦Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±
    dropout: float = 0.3  # Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·
    learning_rate: float = 0.001  # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    batch_size: int = 32  # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
    max_sequence_length: int = 100  # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„Ù„ØªØ³Ù„Ø³Ù„


class PronounClassificationDataGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    def __init__(self, pronouns_db: ArabicPronounsDatabase):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.pronouns_db = pronouns_db
    self.synthetic_data: List[Tuple[np.ndarray, int]] = []

    def generate_synthetic_mfcc_features(
    self, pronoun: PronounEntry, num_samples: int = 100
    ) -> List[np.ndarray]:
    """ØªÙˆÙ„ÙŠØ¯ Ù…ÙŠØ²Ø§Øª MFCC Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ù„Ø¶Ù…ÙŠØ±"""

    features_list = []

        for _ in range(num_samples):
            # Ù…Ø­Ø§ÙƒØ§Ø© Ù…ÙŠØ²Ø§Øª MFCC Ù„Ù„Ø¶Ù…ÙŠØ±
    sequence_length = np.random.randint(20, 80)  # Ø·ÙˆÙ„ Ù…ØªØºÙŠØ±
    mfcc_features = np.random.randn(sequence_length, 40)  # 40 Ù…ÙŠØ²Ø© MFCC

            # Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…Ø§Ø· Ù…Ù…ÙŠØ²Ø© Ù„Ù„Ø¶Ù…ÙŠØ±
            if pronoun.pronoun_type == PronounType.DETACHED:
                # Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø© ØªÙ…ÙŠÙ„ Ù„ØªÙƒÙˆÙ† Ø£Ø·ÙˆÙ„
    mfcc_features *= 1.2
            else:
                # Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø© ØªÙ…ÙŠÙ„ Ù„ØªÙƒÙˆÙ† Ø£Ù‚ØµØ±
    mfcc_features *= 0.8

            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®Øµ ÙˆØ§Ù„Ø¹Ø¯Ø¯
            if pronoun.person == PronounPerson.FIRST:
    mfcc_features[:0:10] += 0.5  # Ù…ÙŠØ²Ø§Øª Ù…Ù…ÙŠØ²Ø© Ù„Ù„Ù…ØªÙƒÙ„Ù…
            elif pronoun.person == PronounPerson.SECOND:
    mfcc_features[:10:20] += 0.5  # Ù…ÙŠØ²Ø§Øª Ù…Ù…ÙŠØ²Ø© Ù„Ù„Ù…Ø®Ø§Ø·Ø¨
            elif pronoun.person == PronounPerson.THIRD:
    mfcc_features[:20:30] += 0.5  # Ù…ÙŠØ²Ø§Øª Ù…Ù…ÙŠØ²Ø© Ù„Ù„ØºØ§Ø¦Ø¨

    features_list.append(mfcc_features)

    return features_list

    def generate_training_data(
    self, samples_per_pronoun: int = 100
    ) -> Tuple[List[np.ndarray], List[int]]:
    """ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""

    X_data = []
    y_data = []

        for pronoun in self.pronouns_db.pronouns:
            # ØªÙˆÙ„ÙŠØ¯ Ø¹ÙŠÙ†Ø§Øª Ù„ÙƒÙ„ Ø¶Ù…ÙŠØ±
    features_list = self.generate_synthetic_mfcc_features(
    pronoun, samples_per_pronoun
    )

            for features in features_list:
    X_data.append(features)
    y_data.append(pronoun.class_id)

    logger.info(
    f"ğŸ¯ ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(X_data)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨ Ù„Ù€ {len(self.pronouns_db.pronouns)} Ø¶Ù…ÙŠØ±"
    )  # noqa: E501

    return X_data, y_data


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC PRONOUNS GENERATOR FROM SYLLABLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPronounsGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""

    def __init__(self, syllables_database_path: str = "complete_arabic_syllable_inventory.json"):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.pronouns_db = ArabicPronounsDatabase()
    self.pattern_analyzer = SyllablePatternAnalyzer(syllables_database_path)
    self.model_config = ModelConfig()
    self.data_generator = PronounClassificationDataGenerator(self.pronouns_db)

    logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©")

    def generate_pronouns_from_syllables(self, syllables: List[str]) -> Dict[str, Any]:
    """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¹Ø·Ø§Ø©"""

        if not syllables:
    return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ø·Ø¹ Ù„Ù„ØªØ­Ù„ÙŠÙ„'}

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©
    matching_pronouns = self.pattern_analyzer.map_syllables_to_pronoun(syllables)

        # ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„ÙƒÙ„ Ø¶Ù…ÙŠØ± Ù…Ø·Ø§Ø¨Ù‚
    detailed_results = []

        for pronoun_text in matching_pronouns:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¶Ù…ÙŠØ± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    pronoun_entry = None
            for p in self.pronouns_db.pronouns:
                if p.text == pronoun_text:
    pronoun_entry = p
    break

            if pronoun_entry:
    detailed_results.append(
    {
    'text': pronoun_entry.text,
    'type': pronoun_entry.pronoun_type.value,
    'person': pronoun_entry.person.value,
    'number': pronoun_entry.number.value,
    'gender': pronoun_entry.gender.value,
    'pattern': pronoun_entry.syllable_pattern,
    'frequency': pronoun_entry.frequency_score,
    'class_id': pronoun_entry.class_id,
    }
    )

    return {
    'input_syllables': syllables,
    'syllable_pattern': self.pattern_analyzer._determine_syllable_pattern(
    syllables
    ),
    'matching_pronouns_count': len(matching_pronouns),
    'pronouns': detailed_results,
    'confidence': self._calculate_confidence(syllables, matching_pronouns),
    }

    def _calculate_confidence(
    self, syllables: List[str], matching_pronouns: List[str]
    ) -> float:
    """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""

        if not matching_pronouns:
    return 0.0

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
    syllable_count_factor = min(len(syllables) / 3.0, 1.0)  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    match_count_factor = 1.0 / len(matching_pronouns)  # Ø¹Ø¯Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª (Ø£Ù‚Ù„ = Ø£ÙØ¶Ù„)

    base_confidence = 0.7  # Ø«Ù‚Ø© Ø£Ø³Ø§Ø³ÙŠØ©

    return min(
    base_confidence + syllable_count_factor * 0.2 + match_count_factor * 0.1,
    1.0,
    )

    def analyze_pronoun_by_text(self, pronoun_text: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø¶Ù…ÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ù†ØµÙŠØ§Ù‹"""

        for pronoun in self.pronouns_db.pronouns:
            if pronoun.text == pronoun_text or pronoun.text == pronoun_text.replace(
    'Ù€', ''
    ):
    return {
    'found': True,
    'text': pronoun.text,
    'type': pronoun.pronoun_type.value,
    'person': pronoun.person.value,
    'number': pronoun.number.value,
    'gender': pronoun.gender.value,
    'pattern': pronoun.syllable_pattern,
    'frequency': pronoun.frequency_score,
    'class_id': pronoun.class_id,
    'usage_contexts': pronoun.usage_contexts,
    }

    return {'found': False, 'message': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¶Ù…ÙŠØ± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'}

    def get_all_pronouns_by_type(self, pronoun_type: str) -> List[Dict[str, Any]]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù…Ù† Ù†ÙˆØ¹ Ù…Ø¹ÙŠÙ†"""

    target_type = (
    PronounType.DETACHED if pronoun_type == "Ù…Ù†ÙØµÙ„" else PronounType.ATTACHED
    )

    results = []
        for pronoun in self.pronouns_db.pronouns:
            if pronoun.pronoun_type == target_type:
    results.append(
    {
    'text': pronoun.text,
    'person': pronoun.person.value,
    'number': pronoun.number.value,
    'gender': pronoun.gender.value,
    'pattern': pronoun.syllable_pattern,
    'frequency': pronoun.frequency_score,
    'class_id': pronoun.class_id,
    }
    )

    return sorted(results, key=lambda x: x['frequency'], reverse=True)

    def generate_comprehensive_report(self) -> Dict[str, Any]:
    """ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¹Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    stats = self.pronouns_db.get_statistics()

    report = {
    'system_info': {
    'version': '1.0.0',
    'total_pronouns': stats['total_pronouns'],
    'model_classes': self.model_config.num_classes,
    'syllable_patterns': len(self.pattern_analyzer.pattern_mappings),
    },
    'pronouns_distribution': stats,
    'pattern_analysis': {
    'available_patterns': list(
    self.pattern_analyzer.pattern_mappings.keys()
    ),
    'pattern_frequencies': {
    pattern: len(pronouns)
                    for pattern, pronouns in self.pattern_analyzer.pattern_mappings.items()
    },
    },
    'model_configuration': {
    'input_size': self.model_config.input_size,
    'hidden_size': self.model_config.hidden_size,
    'num_layers': self.model_config.num_layers,
    'num_classes': self.model_config.num_classes,
    },
    }

    return report

    def save_pronouns_database(self, output_path: str = "arabic_pronouns_database.json"):  # type: ignore[no-untyped def]
    """Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

    pronouns_data = []

        for pronoun in self.pronouns_db.pronouns:
    pronouns_data.append(
    {
    'text': pronoun.text,
    'type': pronoun.pronoun_type.value,
    'person': pronoun.person.value,
    'number': pronoun.number.value,
    'gender': pronoun.gender.value,
    'syllable_pattern': pronoun.syllable_pattern,
    'phonetic_features': pronoun.phonetic_features,
    'usage_contexts': pronoun.usage_contexts,
    'frequency_score': pronoun.frequency_score,
    'class_id': pronoun.class_id,
    }
    )

    output_data = {
    'metadata': {
    'version': '1.0.0',
    'total_pronouns': len(pronouns_data),
    'generation_date': '2025-07 24',
    'description': 'Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØµÙ„Ø© ÙˆØ§Ù„Ù…Ù†ÙØµÙ„Ø©',
    },
    'class_mapping': self.pronouns_db.class_mapping,
    'syllable_patterns': self.pronouns_db.syllable_patterns,
    'pronouns': pronouns_data,
    }

        with open(output_path, 'w', encoding='utf 8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± ÙÙŠ: {output_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION AND TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def demonstrate_arabic_pronouns_generator():  # type: ignore[no-untyped def]
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    print("ğŸ¯ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©")
    print("=" * 60)

    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯
    generator = ArabicPronounsGenerator()

    # Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©
    test_cases = [
        # Ø¶Ù…Ø§Ø¦Ø± Ù…Ù†ÙØµÙ„Ø©
    ['Ø£Ù', 'Ù†ÙØ§'],  # Ø£Ù†Ø§
    ['Ù‡Ù', 'ÙˆÙ'],  # Ù‡Ùˆ
    ['Ù‡Ù', 'ÙŠÙ'],  # Ù‡ÙŠ
    ['Ù†ÙØ­Ù’', 'Ù†Ù'],  # Ù†Ø­Ù†
    ['Ø£ÙÙ†Ù’', 'ØªÙ'],  # Ø£Ù†Øª
    ['Ù‡ÙÙ…'],  # Ù‡Ù…
        # Ø¶Ù…Ø§Ø¦Ø± Ù…ØªØµÙ„Ø©
    ['Ù†ÙÙŠ'],  # Ù€Ù†ÙŠ
    ['Ù‡ÙØ§'],  # Ù€Ù‡Ø§
    ['ÙƒÙ'],  # Ù€Ùƒ
    ['Ù‡ÙÙ…'],  # Ù€Ù‡Ù…
    ]

    print("\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹:")
    print(" " * 50)

    for i, syllables in enumerate(test_cases, 1):
    print(f"\n{i}. Ø§Ù„Ù…Ø¯Ø®Ù„: {syllables}")

    result = generator.generate_pronouns_from_syllables(syllables)

        if result.get('pronouns'):
    print(f"   Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠ: {result['syllable_pattern']}")
    print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª: {result['matching_pronouns_count']}")
    print(f"   Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.2f}")

            for j, pronoun in enumerate(result['pronouns'][:3], 1):
    print(
    f"   {j}. {pronoun['text']} ({pronoun['type']}) - {pronoun['person']}/{pronoun['number']}/{pronoun['gender']}"
    )
        else:
    print("   âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§Øª")

    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:")
    print(" " * 30)

    report = generator.generate_comprehensive_report()

    print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±: {report['system_info']['total_pronouns']}")
    print(f"   Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…Ù†ÙØµÙ„Ø©: {report['pronouns_distribution']['detached_count']}")
    print(f"   Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©: {report['pronouns_distribution']['attached_count']}")
    print(f"   Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©: {report['system_info']['syllable_patterns']}")

    # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ§Ø­Ø©
    print("\nğŸ¨ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    for pattern, frequency in report['pattern_analysis']['pattern_frequencies'].items():
    print(f"   â€¢ {pattern}: {frequency} Ø¶Ù…ÙŠØ±")

    # Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("\nğŸ’¾ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    generator.save_pronouns_database()

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø¹Ø±Ø¶ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©!")
    print("ğŸ¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©!")


if __name__ == "__main__":
    demonstrate_arabic_pronouns_generator()
