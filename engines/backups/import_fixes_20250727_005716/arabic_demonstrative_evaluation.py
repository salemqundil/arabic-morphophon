#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Arabic Demonstrative Pronouns Evaluation System
=======================================================
Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Comprehensive evaluation, benchmarking, and analysis system for Arabic
demonstrative pronouns classification with advanced metrics and visualizations.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - ADVANCED EVALUATION
Date: 2025-07-24
Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc


import torch  # noqa: F401
import numpy as np  # noqa: F401
import matplotlib.pyplot as plt  # noqa: F401
import seaborn as sns  # noqa: F401
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
)  # noqa: F401
from sklearn.metrics import (
    precision_recall_fscore_support,
    roc_curve,
    auc,
)  # noqa: F401
import pandas as pd  # noqa: F401
import json  # noqa: F401
import time  # noqa: F401
from typing import Dict, List, Tuple, Any
from arabic_demonstrative_pronouns_deep_model import (  # noqa: F401
    DemonstrativePronounInference,
    DEMONSTRATIVE_PRONOUNS,
    create_synthetic_data,
    DemonstrativePhoneticProcessor,
)
import warnings  # noqa: F401

warnings.filterwarnings('ignore')

# Ø¥Ø¹Ø¯Ø§Ø¯ matplotlib Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class DemonstrativeEvaluationSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©"""

    def __init__(self):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.inference = DemonstrativePronounInference()
    self.processor = DemonstrativePhoneticProcessor()
    self.results = {}

        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    self.test_data = {
    "Ù‡Ø°Ø§": [["Ù‡ÙØ§", "Ø°ÙØ§"], ["Ù‡ÙÙ€", "Ø°ÙØ§"], ["Ù‡Ù", "Ø°ÙØ§"]],
    "Ù‡Ø°Ù‡": [["Ù‡ÙØ§", "Ø°ÙÙ‡Ù"], ["Ù‡ÙØ§", "Ø°ÙÙ‡"], ["Ù‡ÙÙ€", "Ø°ÙÙ‡Ù"]],
    "Ø°Ù„Ùƒ": [["Ø°ÙØ§", "Ù„ÙÙƒÙ"], ["Ø°ÙØ§Ù„Ù", "ÙƒÙ"], ["Ø°Ù", "Ù„ÙÙƒÙ"]],
    "ØªÙ„Ùƒ": [["ØªÙÙ„", "ÙƒÙ"], ["ØªÙ", "Ù„ÙÙƒÙ"], ["ØªÙÙ„Ù’", "ÙƒÙ"]],
    "Ù‡Ø°Ø§Ù†": [["Ù‡ÙØ§", "Ø°ÙØ§", "Ù†Ù"], ["Ù‡ÙØ§", "Ø°ÙØ§Ù†"], ["Ù‡ÙÙ€", "Ø°ÙØ§", "Ù†Ù"]],
    "Ù‡Ø°ÙŠÙ†": [["Ù‡ÙØ§", "Ø°ÙÙŠÙ’", "Ù†Ù"], ["Ù‡ÙØ§", "Ø°ÙÙŠÙ†"], ["Ù‡ÙÙ€", "Ø°ÙÙŠÙ’", "Ù†Ù"]],
    "Ù‡Ø§ØªØ§Ù†": [["Ù‡ÙØ§", "ØªÙØ§", "Ù†Ù"], ["Ù‡ÙØ§", "ØªÙØ§Ù†"], ["Ù‡ÙÙ€", "ØªÙØ§", "Ù†Ù"]],
    "Ù‡Ø§ØªÙŠÙ†": [["Ù‡ÙØ§", "ØªÙÙŠÙ’", "Ù†Ù"], ["Ù‡ÙØ§", "ØªÙÙŠÙ†"], ["Ù‡ÙÙ€", "ØªÙÙŠÙ’", "Ù†Ù"]],
    "Ù‡Ø¤Ù„Ø§Ø¡": [["Ù‡ÙØ§", "Ø¤Ù", "Ù„ÙØ§", "Ø¡Ù"], ["Ù‡ÙØ§", "Ø¤ÙÙ„Ø§Ø¡"], ["Ù‡ÙÙ€", "Ø¤Ù", "Ù„Ø§Ø¡"]],
    "Ø£ÙˆÙ„Ø¦Ùƒ": [["Ø£ÙÙˆ", "Ù„ÙØ§", "Ø¦Ù", "ÙƒÙ"], ["Ø£ÙÙˆÙ„ÙØ§", "Ø¦ÙÙƒÙ"], ["Ø£ÙÙˆ", "Ù„ÙØ§Ø¦ÙÙƒÙ"]],
    "Ù‡Ù†Ø§": [["Ù‡Ù", "Ù†ÙØ§"], ["Ù‡ÙÙ†ÙØ§"], ["Ù‡ÙÙ€", "Ù†ÙØ§"]],
    "Ù‡Ù†Ø§Ùƒ": [["Ù‡Ù", "Ù†ÙØ§", "ÙƒÙ"], ["Ù‡ÙÙ†ÙØ§", "ÙƒÙ"], ["Ù‡ÙÙ€", "Ù†ÙØ§ÙƒÙ"]],
    "Ù‡Ø§Ù‡Ù†Ø§": [["Ù‡ÙØ§", "Ù‡Ù", "Ù†ÙØ§"], ["Ù‡ÙØ§Ù‡ÙÙ†ÙØ§"], ["Ù‡ÙÙ€", "Ù‡ÙÙ†ÙØ§"]],
    "Ù‡Ù†Ø§Ù„Ùƒ": [["Ù‡Ù", "Ù†ÙØ§", "Ù„Ù", "ÙƒÙ"], ["Ù‡ÙÙ†ÙØ§", "Ù„ÙÙƒÙ"], ["Ù‡ÙÙ€", "Ù†ÙØ§Ù„ÙÙƒÙ"]],
    "Ø°Ø§Ù†Ùƒ": [["Ø°ÙØ§", "Ù†Ù", "ÙƒÙ"], ["Ø°ÙØ§Ù†ÙÙƒÙ"], ["Ø°ÙÙ€", "Ù†ÙÙƒÙ"]],
    "ØªØ§Ù†Ùƒ": [["ØªÙØ§", "Ù†Ù", "ÙƒÙ"], ["ØªÙØ§Ù†ÙÙƒÙ"], ["ØªÙÙ€", "Ù†ÙÙƒÙ"]],
    }

    def generate_comprehensive_test_data(
    self, num_samples: int = 500
    ) -> Tuple[List[np.ndarray], List[int], List[str]]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„Ø©"""

    X = []
    y = []
    syllables_list = []

        # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        for pronoun, variations in self.test_data.items():
            for variation in variations:
    features = self.processor.extract_features(variation)
    X.append(features)
    y.append(list(DEMONSTRATIVE_PRONOUNS.values()).index(pronoun))
    syllables_list.append(variation)

        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©
    X_synthetic, y_synthetic = create_synthetic_data(self.processor, num_samples)
    X.extend(X_synthetic)
    y.extend(y_synthetic)

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
        for _ in range(len(X_synthetic)):
    syllables_list.append(["synthetic"])

    return X, y, syllables_list

    def evaluate_model_performance(self, model_type: str = 'gru') -> Dict[str, Any]:
    """ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„"""

    print(f"ğŸ” ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ {model_type.upper()}...")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    X_test, y_test, syllables_test = self.generate_comprehensive_test_data(300)

        # Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
    predictions = []
    confidences = []
    inference_times = []

        for i, features in enumerate(X_test):
    start_time = time.time()

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø£Ùˆ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
            if syllables_test[i] != ["synthetic"]:
    syllables = syllables_test[i]
            else:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
    syllables = ["Ù…Ù‚", "Ø·Ø¹"]  # placeholder

            try:
    prediction = self.inference.predict_syllables(syllables, model_type)
    confidence_result = self.inference.predict_with_confidence(
    syllables, model_type
    )

    pred_id = list(DEMONSTRATIVE_PRONOUNS.values()).index(prediction)
    predictions.append(pred_id)
    confidences.append(confidence_result['confidence'])

            except Exception as e:
    print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ {i: {e}}")
    predictions.append(0)  # Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    confidences.append(0.0)

    inference_times.append(time.time() - start_time)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    accuracy = accuracy_score(y_test, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
    y_test, predictions, average='weighted'
    )

        # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³
    cm = confusion_matrix(y_test, predictions)

        # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
        class_names = list(DEMONSTRATIVE_PRONOUNS.values())
        class_report = classification_report(
    y_test, predictions, target_names=class_names, output_dict=True
    )

    results = {
    'model_type': model_type,
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1_score': f1,
    'confusion_matrix': cm,
    'classification_report': class_report,
    'avg_confidence': np.mean(confidences),
    'avg_inference_time': np.mean(inference_times),
    'predictions': predictions,
    'true_labels': y_test,
    'confidences': confidences,
    }

    return results

    def benchmark_all_models(self) -> Dict[str, Dict[str, Any]]:
    """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

    print("ğŸ“Š Ø¨Ø¯Ø¡ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„...")

        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹
    print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
    self.inference.train_all_models(num_samples=1000)

        # ØªÙ‚ÙŠÙŠÙ… ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
    models = ['lstm', 'gru', 'transformer']
    results = {}

        for model_type in models:
            try:
    results[model_type] = self.evaluate_model_performance(model_type)
    print(
    f"âœ… {model_type.upper():} Ø¯Ù‚Ø© {results[model_type]['accuracy']:.3f}}"
    )  # noqa: E501
            except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… {model_type: {e}}")
    results[model_type] = None

    self.results = results
    return results

    def create_performance_visualization(self):  # type: ignore[no-untyped def]
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""

        if not self.results:
    print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØµÙˆØ±. ÙŠØ¬Ø¨ ØªØ´ØºÙŠÙ„ benchmark_all_models Ø£ÙˆÙ„Ø§Ù‹.")
    return

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', fontsize=16, y=0.95)

        # 1. Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯Ù‚Ø©
    models = []
    accuracies = []
    f1_scores = []

        for model_type, result in self.results.items():
            if result:
    models.append(model_type.upper())
    accuracies.append(result['accuracy'])
    f1_scores.append(result['f1_score'])

    axes[0, 0].bar(
    models, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8
    )
    axes[0, 0].set_title('Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬')
    axes[0, 0].set_ylabel('Ø§Ù„Ø¯Ù‚Ø©')
    axes[0, 0].set_ylim(0, 1)

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        for i, v in enumerate(accuracies):
    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

        # 2. Ù…Ù‚Ø§Ø±Ù†Ø© F1 Score
    axes[0, 1].bar(
    models, f1_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8
    )
    axes[0, 1].set_title('F1 Score Ø§Ù„Ù†Ù…Ø§Ø°Ø¬')
    axes[0, 1].set_ylabel('F1 Score')
    axes[0, 1].set_ylim(0, 1)

        for i, v in enumerate(f1_scores):
    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

        # 3. Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„
    best_model = max(
    self.results.items(), key=lambda x: x[1]['accuracy'] if x[1] else 0
    )
        if best_model[1]:
    cm = best_model[1]['confusion_matrix']
            class_names = list(DEMONSTRATIVE_PRONOUNS.values())[: cm.shape[0]]

    sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_names[: min(10, len(class_names))],
    yticklabels=class_names[: min(10, len(class_names))],
    ax=axes[1, 0],
    )
    axes[1, 0].set_title(f'Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ - {best_model[0].upper()}')
    axes[1, 0].set_xlabel('Ø§Ù„ØªÙ†Ø¨Ø¤')
    axes[1, 0].set_ylabel('Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©')

        # 4. ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø«Ù‚Ø©
        if best_model[1]:
    confidences = best_model[1]['confidences']
    axes[1, 1].hist(
    confidences, bins=20, color='#96CEB4', alpha=0.8, edgecolor='black'
    )
    axes[1, 1].set_title(f'ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø«Ù‚Ø© - {best_model[0].upper()}')
    axes[1, 1].set_xlabel('Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©')
    axes[1, 1].set_ylabel('Ø§Ù„Ø¹Ø¯Ø¯')
    axes[1, 1].axvline(
    np.mean(confidences),
    color='red',
    linestyle='- ',
    label=f'Ø§Ù„Ù…ØªÙˆØ³Ø·: {np.mean(confidences):.3f}',
    )
    axes[1, 1].legend()

    plt.tight_layout()
    plt.savefig(
    'demonstrative_pronouns_evaluation.png', dpi=300, bbox_inches='tight'
    )
    plt.show()

    print("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù…Ø®Ø·Ø·Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙÙŠ: demonstrative_pronouns_evaluation.png")

    def generate_detailed_report(self) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¹Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡"""

        if not self.results:
    return "âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ§Ø­Ø©"

    report = "ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n"
    report += "=" * 60 + "\n\n"

        # Ù…Ù„Ø®Øµ Ø¹Ø§Ù…
    report += "ğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡:\n"
    report += " " * 20 + "\n"

        for model_type, result in self.results.items():
            if result:
    report += f"ğŸ”¹ {model_type.upper()}:\n"
    report += f"   â€¢ Ø§Ù„Ø¯Ù‚Ø©: {result['accuracy']:.3f}\n"
    report += f"   â€¢ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø±Ø¬Ø­Ø©: {result['precision']:.3f}\n"
    report += f"   â€¢ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡: {result['recall']:.3f}\n"
    report += f"   â€¢ F1-Score: {result['f1_score']:.3f}\n"
    report += f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {result['avg_confidence']:.3f}\n"
    report += f"   â€¢ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬: {result['avg_inference_time']*1000:.2f} Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©\n\n"

        # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
        if any(self.results.values()):
    best_model = max(
    [(k, v) for k, v in self.results.items() if v],
    key=lambda x: x[1]['accuracy'],
    )

    report += f"ğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model[0].upper()}\n"
    report += f"   Ø¯Ù‚Ø©: {best_model[1]['accuracy']:.3f}\n\n"

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ
    report += "ğŸ“‹ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„ÙƒÙ„ ÙØ¦Ø©:\n"
    report += " " * 30 + "\n"

        if best_model[1]:
            class_report = best_model[1]['classification_report']
            for class_name, metrics in class_report.items():
                if isinstance(metrics, dict) and class_name not in [
    'accuracy',
    'macro avg',
    'weighted avg',
    ]:
    report += f"ğŸ”¸ {class_name}:\n"
    report += f"   Ø¯Ù‚Ø©: {metrics['precision']:.3f}, "
    report += f"Ø§Ø³ØªØ¯Ø¹Ø§Ø¡: {metrics['recall']:.3f}, "
    report += f"F1: {metrics['f1 score']:.3f}\n"

        # ØªÙˆØµÙŠØ§Øª
    report += "\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:\n"
    report += " " * 15 + "\n"

        if any(self.results.values()):
    gru_acc = (
    self.results.get('gru', {}).get('accuracy', 0)
                if self.results.get('gru')
                else 0
    )
    lstm_acc = (
    self.results.get('lstm', {}).get('accuracy', 0)
                if self.results.get('lstm')
                else 0
    )

            if gru_acc > 0.9:
    report += "âœ… Ù†Ù…ÙˆØ°Ø¬ GRU ÙŠØ¸Ù‡Ø± Ø£Ø¯Ø§Ø¡Ù‹ Ù…Ù…ØªØ§Ø²Ø§Ù‹ ÙˆÙŠÙÙ†ØµØ­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬\n"
            elif lstm_acc > 0.8:
    report += "âœ… Ù†Ù…ÙˆØ°Ø¬ LSTM ÙŠØ¸Ù‡Ø± Ø£Ø¯Ø§Ø¡Ù‹ Ø¬ÙŠØ¯Ø§Ù‹ ÙˆÙ…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…\n"
            else:
    report += "âš ï¸ ÙŠÙÙ†ØµØ­ Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±\n"

    return report

    def save_results(self, filename: str = "demonstrative_evaluation_results.json"):  # type: ignore[no-untyped def]
    """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù"""

        if not self.results:
    print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ø­ÙØ¸")
    return

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø­ÙØ¸
    serializable_results = {}

        for model_type, result in self.results.items():
            if result:
    serializable_results[model_type] = {
    'accuracy': float(result['accuracy']),
    'precision': float(result['precision']),
    'recall': float(result['recall']),
    'f1_score': float(result['f1_score']),
    'avg_confidence': float(result['avg_confidence']),
    'avg_inference_time': float(result['avg_inference_time']),
    'confusion_matrix': result['confusion_matrix'].tolist(),
    'classification_report': result['classification_report'],
    }

        with open(filename, 'w', encoding='utf 8') as f:
    json.dump(serializable_results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {filename}")


def main():  # type: ignore[no-untyped def]
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"""

    print("ğŸ“Š Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 55)

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    evaluator = DemonstrativeEvaluationSystem()

    # Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø´Ø§Ù…Ù„...")
    evaluator.benchmark_all_models()

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙˆØ±Ø§Øª
    print("\nğŸ“ˆ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª...")
    evaluator.create_performance_visualization()

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    print("\nğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„...")
    report = evaluator.generate_detailed_report()
    print(report)

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    evaluator.save_results()

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    with open("demonstrative_evaluation_report.txt", "w", encoding="utf 8") as f:
    f.write(report)

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„!")
    print("ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©:")
    print("   â€¢ demonstrative_pronouns_evaluation.png")
    print("   â€¢ demonstrative_evaluation_results.json")
    print("   â€¢ demonstrative_evaluation_report.txt")


if __name__ == "__main__":
    main()
