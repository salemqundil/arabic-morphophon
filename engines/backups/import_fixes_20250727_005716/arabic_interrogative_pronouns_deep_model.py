#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Interrogative Pronouns Deep Learning Models
================================================
Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Advanced deep learning models (LSTM, GRU, Transformer) for Arabic interrogative
pronouns classification from phonetic features and syllable patterns.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - DEEP LEARNING MODELS
Date: 2025-07-24
Encoding: UTF 8
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import random
import logging
from typing import Dict, List, Tuple, Any
import math

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø°Ø±Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERROGATIVE PRONOUNS MAPPING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


INTERROGATIVE_PRONOUNS = {
    0: "Ù…ÙÙ†",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ
    1: "Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡
    2: "Ù…ÙØªÙÙ‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù†
    3: "Ø£ÙÙŠÙ’Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù†
    4: "ÙƒÙÙŠÙ’ÙÙ",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙŠÙÙŠØ©
    5: "ÙƒÙÙ…Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙ…ÙŠØ©
    6: "Ø£ÙÙŠÙ‘",  # Ù„Ù„Ø§Ø®ØªÙŠØ§Ø±
    7: "Ù„ÙÙ…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨
    8: "Ù…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ (Ù…Ø¨Ø§Ø´Ø±)
    9: "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
    10: "Ø£ÙÙ†ÙÙ‘Ù‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø·Ø±ÙŠÙ‚Ø©
    11: "Ù„ÙÙ…Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨ (Ù…Ø®ØªØµØ±)
    12: "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ«ÙŠØ±
    13: "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§",  # Ø§Ù„Ù†Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠ
    14: "Ù…ÙÙ‡Ù’Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡
    15: "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ù…ÙƒØ§Ù†
    16: "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø©
    17: "Ù…ÙÙ†Ù’ Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø¨ØªØ£ÙƒÙŠØ¯
}

PRONOUN_TO_ID = {v: k for k, v in INTERROGATIVE_PRONOUNS.items()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHONETIC PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class InterrogativePhoneticProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØªÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):

    self.arabic_phonemes = {
            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    'Ø¨': 0,
    'Øª': 1,
    'Ø«': 2,
    'Ø¬': 3,
    'Ø­': 4,
    'Ø®': 5,
    'Ø¯': 6,
    'Ø°': 7,
    'Ø±': 8,
    'Ø²': 9,
    'Ø³': 10,
    'Ø´': 11,
    'Øµ': 12,
    'Ø¶': 13,
    'Ø·': 14,
    'Ø¸': 15,
    'Ø¹': 16,
    'Øº': 17,
    'Ù': 18,
    'Ù‚': 19,
    'Ùƒ': 20,
    'Ù„': 21,
    'Ù…': 22,
    'Ù†': 23,
    'Ù‡': 24,
    'Ùˆ': 25,
    'ÙŠ': 26,
    'Ø¡': 27,
    'Ø¢': 28,
    'Ø£': 29,
    'Ø¥': 30,
    'Ø¦': 31,
    'Ø¤': 32,
    'Ø©': 33,
            # Ø§Ù„Ø­Ø±ÙƒØ§Øª
    'Ù': 34,
    'Ù': 35,
    'Ù': 36,
    'Ù’': 37,
    'Ù‹': 38,
    'ÙŒ': 39,
    'Ù': 40,
            # Ø£ØµÙˆØ§Øª Ù…Ø±ÙƒØ¨Ø© Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    'Ù…Ù': 41,
    'Ø£Ù': 42,
    'ÙƒÙ': 43,
    'Ù„Ù': 44,
    'Ù†Ù’': 45,
    'ØªÙ': 46,
    'ÙŠÙ’': 47,
    'Ø°Ù': 48,
    'Ù‰': 49,
    'Ø§Ø°Ù': 50,
    'Ù†Ù': 51,
    'ÙÙ': 52,
    'ÙŠÙÙ‘': 53,
    'Ø§Ù†': 54,
    'Ù‡Ù’': 55,
    }

    self.phoneme_features = self._initialize_phoneme_features()
    self.vocab_size = len(self.arabic_phonemes)

    def _initialize_phoneme_features(self) -> Dict[str, np.ndarray]:
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ø£ØµÙˆØ§Øª"""

    features = {}

        # Ø®ØµØ§Ø¦Øµ ØµÙˆØªÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ© (12 Ø£Ø¨Ø¹Ø§Ø¯)
        for phoneme, idx in self.arabic_phonemes.items():
    feature_vector = np.zeros(12)

            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª
    consonants = [
    'Ø¨',
    'Øª',
    'Ø«',
    'Ø¬',
    'Ø­',
    'Ø®',
    'Ø¯',
    'Ø°',
    'Ø±',
    'Ø²',
    'Ø³',
    'Ø´',
    'Øµ',
    'Ø¶',
    'Ø·',
    'Ø¸',
    'Ø¹',
    'Øº',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    ]

            if phoneme in consonants:
    feature_vector[0] = 1.0  # ØµØ§Ù…Øª

    vowels = ['Ùˆ', 'ÙŠ', 'Ø§', 'Ù', 'Ù', 'Ù', 'Ø¢']
            if phoneme in vowels:
    feature_vector[1] = 1.0  # ØµØ§Ø¦Øª

            # Ø£ØµÙˆØ§Øª Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    common_interrogatives = ['Ù…', 'Ø£', 'Ùƒ', 'Ù„', 'Ù†', 'Øª', 'ÙŠ', 'Ø°', 'Ù‡', 'Ù']
            if any(c in phoneme for c in common_interrogatives):
    feature_vector[2] = 1.0

            # Ø£ØµÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ù…Ù…ÙŠØ²Ø©
            if phoneme in ['Ù…Ù', 'Ø£Ù', 'ÙƒÙ', 'Ù„Ù']:
    feature_vector[3] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ù†ÙÙŠØ©
            if phoneme in ['Ù„', 'Ø±', 'Ù†', 'Ù…']:
    feature_vector[4] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø§Ø­ØªÙƒØ§ÙƒÙŠØ©
    fricatives = ['Ù', 'Ø«', 'Ø°', 'Ø³', 'Ø²', 'Ø´', 'Øµ', 'Ø®', 'Øº', 'Ø­', 'Ø¹', 'Ù‡']
            if phoneme in fricatives:
    feature_vector[5] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±ÙŠØ©
    stops = ['Ø¨', 'Øª', 'Ø¯', 'Ø·', 'Ùƒ', 'Ù‚', 'Ø¡']
            if phoneme in stops:
    feature_vector[6] = 1.0

            # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    long_vowels = ['Ø§', 'Ùˆ', 'ÙŠ', 'Ø¢', 'Ù‰']
            if phoneme in long_vowels:
    feature_vector[7] = 1.0

            # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
    short_vowels = ['Ù', 'Ù', 'Ù']
            if phoneme in short_vowels:
    feature_vector[8] = 1.0

            # Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù†Ø·Ù‚
    labial = ['Ø¨', 'Ù', 'Ù…', 'Ùˆ']
    dental = ['Øª', 'Ø¯', 'Ø«', 'Ø°', 'Ù„', 'Ù†', 'Ø±', 'Ø²', 'Ø³', 'Ø´']
    velar = ['Ùƒ', 'Ù‚', 'Øº', 'Ø®']

            if phoneme in labial:
    feature_vector[9] = 1.0  # Ø´ÙÙˆÙŠ
            elif phoneme in dental:
    feature_vector[9] = 0.5  # Ù„Ø«ÙˆÙŠ
            elif phoneme in velar:
    feature_vector[9] = 0.0  # Ø®Ù„ÙÙŠ

            # Ø§Ù„ØªØ´Ø¯ÙŠØ¯ ÙˆØ§Ù„Ø³ÙƒÙˆÙ†
            if phoneme in ['Ù‹', 'ÙŒ', 'Ù', 'Ù’', 'Ù‘']:
    feature_vector[10] = 1.0

            # Ø£ØµÙˆØ§Øª Ù…Ø±ÙƒØ¨Ø© Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    interrogative_compounds = ['Ù…Ù', 'Ø£Ù', 'ÙƒÙ', 'Ù„Ù', 'Ø§Ø°Ù', 'ÙŠÙÙ‘']
            if phoneme in interrogative_compounds:
    feature_vector[11] = 1.0

    features[phoneme] = feature_vector

    return features

    def syllables_to_phonemes(self, syllables: List[str]) -> List[str]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ Ø£ØµÙˆØ§Øª"""

    phonemes = []

        for syllable in syllables:
            # ØªÙÙƒÙŠÙƒ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø¥Ù„Ù‰ Ø£ØµÙˆØ§Øª
            if syllable in self.arabic_phonemes:
                # Ù…Ù‚Ø·Ø¹ Ù…Ø±ÙƒØ¨ Ù…Ø¹Ø±ÙˆÙ
    phonemes.append(syllable)
            else:
                # ØªÙÙƒÙŠÙƒ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø­Ø±Ù Ø¨Ø­Ø±Ù
                for char in syllable:
                    if char in self.arabic_phonemes:
    phonemes.append(char)

    return phonemes

    def encode_phonemes(self, phonemes: List[str], max_length: int = 15) -> np.ndarray:
    """ØªØ±Ù…ÙŠØ² Ø§Ù„Ø£ØµÙˆØ§Øª Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ"""

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ±Ù…ÙŠØ²
    encoded = np.zeros((max_length, self.vocab_size + 12))  # vocab + features

        for i, phoneme in enumerate(phonemes[:max_length]):
            if phoneme in self.arabic_phonemes:
                # One-hot encoding Ù„Ù„ØµÙˆØª
    idx = self.arabic_phonemes[phoneme]
    encoded[i, idx] = 1.0

                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
                if phoneme in self.phoneme_features:
    features = self.phoneme_features[phoneme]
    encoded[i, self.vocab_size : self.vocab_size + 12] = features

    return encoded

    def extract_features(self, syllables: List[str]) -> np.ndarray:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    phonemes = self.syllables_to_phonemes(syllables)
    encoded = self.encode_phonemes(phonemes)

    return encoded


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEP LEARNING MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class InterrogativeLSTM(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„ØªØµÙ†ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
    self,
    input_size: int,
    hidden_size: int,
    num_layers: int,
    num_classes: int, dropout: float = 0.3):
    super(InterrogativeLSTM, self).__init__()

    self.hidden_size = hidden_size
    self.num_layers = num_layers

        # Ø·Ø¨Ù‚Ø© LSTM Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
    self.lstm = nn.LSTM()
    input_size=input_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
    batch_first=True,
    bidirectional=True,
    dropout=dropout if num_layers > 1 else 0)

        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    self.attention = nn.Linear(hidden_size * 2, 1)

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
    self.classifier = nn.Sequential()
    nn.Linear(hidden_size * 2, hidden_size),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(hidden_size, hidden_size // 2),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(hidden_size // 2, num_classes))

    def forward(self, x):
    def forward(self, x):
    batch_size = x.size(0)

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©
    h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)
    c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)

        # LSTM
    lstm_out, _ = self.lstm(x, (h0, c0))

        # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    attention_weights = F.softmax(self.attention(lstm_out), dim=1)
    attended_output = torch.sum(attention_weights * lstm_out, dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(attended_output)

    return output


class InterrogativeGRU(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ GRU Ù„ØªØµÙ†ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
    self,
    input_size: int,
    hidden_size: int,
    num_layers: int,
    num_classes: int, dropout: float = 0.3):
    super(InterrogativeGRU, self).__init__()

    self.hidden_size = hidden_size
    self.num_layers = num_layers

        # Ø·Ø¨Ù‚Ø© GRU Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
    self.gru = nn.GRU()
    input_size=input_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
    batch_first=True,
    bidirectional=True,
    dropout=dropout if num_layers > 1 else 0)

        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
    self.multihead_attention = nn.MultiheadAttention()
    embed_dim=hidden_size * 2, num_heads=4, dropout=dropout, batch_first=True
    )

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
    self.classifier = nn.Sequential()
    nn.Linear(hidden_size * 2, hidden_size),
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(hidden_size, num_classes))

    def forward(self, x):
    def forward(self, x):
    batch_size = x.size(0)

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
    h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)

        # GRU
    gru_out, _ = self.gru(x, h0)

        # Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
    attn_out, _ = self.multihead_attention(gru_out, gru_out, gru_out)

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Ù…ØªÙˆØ³Ø·)
    pooled_output = torch.mean(attn_out, dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(pooled_output)

    return output


class InterrogativeTransformer(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Transformer Ù„ØªØµÙ†ÙŠÙ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
    self,
    input_size: int,
    d_model: int,
    nhead: int,
    num_layers: int,
    num_classes: int, dropout: float = 0.1):
    super(InterrogativeTransformer, self).__init__()

    self.d_model = d_model

        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø¯Ø®Ù„
    self.input_projection = nn.Linear(input_size, d_model)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹
    self.pos_encoding = PositionalEncoding(d_model, dropout)

        # Ø·Ø¨Ù‚Ø§Øª Transformer
    encoder_layer = nn.TransformerEncoderLayer()
    d_model=d_model,
    nhead=nhead,
    dim_feedforward=d_model * 4,
    dropout=dropout,
    activation='gelu',
    batch_first=True)

    self.transformer_encoder = nn.TransformerEncoder()
    encoder_layer, num_layers=num_layers
    )

        # Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ
    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))

        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    self.classifier = nn.Sequential()
    nn.LayerNorm(d_model),
    nn.Linear(d_model, d_model // 2),
    nn.GELU(),
    nn.Dropout(dropout),
    nn.Linear(d_model // 2, num_classes))

    def forward(self, x):
    def forward(self, x):
    batch_size = x.size(0)

        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø¯Ø®Ù„
    x = self.input_projection(x)

        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ
    cls_tokens = self.cls_token.expand(batch_size, -1, -1)
    x = torch.cat([cls_tokens, x], dim=1)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹
    x = self.pos_encoding(x)

        # Transformer
    transformer_out = self.transformer_encoder(x)

        # Ø£Ø®Ø° Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ
    cls_output = transformer_out[: 0]

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(cls_output)

    return output


class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ Ù„Ù„Ù€ Transformer"""

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):

    super(PositionalEncoding, self).__init__()

    self.dropout = nn.Dropout(p=dropout)

    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

    div_term = torch.exp()
    torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
    )

    pe[: 0::2] = torch.sin(position * div_term)
    pe[: 1::2] = torch.cos(position * div_term)

    pe = pe.unsqueeze(0).transpose(0, 1)
    self.register_buffer('pe', pe)

    def forward(self, x):
    def forward(self, x):
    x = x + self.pe[: x.size(1), :].transpose(0, 1)
    return self.dropout(x)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def create_synthetic_data()
    processor: InterrogativePhoneticProcessor, num_samples: int = 1200
) -> Tuple[List[np.ndarray], List[int]]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©"""

    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    base_data = {
    "Ù…ÙÙ†": [["Ù…ÙÙ†Ù’"], ["Ù…Ù", "Ù†Ù’"]],
    "Ù…ÙØ§": [["Ù…ÙØ§"], ["Ù…Ù", "Ø§"]],
    "Ù…ÙØªÙÙ‰": [["Ù…Ù", "ØªÙÙ‰"], ["Ù…Ù", "ØªÙ", "Ù‰"]],
    "Ø£ÙÙŠÙ’Ù†Ù": [["Ø£ÙÙŠÙ’", "Ù†Ù"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù"]],
    "ÙƒÙÙŠÙ’ÙÙ": [["ÙƒÙÙŠÙ’", "ÙÙ"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ"]],
    "ÙƒÙÙ…Ù’": [["ÙƒÙÙ…Ù’"], ["ÙƒÙ", "Ù…Ù’"]],
    "Ø£ÙÙŠÙ‘": [["Ø£ÙÙŠÙ‘"], ["Ø£Ù", "ÙŠÙ‘"]],
    "Ù„ÙÙ…ÙØ§Ø°ÙØ§": [["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"], ["Ù„Ù", "Ù…Ù", "Ø§", "Ø°ÙØ§"]],
    "Ù…ÙØ§Ø°ÙØ§": [["Ù…ÙØ§", "Ø°ÙØ§"], ["Ù…Ù", "Ø§", "Ø°ÙØ§"]],
    "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù": [["Ø£ÙÙŠÙ’", "ÙŠÙØ§", "Ù†Ù"], ["Ø£Ù", "ÙŠÙÙ‘", "Ø§", "Ù†Ù"]],
    "Ø£ÙÙ†ÙÙ‘Ù‰": [["Ø£ÙÙ†Ù’", "Ù†ÙÙ‰"], ["Ø£Ù", "Ù†ÙÙ‘", "Ù‰"]],
    "Ù„ÙÙ…Ù": [["Ù„ÙÙ…Ù"], ["Ù„Ù", "Ù…Ù"]],
    "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’": [["ÙƒÙØ£ÙÙŠÙ’", "ÙŠÙÙ†Ù’"], ["ÙƒÙ", "Ø£Ù", "ÙŠÙÙ‘", "Ù†Ù’"]],
    "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§": [["Ø£ÙÙŠÙ’", "ÙŠÙ", "Ù‡ÙØ§"], ["Ø£Ù", "ÙŠÙÙ‘", "Ù‡ÙØ§"]],
    "Ù…ÙÙ‡Ù’Ù…ÙØ§": [["Ù…ÙÙ‡Ù’", "Ù…ÙØ§"], ["Ù…Ù", "Ù‡Ù’", "Ù…ÙØ§"]],
    "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§": [["Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"]],
    "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§": [["ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ", "Ù…ÙØ§"]],
    "Ù…ÙÙ†Ù’ Ø°ÙØ§": [["Ù…ÙÙ†Ù’", "Ø°ÙØ§"], ["Ù…Ù", "Ù†Ù’", "Ø°ÙØ§"]],
    }

    X = []
    y = []

    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
    for _ in range(num_samples):
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ø³Ù… Ø§Ø³ØªÙÙ‡Ø§Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    pronoun = random.choice(list(base_data.keys()))
    syllables = random.choice(base_data[pronoun])

        # Ø¥Ø¶Ø§ÙØ© ØªØ´ÙˆÙŠØ´ Ø·ÙÙŠÙ (ØªÙ†ÙˆÙŠØ¹)
        if random.random() < 0.15:  # 15% ØªØ´ÙˆÙŠØ´
    syllables = syllables.copy()

            # ØªÙ†ÙˆÙŠØ¹Ø§Øª ØµÙˆØªÙŠØ© Ø·ÙÙŠÙØ©
            if random.random() < 0.5 and len(len(syllables) -> 1) > 1:
                # ØªØ¨Ø¯ÙŠÙ„ Ø­Ø±ÙƒØ© Ø£Ø­ÙŠØ§Ù†Ø§Ù‹
    variations = {'Ù': ['Ù', 'Ù'], 'Ù': ['Ù', 'Ù'], 'Ù': ['Ù', 'Ù']}

                for i, syll in enumerate(syllables):
                    for orig, alts in variations.items():
                        if orig in syll and random.random() < 0.3:
    syllables[i] = syll.replace(orig, random.choice(alts))

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ
    features = processor.extract_features(syllables)
    label = PRONOUN_TO_ID[pronoun]

    X.append(features)
    y.append(label)

    return X, y


def train_model()
    model: nn.Module,
    X: List[np.ndarray],
    y: List[int],
    epochs: int = 40,
    batch_size: int = 32,
    lr: float = 0.001) -> Dict[str, List[float]]:
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ tensors
    X_tensor = torch.stack([torch.FloatTensor(x) for x in X])
    y_tensor = torch.LongTensor(y)

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_size = int(0.8 * len(X_tensor))
    train_X, test_X = X_tensor[:train_size], X_tensor[train_size:]
    train_y, test_y = y_tensor[:train_size], y_tensor[train_size:]

    # DataLoader
    train_dataset = torch.utils.data.TensorDataset(train_X, train_y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Optimizer ÙˆCriterion
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}

    model.train()
    for epoch in range(epochs):
    total_loss = 0
    correct_train = 0
    total_train = 0

        for batch_X, batch_y in train_loader:
    batch_X, batch_y = batch_X.to(device), batch_y.to(device)

    optimizer.zero_grad()
    outputs = model(batch_X)
    loss = criterion(outputs, batch_y)
    loss.backward()

            # Gradient clipping
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()

    total_loss += loss.item()
    _, predicted = torch.max(outputs.data, 1)
    total_train += batch_y.size(0)
    correct_train += (predicted == batch_y).sum().item()

    scheduler.step()

        # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    model.eval()
        with torch.no_grad():
    test_X_device = test_X.to(device)
    test_outputs = model(test_X_device)
    _, test_predicted = torch.max(test_outputs.data, 1)
    test_accuracy = (test_predicted == test_y.to(device)).float().mean().item()

    model.train()

    train_accuracy = correct_train / total_train
    avg_loss = total_loss / len(train_loader)

    history['train_loss'].append(avg_loss)
    history['train_acc'].append(train_accuracy)
    history['test_acc'].append(test_accuracy)

        if (epoch + 1) % 10 == 0:
    print()
    f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.4f | Test} Acc: {test_accuracy:.4f}}"
    )

    return history


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFERENCE SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class InterrogativePronounInference:
    """Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):

    self.processor = InterrogativePhoneticProcessor()
    self.models = {}
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    self.model_configs = {
    'lstm': {
    'input_size': self.processor.vocab_size + 12,
    'hidden_size': 64,
    'num_layers': 2,
    'num_classes': len(INTERROGATIVE_PRONOUNS),
    'dropout': 0.3,
    },
    'gru': {
    'input_size': self.processor.vocab_size + 12,
    'hidden_size': 64,
    'num_layers': 2,
    'num_classes': len(INTERROGATIVE_PRONOUNS),
    'dropout': 0.3,
    },
    'transformer': {
    'input_size': self.processor.vocab_size + 12,
    'd_model': 128,
    'nhead': 8,
    'num_layers': 4,
    'num_classes': len(INTERROGATIVE_PRONOUNS),
    'dropout': 0.1,
    },
    }

    self._initialize_models()

    def _initialize_models(self):
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    self.models['lstm'] = InterrogativeLSTM(**self.model_configs['lstm'])
    self.models['gru'] = InterrogativeGRU(**self.model_configs['gru'])
    self.models['transformer'] = InterrogativeTransformer()
    **self.model_configs['transformer']
    )

        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²
        for model in self.models.values():
    model.to(self.device)

    def train_all_models(self, num_samples: int = 1800):
    """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

    print("ğŸ§  Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…...")
    X, y = create_synthetic_data(self.processor, num_samples)

    results = {}

        for model_name, model in self.models.items():
    print(f"\nğŸš€ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {model_name.upper()}...")
    history = train_model(model, X, y, epochs=35)
    results[model_name] = {
    'final_train_acc': history['train_acc'][ 1],
    'final_test_acc': history['test_acc'][ 1],
    'history': history,
    }
    print()
    f"âœ… {model_name.upper()} - Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {history['train_acc'][ 1]:.3f}, Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {history['test_acc'][-1]:.3f}"
    )

    return results

    def predict_syllables()
    self, syllables: List[str], model_type: str = 'transformer'
    ) -> str:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³Ù… Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        if model_type not in self.models:
    raise ValueError(f"Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {model_type}")

    model = self.models[model_type]
    model.eval()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ
    features = self.processor.extract_features(syllables)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ tensor
    input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        # Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
    outputs = model(input_tensor)
    probabilities = F.softmax(outputs, dim=1)
    predicted_class = torch.argmax(outputs, dim=1).item()
    probabilities[0][predicted_class].item()

    predicted_pronoun = INTERROGATIVE_PRONOUNS[predicted_class]

    return predicted_pronoun

    def predict_with_confidence()
    self, syllables: List[str], model_type: str = 'transformer'
    ) -> Dict[str, Any]:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ø¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ø¨Ø¯Ø§Ø¦Ù„"""

        if model_type not in self.models:
    raise ValueError(f"Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {model_type}")

    model = self.models[model_type]
    model.eval()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ
    features = self.processor.extract_features(syllables)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ tensor
    input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        # Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
    outputs = model(input_tensor)
    probabilities = F.softmax(outputs, dim=1)[0]

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)

    results = {
    'best_prediction': INTERROGATIVE_PRONOUNS[sorted_indices[0].item()],
    'confidence': sorted_probs[0].item(),
    'alternatives': [],
    }

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„
        for i in range(min(3, len(sorted_indices))):
    idx = sorted_indices[i].item()
    prob = sorted_probs[i].item()
    results['alternatives'].append()
    {'pronoun': INTERROGATIVE_PRONOUNS[idx], 'confidence': prob}
    )

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…"""

    print("ğŸ§  Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 55)

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
    inference = InterrogativePronounInference()

    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
    results = inference.train_all_models(num_samples=1600)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:")
    for model_name, result in results.items():
    print(f"   {model_name.upper()}: {result['final_test_acc']:.1%} Ø¯Ù‚Ø© Ø§Ø®ØªØ¨Ø§Ø±}")

    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤
    print("\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤:")
    test_cases = [
    ["Ù…ÙÙ†Ù’"],  # Ù…ÙÙ†
    ["Ù…ÙØ§"],  # Ù…Ø§
    ["Ù…Ù", "ØªÙÙ‰"],  # Ù…ØªÙ‰
    ["Ø£ÙÙŠÙ’", "Ù†Ù"],  # Ø£ÙŠÙ†
    ["ÙƒÙÙŠÙ’", "ÙÙ"],  # ÙƒÙŠÙ
    ["ÙƒÙÙ…Ù’"],  # ÙƒÙ…
    ["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"],  # Ù„Ù…Ø§Ø°Ø§
    ["Ø£ÙÙŠÙ‘"],  # Ø£ÙŠ
    ]

    for syllables in test_cases:
    print(f"\n   Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {syllables}")

        for model_type in ['lstm', 'gru', 'transformer']:
    prediction = inference.predict_syllables(syllables, model_type)
    print(f"     {model_type.upper()}: {prediction}")

    # Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙØµÙ„
    print("\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙØµÙ„ (Transformer):")
    detailed_result = inference.predict_with_confidence(["Ù…Ù", "ØªÙÙ‰"], 'transformer')
    print(f"   Ø£ÙØ¶Ù„ ØªÙ†Ø¨Ø¤: {detailed_result['best_prediction']}")
    print(f"   Ø§Ù„Ø«Ù‚Ø©: {detailed_result['confidence']:.3f}")
    print("   Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„:")
    for alt in detailed_result['alternatives']:
    print(f"     - {alt['pronoun']: {alt['confidence']:.3f}}")

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±!")


if __name__ == "__main__":
    main()

