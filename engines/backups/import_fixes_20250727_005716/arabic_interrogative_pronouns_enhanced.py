#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Arabic Interrogative Pronouns Deep Learning System
=========================================================
Ù†Ø¸Ø§Ù… Ù…Ø­Ø³Ù† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Optimized version with improved Transformer architecture and
enhanced training strategies for Arabic interrogative pronouns.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 2.0.0 - ENHANCED MODELS
Date: 2025-07-24
Encoding: UTF 8
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
import random
import logging
from typing import Di        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    self.model_configs = {
    'enhanced_transformer': {
    'vocab_size': self.processor.vocab_size,
    'feature_dim': self.processor.feature_dim,
    'd_model': 256,
    'nhead': 8,
    'num_layers': 6,
    'num_classes': len(INTERROGATIVE_PRONOUNS),
    'dropout': 0.1,
    'max_seq_len': 15
    }
    }le, Any, Optional
from dataclasses import dataclass
import json
import math
from collections import Counter

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø°Ø±Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


INTERROGATIVE_PRONOUNS = {
    0: "Ù…ÙÙ†",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ
    1: "Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡
    2: "Ù…ÙØªÙÙ‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù†
    3: "Ø£ÙÙŠÙ’Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù†
    4: "ÙƒÙÙŠÙ’ÙÙ",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙŠÙÙŠØ©
    5: "ÙƒÙÙ…Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙ…ÙŠØ©
    6: "Ø£ÙÙŠÙ‘",  # Ù„Ù„Ø§Ø®ØªÙŠØ§Ø±
    7: "Ù„ÙÙ…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨
    8: "Ù…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ (Ù…Ø¨Ø§Ø´Ø±)
    9: "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
    10: "Ø£ÙÙ†ÙÙ‘Ù‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø·Ø±ÙŠÙ‚Ø©
    11: "Ù„ÙÙ…Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨ (Ù…Ø®ØªØµØ±)
    12: "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ«ÙŠØ±
    13: "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§",  # Ø§Ù„Ù†Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠ
    14: "Ù…ÙÙ‡Ù’Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡
    15: "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ù…ÙƒØ§Ù†
    16: "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø©
    17: "Ù…ÙÙ†Ù’ Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø¨ØªØ£ÙƒÙŠØ¯
}

PRONOUN_TO_ID = {v: k for k, v in INTERROGATIVE_PRONOUNS.items()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED PHONETIC PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedPhoneticProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØªÙŠ Ù…Ø­Ø³Ù† Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):

    self.arabic_phonemes = {
            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    'Ø¨': 0,
    'Øª': 1,
    'Ø«': 2,
    'Ø¬': 3,
    'Ø­': 4,
    'Ø®': 5,
    'Ø¯': 6,
    'Ø°': 7,
    'Ø±': 8,
    'Ø²': 9,
    'Ø³': 10,
    'Ø´': 11,
    'Øµ': 12,
    'Ø¶': 13,
    'Ø·': 14,
    'Ø¸': 15,
    'Ø¹': 16,
    'Øº': 17,
    'Ù': 18,
    'Ù‚': 19,
    'Ùƒ': 20,
    'Ù„': 21,
    'Ù…': 22,
    'Ù†': 23,
    'Ù‡': 24,
    'Ùˆ': 25,
    'ÙŠ': 26,
    'Ø¡': 27,
    'Ø¢': 28,
    'Ø£': 29,
    'Ø¥': 30,
    'Ø¦': 31,
    'Ø¤': 32,
    'Ø©': 33,
            # Ø§Ù„Ø­Ø±ÙƒØ§Øª
    'Ù': 34,
    'Ù': 35,
    'Ù': 36,
    'Ù’': 37,
    'Ù‹': 38,
    'ÙŒ': 39,
    'Ù': 40,
            # Ø£ØµÙˆØ§Øª Ù…Ø±ÙƒØ¨Ø© Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    'Ù…Ù': 41,
    'Ø£Ù': 42,
    'ÙƒÙ': 43,
    'Ù„Ù': 44,
    'Ù†Ù’': 45,
    'ØªÙ': 46,
    'ÙŠÙ’': 47,
    'Ø°Ù': 48,
    'Ù‰': 49,
    'Ø§Ø°Ù': 50,
    'Ù†Ù': 51,
    'ÙÙ': 52,
    'ÙŠÙÙ‘': 53,
    'Ø§Ù†': 54,
    'Ù‡Ù’': 55,
            # Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
    '<PAD>': 56,
    '<UNK>': 57,
    '<START>': 58,
    '<END>': 59,
    }

    self.vocab_size = len(self.arabic_phonemes)
    self.feature_dim = 16  # Ø²ÙŠØ§Ø¯Ø© Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ
    self.phoneme_features = self._initialize_enhanced_features()

    def _initialize_enhanced_features(self) -> Dict[str, np.ndarray]:
    """ØªÙ‡ÙŠØ¦Ø© Ø®ØµØ§Ø¦Øµ ØµÙˆØªÙŠØ© Ù…Ø­Ø³Ù†Ø©"""

    features = {}

        for phoneme, idx in self.arabic_phonemes.items():
            # Ø®ØµØ§Ø¦Øµ Ù…Ø­Ø³Ù†Ø© (16 Ø¨Ø¹Ø¯)
    feature_vector = np.zeros(self.feature_dim)

            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    consonants = [
    'Ø¨',
    'Øª',
    'Ø«',
    'Ø¬',
    'Ø­',
    'Ø®',
    'Ø¯',
    'Ø°',
    'Ø±',
    'Ø²',
    'Ø³',
    'Ø´',
    'Øµ',
    'Ø¶',
    'Ø·',
    'Ø¸',
    'Ø¹',
    'Øº',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    ]

            if phoneme in consonants:
    feature_vector[0] = 1.0  # ØµØ§Ù…Øª

    vowels = ['Ùˆ', 'ÙŠ', 'Ø§', 'Ù', 'Ù', 'Ù', 'Ø¢', 'Ù‰']
            if phoneme in vowels:
    feature_vector[1] = 1.0  # ØµØ§Ø¦Øª

            # Ø£ØµÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    primary_interrogatives = ['Ù…', 'Ø£', 'Ùƒ', 'Ù„']
            if any(c in phoneme for c in primary_interrogatives):
    feature_vector[2] = 1.0

            # Ø£ØµÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
            if phoneme in ['Ù…Ù', 'Ø£Ù', 'ÙƒÙ', 'Ù„Ù']:
    feature_vector[3] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ù†ÙÙŠØ©
            if phoneme in ['Ù„', 'Ø±', 'Ù†', 'Ù…']:
    feature_vector[4] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø§Ø­ØªÙƒØ§ÙƒÙŠØ©
    fricatives = ['Ù', 'Ø«', 'Ø°', 'Ø³', 'Ø²', 'Ø´', 'Øµ', 'Ø®', 'Øº', 'Ø­', 'Ø¹', 'Ù‡']
            if phoneme in fricatives:
    feature_vector[5] = 1.0

            # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±ÙŠØ©
    stops = ['Ø¨', 'Øª', 'Ø¯', 'Ø·', 'Ùƒ', 'Ù‚', 'Ø¡']
            if phoneme in stops:
    feature_vector[6] = 1.0

            # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    long_vowels = ['Ø§', 'Ùˆ', 'ÙŠ', 'Ø¢', 'Ù‰']
            if phoneme in long_vowels:
    feature_vector[7] = 1.0

            # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
    short_vowels = ['Ù', 'Ù', 'Ù']
            if phoneme in short_vowels:
    feature_vector[8] = 1.0

            # Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù†Ø·Ù‚ (Ù…Ø­Ø³Ù†)
    labial = ['Ø¨', 'Ù', 'Ù…', 'Ùˆ']
    dental = ['Øª', 'Ø¯', 'Ø«', 'Ø°', 'Ù„', 'Ù†', 'Ø±', 'Ø²', 'Ø³']
    palatal = ['Ø´', 'Ø¬', 'ÙŠ']
    velar = ['Ùƒ', 'Ù‚', 'Øº', 'Ø®']
    pharyngeal = ['Ø­', 'Ø¹']

            if phoneme in labial:
    feature_vector[9] = 1.0  # Ø´ÙÙˆÙŠ
            elif phoneme in dental:
    feature_vector[10] = 1.0  # Ù„Ø«ÙˆÙŠ
            elif phoneme in palatal:
    feature_vector[11] = 1.0  # Ø­Ù„Ù‚ÙŠ
            elif phoneme in velar:
    feature_vector[12] = 1.0  # Ø·Ø¨Ù‚ÙŠ
            elif phoneme in pharyngeal:
    feature_vector[13] = 1.0  # Ø¨Ù„Ø¹ÙˆÙ…ÙŠ

            # Ø§Ù„ØªØ´Ø¯ÙŠØ¯ ÙˆØ§Ù„Ø³ÙƒÙˆÙ†
            if phoneme in ['Ù‹', 'ÙŒ', 'Ù', 'Ù’', 'Ù‘']:
    feature_vector[14] = 1.0

            # Ø£ØµÙˆØ§Øª Ù…Ø±ÙƒØ¨Ø© Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…
    special_interrogatives = ['Ù…Ù', 'Ø£Ù', 'ÙƒÙ', 'Ù„Ù', 'Ø§Ø°Ù', 'ÙŠÙÙ‘', 'Ø§Ù†']
            if phoneme in special_interrogatives:
    feature_vector[15] = 1.0

    features[phoneme] = feature_vector

    return features

    def encode_syllables_enhanced(self, syllables: List[str], max_length: int = 12) -> np.ndarray:
    """ØªØ±Ù…ÙŠØ² Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ±Ù…ÙŠØ²
    encoded = np.zeros((max_length, self.vocab_size + self.feature_dim))

        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    start_idx = self.arabic_phonemes['<START>']
    encoded[0, start_idx] = 1.0

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    pos = 1
        for syllable in syllables:
            if pos >= max_length - 1:  # ØªØ±Ùƒ Ù…ÙƒØ§Ù† Ù„Ø±Ù…Ø² Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    break

            # ØªÙÙƒÙŠÙƒ Ø§Ù„Ù…Ù‚Ø·Ø¹
    phonemes = self._decompose_syllable(syllable)

            for phoneme in phonemes:
                if pos >= max_length - 1:
    break

                if phoneme in self.arabic_phonemes:
                    # One hot encoding
    idx = self.arabic_phonemes[phoneme]
    encoded[pos, idx] = 1.0

                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
                    if phoneme in self.phoneme_features:
    features = self.phoneme_features[phoneme]
    encoded[pos, self.vocab_size :] = features

    pos += 1

        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        if pos < max_length:
    end_idx = self.arabic_phonemes['<END>']
    encoded[pos, end_idx] = 1.0

    return encoded

    def _decompose_syllable(self, syllable: str) -> List[str]:
    """ØªÙÙƒÙŠÙƒ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø¥Ù„Ù‰ Ø£ØµÙˆØ§Øª"""

        if syllable in self.arabic_phonemes:
    return [syllable]

        # ØªÙÙƒÙŠÙƒ ØªØ¯Ø±ÙŠØ¬ÙŠ
    phonemes = []
    i = 0
        while i < len(syllable):
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ø£Ø·ÙˆÙ„ ØµÙˆØª Ù…Ø±ÙƒØ¨
    found = False
            for length in range(min(3, len(syllable) - i), 0, -1):
    candidate = syllable[i : i + length]
                if candidate in self.arabic_phonemes:
    phonemes.append(candidate)
    i += length
    found = True
    break

            if not found:
                # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ
    phonemes.append('<UNK>')
    i += 1

    return phonemes


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedTransformer(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Transformer Ù…Ø­Ø³Ù† Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
    self,
    vocab_size: int,
    feature_dim: int,
    d_model: int = 256,
    nhead: int = 8,
    num_layers: int = 6,
    num_classes: int = 18,
    dropout: float = 0.1, max_seq_len: int = 12):
    super(EnhancedTransformer, self).__init__()

    self.d_model = d_model
    self.vocab_size = vocab_size
    self.feature_dim = feature_dim

        # ØªØ¶Ù…ÙŠÙ† Ù…Ø­Ø³Ù†
    self.phoneme_embedding = nn.Embedding(vocab_size, d_model // 2)
    self.feature_projection = nn.Linear(feature_dim, d_model // 2)

        # Ø·Ø¨Ù‚Ø© Ø¯Ù…Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    self.embedding_fusion = nn.Linear(d_model, d_model)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø³Ù†
    self.pos_encoding = EnhancedPositionalEncoding(d_model, dropout, max_seq_len)

        # Ø·Ø¨Ù‚Ø© ØªØ·Ø¨ÙŠØ¹ Ø£ÙˆÙ„ÙŠØ©
    self.input_norm = nn.LayerNorm(d_model)

        # Ø·Ø¨Ù‚Ø§Øª Transformer Ù…Ø­Ø³Ù†Ø©
    encoder_layer = nn.TransformerEncoderLayer()
    d_model=d_model,
    nhead=nhead,
    dim_feedforward=d_model * 4,
    dropout=dropout,
    activation='gelu',
    batch_first=True,
    norm_first=True,  # Pre-norm Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹
    )

    self.transformer_encoder = nn.TransformerEncoder()
    encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(d_model)
    )

        # Ø¢Ù„ÙŠØ© Ø§Ù†ØªØ¨Ø§Ù‡ Ø¹Ø§Ù„Ù…ÙŠØ©
    self.global_attention = nn.MultiheadAttention()
    embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True
    )

        # Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ù„Ù…
    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¯Ø±Ø¬Ø©
    self.classifier = nn.Sequential()
    nn.LayerNorm(d_model),
    nn.Linear(d_model, d_model // 2),
    nn.GELU(),
    nn.Dropout(dropout),
    nn.Linear(d_model // 2, d_model // 4),
    nn.GELU(),
    nn.Dropout(dropout),
    nn.Linear(d_model // 4, num_classes))

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    self._init_weights()

    def _init_weights(self):
    """ØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

        for module in self.modules():
            if isinstance(module, nn.Linear):
    nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
    nn.init.normal_(module.weight, std=0.02)

    def forward(self, x):
    def forward(self, x):
    batch_size, seq_len, feature_size = x.shape

        # ÙØµÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª ÙˆØ§Ù„Ø®ØµØ§Ø¦Øµ
    phoneme_part = x[: : : self.vocab_size]  # One-hot phonemes
    feature_part = x[: : self.vocab_size :]  # Phonetic features

        # ØªØ­ÙˆÙŠÙ„ one-hot Ø¥Ù„Ù‰ indices
    phoneme_indices = torch.argmax(phoneme_part, dim=-1)

        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø£ØµÙˆØ§Øª
    phoneme_emb = self.phoneme_embedding(phoneme_indices)  # [batch, seq, d_model//2]

        # Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø®ØµØ§Ø¦Øµ
    feature_emb = self.feature_projection(feature_part)  # [batch, seq, d_model//2]

        # Ø¯Ù…Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    combined_emb = torch.cat([phoneme_emb, feature_emb], dim=-1)  # [batch, seq, d_model]
    combined_emb = self.embedding_fusion(combined_emb)

        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ
    cls_tokens = self.cls_token.expand(batch_size, -1,  1)
    x = torch.cat([cls_tokens, combined_emb], dim=1)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹
    x = self.pos_encoding(x)

        # ØªØ·Ø¨ÙŠØ¹ Ø£ÙˆÙ„ÙŠ
    x = self.input_norm(x)

        # Transformer
    transformer_out = self.transformer_encoder(x)

        # Ø§Ù†ØªØ¨Ø§Ù‡ Ø¹Ø§Ù„Ù…ÙŠ
    attended_out, _ = self.global_attention(transformer_out, transformer_out, transformer_out)

        # Ø£Ø®Ø° Ø±Ù…Ø² Ø§Ù„ØªØµÙ†ÙŠÙ
    cls_output = attended_out[: 0]

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(cls_output)

    return output


class EnhancedPositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ Ù…Ø­Ø³Ù†"""

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 15):

    super(EnhancedPositionalEncoding, self).__init__()

    self.dropout = nn.Dropout(p=dropout)

        # ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ Ù…Ø·Ù„Ù‚
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

    pe[: 0::2] = torch.sin(position * div_term)
    pe[: 1::2] = torch.cos(position * div_term)

    pe = pe.unsqueeze(0)
    self.register_buffer('pe', pe)

        # ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ù„Ù…
    self.learned_pe = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)

    def forward(self, x):
    def forward(self, x):
    seq_len = x.size(1)

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        if seq_len <= self.pe.size(1) and seq_len <= self.learned_pe.size(1):
            # Ø¯Ù…Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø·Ù„Ù‚ ÙˆØ§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ù„Ù…
    pos_encoding = (self.pe[: :seq_len] + self.learned_pe[: :seq_len]) * 0.5
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø·Ù„Ù‚ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø·ÙˆÙ„ Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
    pos_encoding = self.pe[: :seq_len] if seq_len <= self.pe.size(1) else self.pe

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¥Ø¶Ø§ÙØ©
        if pos_encoding.size(1) == x.size(1):
    x = x + pos_encoding

    return self.dropout(x)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED TRAINING SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def create_enhanced_synthetic_data()
    processor: EnhancedPhoneticProcessor, num_samples: int = 2000
) -> Tuple[List[np.ndarray], List[int]]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø³Ù†Ø©"""

    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    enhanced_data = {
    "Ù…ÙÙ†": [["Ù…ÙÙ†Ù’"], ["Ù…Ù", "Ù†Ù’"], ["Ù…ÙÙ†Ù’"], ["Ù…ÙÙ†Ù’"]],
    "Ù…ÙØ§": [["Ù…ÙØ§"], ["Ù…Ù", "Ø§"], ["Ù…ÙØ§"], ["Ù…ÙØ§"]],
    "Ù…ÙØªÙÙ‰": [["Ù…Ù", "ØªÙÙ‰"], ["Ù…Ù", "ØªÙ", "Ù‰"], ["Ù…Ù", "ØªÙÙ‰"], ["Ù…Ù", "ØªÙÙ‰"]],
    "Ø£ÙÙŠÙ’Ù†Ù": [["Ø£ÙÙŠÙ’", "Ù†Ù"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù"], ["Ø£ÙÙŠÙ’", "Ù†Ù"], ["Ø£ÙÙŠÙ’", "Ù†Ù"]],
    "ÙƒÙÙŠÙ’ÙÙ": [["ÙƒÙÙŠÙ’", "ÙÙ"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ"], ["ÙƒÙÙŠÙ’", "ÙÙ"], ["ÙƒÙÙŠÙ’", "ÙÙ"]],
    "ÙƒÙÙ…Ù’": [["ÙƒÙÙ…Ù’"], ["ÙƒÙ", "Ù…Ù’"], ["ÙƒÙÙ…Ù’"], ["ÙƒÙÙ…Ù’"]],
    "Ø£ÙÙŠÙ‘": [["Ø£ÙÙŠÙ‘"], ["Ø£Ù", "ÙŠÙ‘"], ["Ø£ÙÙŠÙ‘"], ["Ø£ÙÙŠÙ‘"]],
    "Ù„ÙÙ…ÙØ§Ø°ÙØ§": [["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"], ["Ù„Ù", "Ù…Ù", "Ø§", "Ø°ÙØ§"], ["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"]],
    "Ù…ÙØ§Ø°ÙØ§": [["Ù…ÙØ§", "Ø°ÙØ§"], ["Ù…Ù", "Ø§", "Ø°ÙØ§"], ["Ù…ÙØ§", "Ø°ÙØ§"]],
    "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù": [["Ø£ÙÙŠÙ’", "ÙŠÙØ§", "Ù†Ù"], ["Ø£Ù", "ÙŠÙÙ‘", "Ø§", "Ù†Ù"], ["Ø£ÙÙŠÙÙ‘", "Ø§", "Ù†Ù"]],
    "Ø£ÙÙ†ÙÙ‘Ù‰": [["Ø£ÙÙ†Ù’", "Ù†ÙÙ‰"], ["Ø£Ù", "Ù†ÙÙ‘", "Ù‰"], ["Ø£ÙÙ†ÙÙ‘", "Ù‰"]],
    "Ù„ÙÙ…Ù": [["Ù„ÙÙ…Ù"], ["Ù„Ù", "Ù…Ù"], ["Ù„ÙÙ…Ù"], ["Ù„Ù", "Ù…Ù"]],
    "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’": [["ÙƒÙØ£ÙÙŠÙ’", "ÙŠÙÙ†Ù’"], ["ÙƒÙ", "Ø£Ù", "ÙŠÙÙ‘", "Ù†Ù’"], ["ÙƒÙØ£ÙÙŠÙÙ‘", "Ù†Ù’"]],
    "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§": [["Ø£ÙÙŠÙ’", "ÙŠÙ", "Ù‡ÙØ§"], ["Ø£Ù", "ÙŠÙÙ‘", "Ù‡ÙØ§"], ["Ø£ÙÙŠÙÙ‘", "Ù‡ÙØ§"]],
    "Ù…ÙÙ‡Ù’Ù…ÙØ§": [["Ù…ÙÙ‡Ù’", "Ù…ÙØ§"], ["Ù…Ù", "Ù‡Ù’", "Ù…ÙØ§"], ["Ù…ÙÙ‡Ù’", "Ù…ÙØ§"]],
    "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§": [["Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"], ["Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"]],
    "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§": [["ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ", "Ù…ÙØ§"], ["ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"]],
    "Ù…ÙÙ†Ù’ Ø°ÙØ§": [["Ù…ÙÙ†Ù’", "Ø°ÙØ§"], ["Ù…Ù", "Ù†Ù’", "Ø°ÙØ§"], ["Ù…ÙÙ†Ù’", "Ø°ÙØ§"]],
    }

    X = []
    y = []

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù„Ù„ØªÙˆØ§Ø²Ù†
    class_weights = {}
    for pronoun in enhanced_data.keys():
        class_weights[PRONOUN_TO_ID[pronoun]] = 1.0 / len(enhanced_data)

    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹ ØªÙˆØ§Ø²Ù† Ø§Ù„ÙØ¦Ø§Øª
    samples_per_class = num_samples // len(enhanced_data)

    for pronoun, syllable_variants in enhanced_data.items():
    pronoun_id = PRONOUN_TO_ID[pronoun]

        for _ in range(samples_per_class):
            # Ø§Ø®ØªÙŠØ§Ø± ØªÙ†ÙˆÙŠØ¹ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    syllables = random.choice(syllable_variants).copy()

            # Ø¥Ø¶Ø§ÙØ© ØªØ´ÙˆÙŠØ´ Ù…Ø¹ØªØ¯Ù„ (20% Ù…Ù† Ø§Ù„ÙˆÙ‚Øª)
            if random.random() < 0.2:
    syllables = apply_phonetic_noise(syllables)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    features = processor.encode_syllables_enhanced(syllables)

    X.append(features)
    y.append(pronoun_id)

    # Ø®Ù„Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    combined = list(zip(X, y))
    random.shuffle(combined)
    X, y = zip(*combined)

    return list(X), list(y)


def apply_phonetic_noise(syllables: List[str]) -> List[str]:
    """ØªØ·Ø¨ÙŠÙ‚ ØªØ´ÙˆÙŠØ´ ØµÙˆØªÙŠ Ø·ÙÙŠÙ"""

    noisy_syllables = syllables.copy()

    # ØªÙ†ÙˆÙŠØ¹Ø§Øª Ø§Ù„Ø­Ø±ÙƒØ§Øª
    vowel_variations = {'Ù': ['Ù', 'Ù'], 'Ù': ['Ù', 'Ù'], 'Ù': ['Ù', 'Ù']}

    for i, syllable in enumerate(noisy_syllables):
        if random.random() < 0.3:  # 30% Ø§Ø­ØªÙ…Ø§Ù„ ØªØºÙŠÙŠØ±
            for orig_vowel, alt_vowels in vowel_variations.items():
                if orig_vowel in syllable:
    new_vowel = random.choice(alt_vowels)
    noisy_syllables[i] = syllable.replace(orig_vowel, new_vowel)
    break

    return noisy_syllables


def train_enhanced_model()
    model: nn.Module, X: List[np.ndarray], y: List[int], epochs: int = 50, batch_size: int = 32, lr: float = 0.0005
) -> Dict[str, List[float]]:
    """ØªØ¯Ø±ÙŠØ¨ Ù…Ø­Ø³Ù† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_tensor = torch.stack([torch.FloatTensor(x) for x in X])
    y_tensor = torch.LongTensor(y)

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø·Ø¨Ù‚ÙŠØ©
    from sklearn.model_selection import train_test_split

    train_X, test_X, train_y, test_y = train_test_split()
    X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=42
    )

    # Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†
    class_counts = Counter(train_y.numpy())
    total_samples = len(train_y)
    class_weights = {}
    for class_id in range(len(INTERROGATIVE_PRONOUNS)):
        if class_id in class_counts:
            class_weights[class_id] = total_samples / (len(class_counts) * class_counts[class_id])
        else:
            class_weights[class_id] = 1.0

    # ØªØ­ÙˆÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª Ø¥Ù„Ù‰ tensor
    weight_tensor = torch.FloatTensor([class_weights[i] for i in range(len(INTERROGATIVE_PRONOUNS))])

    # DataLoader Ù…Ø¹ Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…ÙˆØ²ÙˆÙ†Ø©
    train_dataset = torch.utils.data.TensorDataset(train_X, train_y)

    # Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
    sample_weights = [class_weights[label.item()] for label in train_y]
    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)

    # Optimizer Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.999))

    # Loss function Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„ÙØ¦Ø§Øª
    criterion = nn.CrossEntropyLoss(weight=weight_tensor.to(device))

    # Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    scheduler = optim.lr_scheduler.OneCycleLR()
    optimizer, max_lr=lr * 5, epochs=epochs, steps_per_epoch=len(train_loader), pct_start=0.1, anneal_strategy='cos'
    )

    history = {'train_loss': [], 'train_acc': [], 'test_acc': [], 'lr': []}

    best_test_acc = 0
    patience = 10
    patience_counter = 0

    model.train()
    for epoch in range(epochs):
    total_loss = 0
    correct_train = 0
    total_train = 0

        for batch_X, batch_y in train_loader:
    batch_X, batch_y = batch_X.to(device), batch_y.to(device)

    optimizer.zero_grad()

            # Forward pass
    outputs = model(batch_X)
    loss = criterion(outputs, batch_y)

            # Backward pass
    loss.backward()

            # Gradient clipping
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    scheduler.step()

    total_loss += loss.item()
    _, predicted = torch.max(outputs.data, 1)
    total_train += batch_y.size(0)
    correct_train += (predicted == batch_y).sum().item()

        # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    model.eval()
        with torch.no_grad():
    test_X_device = test_X.to(device)
    test_outputs = model(test_X_device)
    _, test_predicted = torch.max(test_outputs.data, 1)
    test_accuracy = (test_predicted == test_y.to(device)).float().mean().item()

    model.train()

    train_accuracy = correct_train / total_train
    avg_loss = total_loss / len(train_loader)
    current_lr = scheduler.get_last_lr()[0]

    history['train_loss'].append(avg_loss)
    history['train_acc'].append(train_accuracy)
    history['test_acc'].append(test_accuracy)
    history['lr'].append(current_lr)

        # Early stopping
        if test_accuracy > best_test_acc:
    best_test_acc = test_accuracy
    patience_counter = 0
        else:
    patience_counter += 1

        if (epoch + 1) % 10 == 0:
    print()
    f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | "
    f"Train Acc: {train_accuracy:.4f} | Test Acc: {test_accuracy:.4f} | }"
    f"LR: {current_lr:.6f}"
    )

        if patience_counter >= patience and epoch > 20:
    print(f"Early stopping at epoch {epoch+1}")
    break

    return history


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED INFERENCE SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedInterrogativeInference:
    """Ù†Ø¸Ø§Ù… Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ø­Ø³Ù† Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):

    self.processor = EnhancedPhoneticProcessor()
    self.models = {}
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    self.model_configs = {
    'enhanced_transformer': {
    'vocab_size': self.processor.vocab_size,
    'feature_dim': self.processor.feature_dim,
    'd_model': 256,
    'nhead': 8,
    'num_layers': 6,
    'num_classes': len(INTERROGATIVE_PRONOUNS),
    'dropout': 0.1,
    'max_seq_len': 12,
    }
    }

    self._initialize_models()

    def _initialize_models(self):
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""

    self.models['enhanced_transformer'] = EnhancedTransformer(**self.model_configs['enhanced_transformer'])

        for model in self.models.values():
    model.to(self.device)

    def train_enhanced_models(self, num_samples: int = 3000):
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""

    print("ğŸ§  Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†Ø©...")
    X, y = create_enhanced_synthetic_data(self.processor, num_samples)

    results = {}

        for model_name, model in self.models.items():
    print(f"\nğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù† {model_name.upper()}...")
    history = train_enhanced_model(model, X, y, epochs=40, lr=0.0005)

    results[model_name] = {
    'final_train_acc': history['train_acc'][ 1],
    'final_test_acc': history['test_acc'][ 1],
    'best_test_acc': max(history['test_acc']),
    'history': history,
    }

    print()
    f"âœ… {model_name.upper()} - Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {history['train_acc'][-1]:.3f, }"
    f"Ø£ÙØ¶Ù„ Ø¯Ù‚Ø© Ø§Ø®ØªØ¨Ø§Ø±: {max(history['test_acc']):.3f}"
    )

    return results

    def predict_enhanced(self, syllables: List[str], model_type: str = 'enhanced_transformer') -> Dict[str, Any]:
    """ØªÙ†Ø¨Ø¤ Ù…Ø­Ø³Ù† Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„"""

        if model_type not in self.models:
    raise ValueError(f"Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {model_type}")

    model = self.models[model_type]
    model.eval()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    features = self.processor.encode_syllables_enhanced(syllables)
    input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        # Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
    outputs = model(input_tensor)
    probabilities = F.softmax(outputs, dim=1)[0]

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)

    results = {
    'input_syllables': syllables,
    'best_prediction': INTERROGATIVE_PRONOUNS[sorted_indices[0].item()],
    'confidence': sorted_probs[0].item(),
    'alternatives': [],
    'entropy': -torch.sum(probabilities * torch.log(probabilities + 1e 10)).item(),
    'max_prob': sorted_probs[0].item(),
    'prediction_strength': ()
    'high' if sorted_probs[0].item()  > 0.8 else 'medium' if sorted_probs[0].item()  > 0.5 else 'low'
    ),
    }

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„
        for i in range(min(5, len(sorted_indices))):
    idx = sorted_indices[i].item()
    prob = sorted_probs[i].item()
    results['alternatives'].append()
    {
    'pronoun': INTERROGATIVE_PRONOUNS[idx],
    'confidence': prob,
    'relative_strength': prob / sorted_probs[0].item() if sorted_probs[0].item()  > 0 else 0,
    }
    )

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†"""

    print("ğŸ§  Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ - Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 65)

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
    enhanced_inference = EnhancedInterrogativeInference()

    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©...")
    results = enhanced_inference.train_enhanced_models(num_samples=2500)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†:")
    for model_name, result in results.items():
    print(f"   {model_name.upper()}: {result['best_test_acc']:.1% Ø£ÙØ¶Ù„} Ø¯Ù‚Ø© Ø§Ø®ØªØ¨Ø§Ø±}")

    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø­Ø³Ù†
    print(f"\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø­Ø³Ù†:")
    test_cases = [
    ["Ù…ÙÙ†Ù’"],  # Ù…ÙÙ†
    ["Ù…ÙØ§"],  # Ù…Ø§
    ["Ù…Ù", "ØªÙÙ‰"],  # Ù…ØªÙ‰
    ["Ø£ÙÙŠÙ’", "Ù†Ù"],  # Ø£ÙŠÙ†
    ["ÙƒÙÙŠÙ’", "ÙÙ"],  # ÙƒÙŠÙ
    ["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"],  # Ù„Ù…Ø§Ø°Ø§
    ]

    for syllables in test_cases:
    result = enhanced_inference.predict_enhanced(syllables)
    print(f"\n   Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {syllables}")
    print(f"     Ø§Ù„ØªÙ†Ø¨Ø¤: {result['best_prediction']} (Ø«Ù‚Ø©: {result['confidence']:.3f)}")
    print(f"     Ù‚ÙˆØ© Ø§Ù„ØªÙ†Ø¨Ø¤: {result['prediction_strength']}")
    print(f"     Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ø£ÙˆÙ„Ù‰:")
        for alt in result['alternatives'][:3]:
    print(f"       - {alt['pronoun']: {alt['confidence']:.3f}}")

    print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†!")


if __name__ == "__main__":
    main()

