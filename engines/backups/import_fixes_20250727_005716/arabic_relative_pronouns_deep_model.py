#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Relative Pronouns Deep Learning Models
===========================================
Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Deep learning models for Arabic relative pronouns classification from syllable sequences
using RNN, LSTM, GRU, and Transformer architectures with MFCC audio features.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - DEEP LEARNING MODELS
Date: 2025-07-24
Encoding: UTF 8
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import logging
from typing import Dict, List, Any, Tuple
from sklearn.metrics import ()
    precision_recall_fscore_support,
    confusion_matrix)
import librosa
from pathlib import Path

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHONETIC PROCESSOR FOR ARABIC RELATIVE PRONOUNS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPhoneticProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__(self):

        # Ù…Ø®Ø²ÙˆÙ† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„
    self.arabic_phonemes = [
            # Ø£ØµÙˆØ§Øª ØµØ§Ù…ØªØ©
    'b',
    't',
    'th',
    'j',
    'h',
    'kh',
    'd',
    'dh',
    'r',
    'z',
    's',
    'sh',
    'á¹£',
    'á¸',
    'á¹­',
    'áº“',
    'Ê•',
    'gh',
    'f',
    'q',
    'k',
    'l',
    'm',
    'n',
    'h',
    'w',
    'y',
            # Ø£ØµÙˆØ§Øª ØµØ§Ø¦ØªØ© Ù‚ØµÙŠØ±Ø©
    'a',
    'i',
    'u',
            # Ø£ØµÙˆØ§Øª ØµØ§Ø¦ØªØ© Ø·ÙˆÙŠÙ„Ø©
    'aa',
    'ii',
    'uu',
            # Ø£ØµÙˆØ§Øª Ù…Ø±ÙƒØ¨Ø©
    'ay',
    'aw',
            # Ø£ØµÙˆØ§Øª Ø®Ø§ØµØ©
    'Ê”',  # Ù‡Ù…Ø²Ø©
            # Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
    '<PAD>',
    '<UNK>',
    '<START>',
    '<END>',
    ]

    self.phoneme_to_idx = {p: idx for idx, p in enumerate(self.arabic_phonemes)}
    self.idx_to_phoneme = {idx: p for idx, p in enumerate(self.arabic_phonemes)}
    self.vocab_size = len(self.arabic_phonemes)

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª
    self.arabic_to_phoneme_map = {
    'Ø§': 'aa',
    'Ø£': 'a',
    'Ø¥': 'i',
    'Ø¢': 'aa',
    'Ø¨': 'b',
    'Øª': 't',
    'Ø«': 'th',
    'Ø¬': 'j',
    'Ø­': 'h',
    'Ø®': 'kh',
    'Ø¯': 'd',
    'Ø°': 'dh',
    'Ø±': 'r',
    'Ø²': 'z',
    'Ø³': 's',
    'Ø´': 'sh',
    'Øµ': 'á¹£',
    'Ø¶': 'á¸',
    'Ø·': 'á¹­',
    'Ø¸': 'áº“',
    'Ø¹': 'Ê•',
    'Øº': 'gh',
    'Ù': 'f',
    'Ù‚': 'q',
    'Ùƒ': 'k',
    'Ù„': 'l',
    'Ù…': 'm',
    'Ù†': 'n',
    'Ù‡': 'h',
    'Ùˆ': 'w',
    'ÙŠ': 'y',
    'Ù‰': 'aa',
    'Ø©': 'h',
    'Ø¡': 'Ê”',
            # Ø­Ø±ÙƒØ§Øª
    'Ù': 'a',
    'Ù': 'i',
    'Ù': 'u',
    'Ù‹': 'an',
    'Ù': 'in',
    'ÙŒ': 'un',
    'Ù’': '',  # Ø³ÙƒÙˆÙ†
    'Ù‘': '',  # Ø´Ø¯Ø© (Ù…Ø¶Ø§Ø¹ÙØ©)
    }

    logger.info(f"ğŸ“š ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª: {self.vocab_size ÙÙˆÙ†ÙŠÙ…}")

    def text_to_phonemes(self, text: str) -> List[str]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª"""

    phonemes = []
    i = 0

        while i < len(text):
    char = text[i]

            if char in self.arabic_to_phoneme_map:
    phoneme = self.arabic_to_phoneme_map[char]
                if phoneme:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ÙØ§Ø±ØºØ©
    phonemes.append(phoneme)

    i += 1

    return phonemes

    def syllables_to_phonemes(self, syllables: List[str]) -> List[str]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª"""

    all_phonemes = []

        for syllable in syllables:
    syllable_phonemes = self.text_to_phonemes(syllable)
    all_phonemes.extend(syllable_phonemes)

    return all_phonemes

    def encode_phonemes(self, phonemes: List[str]) -> List[int]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…"""

    return [
    self.phoneme_to_idx.get(p, self.phoneme_to_idx['<UNK>']) for p in phonemes
    ]

    def decode_phonemes(self, indices: List[int]) -> List[str]:
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª"""

    return [self.idx_to_phoneme.get(idx, '<UNK>') for idx in indices]

    def pad_sequence(self, sequence: List[int], max_length: int) -> List[int]:
    """Ø¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ Ù„Ù„ØªØ³Ù„Ø³Ù„"""

    pad_token = self.phoneme_to_idx['<PAD>']

        if len(sequence) >= max_length:
    return sequence[:max_length]
        else:
    return sequence + [pad_token] * (max_length - len(sequence))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUDIO FEATURE PROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MFCCFeatureProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø®ØµØ§Ø¦Øµ MFCC Ù„Ù„ØµÙˆØª"""

    def __init__(self, n_mfcc: int = 40, sample_rate: int = 16000):

    self.n_mfcc = n_mfcc
    self.sample_rate = sample_rate
    self.n_fft = 2048
    self.hop_length = 512

    def extract_mfcc_features(self, audio_file: str) -> np.ndarray:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ MFCC Ù…Ù† Ù…Ù„Ù ØµÙˆØªÙŠ"""

        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
    y, sr = librosa.load(audio_file, sr=self.sample_rate)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ MFCC
    mfccs = librosa.feature.mfcc()
    y=y,
    sr=sr,
    n_mfcc=self.n_mfcc,
    n_fft=self.n_fft,
    hop_length=self.hop_length)

            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø®ØµØ§Ø¦Øµ
    mfccs = (mfccs - np.mean(mfccs, axis=1, keepdims=True)) / ()
    np.std(mfccs, axis=1, keepdims=True) + 1e 8
    )

    return mfccs.T  # Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ (time_steps, features)

        except Exception as e:
    logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ MFCC: {e}")
    return np.zeros((100, self.n_mfcc))  # Ø¥Ø±Ø¬Ø§Ø¹ Ø®ØµØ§Ø¦Øµ ÙØ§Ø±ØºØ©

    def simulate_mfcc_from_phonemes(self, phonemes: List[str]) -> np.ndarray:
    """Ù…Ø­Ø§ÙƒØ§Ø© Ø®ØµØ§Ø¦Øµ MFCC Ù…Ù† Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)"""

        # Ø¥Ù†Ø´Ø§Ø¡ Ø®ØµØ§Ø¦Øµ ØµÙˆØªÙŠØ© Ù…Ø­Ø§ÙƒØ§Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª
    sequence_length = len(phonemes) * 10  # 10 Ø¥Ø·Ø§Ø±Ø§Øª Ù„ÙƒÙ„ ÙÙˆÙ†ÙŠÙ…
    features = np.random.randn(sequence_length, self.n_mfcc) * 0.1

        # Ø¥Ø¶Ø§ÙØ© Ø®ØµØ§Ø¦Øµ Ù…Ù…ÙŠØ²Ø© Ù„ÙƒÙ„ ÙÙˆÙ†ÙŠÙ…
        for i, phoneme in enumerate(phonemes):
    start_frame = i * 10
    end_frame = (i + 1) * 10

            # Ø®ØµØ§Ø¦Øµ Ù…Ù…ÙŠØ²Ø© Ù„ÙƒÙ„ Ù†ÙˆØ¹ ÙÙˆÙ†ÙŠÙ…
            if phoneme in ['a', 'i', 'u', 'aa', 'ii', 'uu']:  # Ø£ØµÙˆØ§Øª ØµØ§Ø¦ØªØ©
    features[start_frame:end_frame, :5] += 0.5
            elif phoneme in ['m', 'n']:  # Ø£ØµÙˆØ§Øª Ø£Ù†ÙÙŠØ©
    features[start_frame:end_frame, 5:10] += 0.3
            elif phoneme in ['l', 'r']:  # Ø£ØµÙˆØ§Øª Ø³Ø§Ø¦Ù„Ø©
    features[start_frame:end_frame, 10:15] += 0.4

    return features


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATASET CLASSES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    data: List[Tuple[List[str], int]],
    processor: ArabicPhoneticProcessor, max_length: int = 20):
    self.data = data
    self.processor = processor
    self.max_length = max_length

    def __len__(self):
    def __len__(self):
    return len(self.data)

    def __getitem__(self, idx):
    def __getitem__(self, idx):
    syllables, label = self.data[idx]

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª
    phonemes = self.processor.syllables_to_phonemes(syllables)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙˆØ¥Ø¶Ø§ÙØ© Ø­Ø´Ùˆ
    encoded = self.processor.encode_phonemes(phonemes)
    padded = self.processor.pad_sequence(encoded, self.max_length)

    return torch.tensor(padded, dtype=torch.long), torch.tensor()
    label, dtype=torch.long
    )


class RelativePronounAudioDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ù…Ø¹ Ø§Ù„ØµÙˆØª"""

    def __init__()
    self,
    data: List[Tuple[List[str], str, int]],
    phonetic_processor: ArabicPhoneticProcessor,
    audio_processor: MFCCFeatureProcessor,
    max_phoneme_length: int = 20, max_audio_length: int = 200):
    self.data = data
    self.phonetic_processor = phonetic_processor
    self.audio_processor = audio_processor
    self.max_phoneme_length = max_phoneme_length
    self.max_audio_length = max_audio_length

    def __len__(self):
    def __len__(self):
    return len(self.data)

    def __getitem__(self, idx):
    def __getitem__(self, idx):
    syllables, audio_file, label = self.data[idx]

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª
    phonemes = self.phonetic_processor.syllables_to_phonemes(syllables)
    encoded_phonemes = self.phonetic_processor.encode_phonemes(phonemes)
    padded_phonemes = self.phonetic_processor.pad_sequence()
    encoded_phonemes, self.max_phoneme_length
    )

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª
        if audio_file and Path(audio_file).exists():
    audio_features = self.audio_processor.extract_mfcc_features(audio_file)
        else:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø®ØµØ§Ø¦Øµ ØµÙˆØªÙŠØ©
    audio_features = self.audio_processor.simulate_mfcc_from_phonemes(phonemes)

        # Ø­Ø´Ùˆ Ø£Ùˆ Ù‚Ø·Ø¹ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
        if len(audio_features) >= self.max_audio_length:
    audio_features = audio_features[: self.max_audio_length]
        else:
    padding = np.zeros()
    (self.max_audio_length - len(audio_features), audio_features.shape[1])
    )
    audio_features = np.vstack([audio_features, padding])

    return ()
    torch.tensor(padded_phonemes, dtype=torch.long),
    torch.tensor(audio_features, dtype=torch.float32),
    torch.tensor(label, dtype=torch.long))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEP LEARNING MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicRelativePronounLSTM(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    def __init__()
    self,
    vocab_size: int,
    embed_dim: int,
    hidden_size: int,
    num_classes: int,
    num_layers: int = 2, dropout: float = 0.1):
    super().__init__()

    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
    self.lstm = nn.LSTM()
    embed_dim,
    hidden_size,
    num_layers,
    batch_first=True,
    bidirectional=True,
    dropout=dropout)
    self.dropout = nn.Dropout(dropout)
    self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional

    def forward(self, x):
    def forward(self, x):
        # x: (batch_size, seq_len)
    embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)

        # LSTM forward pass
    lstm_out, (hidden, cell) = self.lstm(embedded)

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ø®Ø± Ø¥Ø®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø§ØªØ¬Ø§Ù‡ÙŠÙ†
        forward_out = lstm_out[:  1, : self.lstm.hidden_size]
    backward_out = lstm_out[: 0, self.lstm.hidden_size :]
    combined = torch.cat([forward_out, backward_out], dim=1)

        # Dropout ÙˆØ·Ø¨Ù‚Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
    dropped = self.dropout(combined)
    output = self.fc(dropped)

    return output


class ArabicRelativePronounTransformer(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Transformer Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    def __init__()
    self,
    vocab_size: int,
    d_model: int,
    nhead: int,
    num_encoder_layers: int,
    num_classes: int,
    dim_feedforward: int = 2048,
    dropout: float = 0.1, max_seq_length: int = 50):
    super().__init__()

    self.d_model = d_model
    self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
    self.pos_encoding = PositionalEncoding(d_model, dropout, max_seq_length)

    encoder_layer = nn.TransformerEncoderLayer()
    d_model=d_model,
    nhead=nhead,
    dim_feedforward=dim_feedforward,
    dropout=dropout,
    batch_first=True)

    self.transformer_encoder = nn.TransformerEncoder()
    encoder_layer, num_encoder_layers
    )
    self.classifier = nn.Linear(d_model, num_classes)

    def forward(self, x, src_key_padding_mask=None):
    def forward(self, x, src_key_padding_mask=None):
        # x: (batch_size, seq_len)
    embedded = self.embedding(x) * np.sqrt(self.d_model)
    embedded = self.pos_encoding(embedded)

        # Ø¥Ù†Ø´Ø§Ø¡ mask Ù„Ù„Ø­Ø´Ùˆ
        if src_key_padding_mask is None:
    src_key_padding_mask = x == 0  # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† 0 Ù‡Ùˆ Ø±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ

        # Transformer encoding
    encoded = self.transformer_encoder()
    embedded, src_key_padding_mask=src_key_padding_mask
    )

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø´Ùˆ)
    mask = (~src_key_padding_mask).float().unsqueeze( 1)
    pooled = (encoded * mask).sum(dim=1) / mask.sum(dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(pooled)

    return output


class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ¶Ø¹ Ù„Ù„Ù€ Transformer"""

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):

    super().__init__()
    self.dropout = nn.Dropout(p=dropout)

    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp()
    torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
    )

    pe[: 0::2] = torch.sin(position * div_term)
    pe[: 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0).transpose(0, 1)

    self.register_buffer('pe', pe)

    def forward(self, x):
    def forward(self, x):
    x = x + self.pe[: x.size(1), :].transpose(0, 1)
    return self.dropout(x)


class MultimodalRelativePronounModel(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© (Ù†Øµ + ØµÙˆØª)"""

    def __init__()
    self,
    vocab_size: int,
    text_embed_dim: int,
    audio_input_dim: int,
    hidden_size: int,
    num_classes: int, dropout: float = 0.1):
    super().__init__()

        # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†Øµ
    self.text_embedding = nn.Embedding(vocab_size, text_embed_dim, padding_idx=0)
    self.text_lstm = nn.LSTM()
    text_embed_dim, hidden_size, batch_first=True, bidirectional=True
    )

        # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØª
    self.audio_lstm = nn.LSTM()
    audio_input_dim, hidden_size, batch_first=True, bidirectional=True
    )

        # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    self.attention = nn.MultiheadAttention()
    hidden_size * 2, num_heads=8, batch_first=True
    )

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
    self.dropout = nn.Dropout(dropout)
    self.fusion_layer = nn.Linear(hidden_size * 4, hidden_size)
    self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, text_input, audio_input):
    def forward(self, text_input, audio_input):
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
    text_embedded = self.text_embedding(text_input)
    text_output, _ = self.text_lstm(text_embedded)
    text_pooled = text_output.mean(dim=1)  # Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª
    audio_output, _ = self.audio_lstm(audio_input)
    audio_pooled = audio_output.mean(dim=1)  # Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ³Ù„Ø³Ù„

        # Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
    combined = torch.cat([text_pooled, audio_pooled], dim=1)

        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¯Ù…Ø¬
    fused = F.relu(self.fusion_layer(combined))
    fused = self.dropout(fused)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
    output = self.classifier(fused)

    return output


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING AND EVALUATION ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounTrainer:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__(self, model, device='cpu'):

    self.model = model.to(device)
    self.device = device
    self.criterion = nn.CrossEntropyLoss()
    self.optimizer = optim.Adam(model.parameters(), lr=0.001)
    self.scheduler = optim.lr_scheduler.ReduceLROnPlateau()
    self.optimizer, patience=5, factor=0.5
    )

    self.train_losses = []
    self.val_losses = []
    self.train_accuracies = []
    self.val_accuracies = []

    def train_epoch(self, dataloader):
    """ØªØ¯Ø±ÙŠØ¨ Ø­Ù‚Ø¨Ø© ÙˆØ§Ø­Ø¯Ø©"""

    self.model.train()
    total_loss = 0
    correct = 0
    total = 0

        for batch_idx, batch_data in enumerate(dataloader):
            if len(batch_data) == 2:  # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ ÙÙ‚Ø·
    inputs, labels = batch_data
    inputs, labels = inputs.to(self.device), labels.to(self.device)
    outputs = self.model(inputs)
            else:  # Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    text_inputs, audio_inputs, labels = batch_data
    text_inputs = text_inputs.to(self.device)
    audio_inputs = audio_inputs.to(self.device)
    labels = labels.to(self.device)
    outputs = self.model(text_inputs, audio_inputs)

    loss = self.criterion(outputs, labels)

    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

    total_loss += loss.item()
    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += predicted.eq(labels).sum().item()

    avg_loss = total_loss / len(dataloader)
    accuracy = 100.0 * correct / total

    self.train_losses.append(avg_loss)
    self.train_accuracies.append(accuracy)

    return avg_loss, accuracy

    def validate(self, dataloader):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

    self.model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []

        with torch.no_grad():
            for batch_data in dataloader:
                if len(batch_data) == 2:  # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ ÙÙ‚Ø·
    inputs, labels = batch_data
    inputs, labels = inputs.to(self.device), labels.to(self.device)
    outputs = self.model(inputs)
                else:  # Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    text_inputs, audio_inputs, labels = batch_data
    text_inputs = text_inputs.to(self.device)
    audio_inputs = audio_inputs.to(self.device)
    labels = labels.to(self.device)
    outputs = self.model(text_inputs, audio_inputs)

    loss = self.criterion(outputs, labels)
    total_loss += loss.item()

    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += predicted.eq(labels).sum().item()

    all_predictions.extend(predicted.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = 100.0 * correct / total

    self.val_losses.append(avg_loss)
    self.val_accuracies.append(accuracy)

    return avg_loss, accuracy, all_predictions, all_labels

    def train(self, train_dataloader, val_dataloader, epochs=50):
    """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙƒØ§Ù…Ù„"""

    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù€ {epochs} Ø­Ù‚Ø¨Ø©")

    best_val_accuracy = 0

        for epoch in range(epochs):
            # ØªØ¯Ø±ÙŠØ¨
    train_loss, train_acc = self.train_epoch(train_dataloader)

            # ØªÙ‚ÙŠÙŠÙ…
    val_loss, val_acc, _, _ = self.validate(val_dataloader)

            # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    self.scheduler.step(val_loss)

            # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            if val_acc > best_val_accuracy:
    best_val_accuracy = val_acc
    torch.save(self.model.state_dict(), 'best_relative_pronoun_model.pth')

            if epoch % 10 == 0:
    logger.info()
    f"Ø­Ù‚Ø¨Ø© {epoch+1}/{epochs}: "
    f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
    f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%"
    )

    logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_val_accuracy:.2f}%")

    return best_val_accuracy

    def evaluate_detailed(self, dataloader, class_names):
    """ØªÙ‚ÙŠÙŠÙ… Ù…ÙØµÙ„ Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªÙ‚Ø¯Ù…Ø©"""

    _, accuracy, predictions, labels = self.validate(dataloader)

        # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    precision, recall, f1, _ = precision_recall_fscore_support()
    labels, predictions, average='weighted'
    )
    conf_matrix = confusion_matrix(labels, predictions)

    results = {
    'accuracy': accuracy / 100,  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø©
    'precision': precision,
    'recall': recall,
    'f1_score': f1,
    'confusion_matrix': conf_matrix.tolist(),
    'class_names': class_names,
    }

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFERENCE ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class RelativePronounInferenceEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©"""

    def __init__()
    self,
    model,
    processor: ArabicPhoneticProcessor,
        class_names: List[str], device='cpu'):
    self.model = model.to(device)
    self.processor = processor
    self.class_names = class_names
    self.device = device

    self.model.eval()

    def predict_from_syllables()
    self, syllables: List[str], max_length: int = 20
    ) -> Dict[str, Any]:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ ÙÙˆÙ†ÙŠÙ…Ø§Øª
    phonemes = self.processor.syllables_to_phonemes(syllables)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙˆØ­Ø´Ùˆ
    encoded = self.processor.encode_phonemes(phonemes)
    padded = self.processor.pad_sequence(encoded, max_length)

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ tensor
    input_tensor = torch.tensor([padded], dtype=torch.long).to(self.device)

        with torch.no_grad():
    outputs = self.model(input_tensor)
    probabilities = F.softmax(outputs, dim=1)
    predicted_class = outputs.argmax(1).item()
    confidence = probabilities.max().item()

    return {
    'predicted_class': self.class_names[predicted_class],
    'confidence': confidence,
    'class_probabilities': {
    name: prob
                for name, prob in zip(self.class_names, probabilities[0].cpu().numpy())
    },
    'input_syllables': syllables,
    'phonemes': phonemes,
    }

    def predict_batch()
    self, syllables_list: List[List[str]], max_length: int = 20
    ) -> List[Dict[str, Any]]:
    """Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    results = []

        for syllables in syllables_list:
    result = self.predict_from_syllables(syllables, max_length)
    results.append(result)

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION FUNCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def demonstrate_deep_models():
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©"""

    print("ğŸ§  Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 60)

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    processor = ArabicPhoneticProcessor()

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    training_data = [
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ"], 0),  # Ø§Ù„Ø°ÙŠ
    (["Ø§Ù„Ù’", "ØªÙÙŠ"], 1),  # Ø§Ù„ØªÙŠ
    (["Ø§Ù„Ù’", "Ù„Ù", "Ø°ÙØ§", "Ù†Ù"], 2),  # Ø§Ù„Ù„Ø°Ø§Ù†
    (["Ø§Ù„Ù’", "Ù„Ù", "ØªÙØ§", "Ù†Ù"], 3),  # Ø§Ù„Ù„ØªØ§Ù†
    (["Ø§Ù„Ù’", "Ø°ÙÙŠ", "Ù†Ù"], 4),  # Ø§Ù„Ø°ÙŠÙ†
    (["Ø§Ù„Ù’", "Ù„ÙØ§", "ØªÙÙŠ"], 5),  # Ø§Ù„Ù„Ø§ØªÙŠ
    (["Ù…ÙÙ†Ù’"], 6),  # Ù…ÙÙ†
    (["Ù…ÙØ§"], 7),  # Ù…Ø§
    ]

    class_names = ["Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ù„Ø°Ø§Ù†", "Ø§Ù„Ù„ØªØ§Ù†", "Ø§Ù„Ø°ÙŠÙ†", "Ø§Ù„Ù„Ø§ØªÙŠ", "Ù…ÙÙ†", "Ù…Ø§"]

    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    dataset = RelativePronounDataset(training_data, processor, max_length=15)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    models = {
    "LSTM": ArabicRelativePronounLSTM()
    vocab_size=processor.vocab_size,
    embed_dim=64,
    hidden_size=128,
    num_classes=len(class_names),
    num_layers=2),
    "Transformer": ArabicRelativePronounTransformer()
    vocab_size=processor.vocab_size,
    d_model=128,
    nhead=8,
    num_encoder_layers=3,
    num_classes=len(class_names),
    dim_feedforward=512),
    }

    # ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    for model_name, model in models.items():
    print(f"\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ {model_name}")
    print(" " * 30)

    trainer = RelativePronounTrainer(model)

        # ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹
        for epoch in range(5):
    train_loss, train_acc = trainer.train_epoch(dataloader)
    print(f"Ø­Ù‚Ø¨Ø© {epoch+1}: Loss={train_loss:.4f}, Accuracy={train_acc:.2f}%")

        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬
    inference_engine = RelativePronounInferenceEngine(model, processor, class_names)

    test_cases = [
    ["Ø§Ù„Ù’", "Ø°ÙÙŠ"],  # Ø§Ù„Ø°ÙŠ
    ["Ø§Ù„Ù’", "ØªÙÙŠ"],  # Ø§Ù„ØªÙŠ
    ["Ù…ÙÙ†Ù’"],  # Ù…ÙÙ†
    ]

    print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}:")
        for syllables in test_cases:
    result = inference_engine.predict_from_syllables(syllables)
    print()
    f"   {syllables} â†’ {result['predicted_class']} (Ø«Ù‚Ø©: {result['confidence']:.2f})"
    )

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©!")


if __name__ == "__main__":
    demonstrate_deep_models()

