#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Demonstrative Pronouns Generation System
=============================================
Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©

Comprehensive system for generating Arabic demonstrative pronouns from syllable
sequences using advanced pattern recognition and morphological analysis.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - DEMONSTRATIVE PRONOUNS SYSTEM
Date: 2025-07-24
Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc


import logging  # noqa: F401
import json  # noqa: F401
import yaml  # noqa: F401
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict  # noqa: F401
from enum import Enum  # noqa: F401
from pathlib import Path  # noqa: F401
import numpy as np  # noqa: F401

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig()
    level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATIVE PRONOUNS CLASSIFICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DemonstrativeCategory(Enum):
    """ÙØ¦Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©"""

    NEAR_MASCULINE_SINGULAR = "Ù‚Ø±ÙŠØ¨_Ù…Ø°ÙƒØ±_Ù…ÙØ±Ø¯"  # Ù‡Ø°Ø§
    NEAR_FEMININE_SINGULAR = "Ù‚Ø±ÙŠØ¨_Ù…Ø¤Ù†Ø«_Ù…ÙØ±Ø¯"  # Ù‡Ø°Ù‡
    FAR_MASCULINE_SINGULAR = "Ø¨Ø¹ÙŠØ¯_Ù…Ø°ÙƒØ±_Ù…ÙØ±Ø¯"  # Ø°Ù„Ùƒ
    FAR_FEMININE_SINGULAR = "Ø¨Ø¹ÙŠØ¯_Ù…Ø¤Ù†Ø«_Ù…ÙØ±Ø¯"  # ØªÙ„Ùƒ
    NEAR_MASCULINE_DUAL = "Ù‚Ø±ÙŠØ¨_Ù…Ø°ÙƒØ±_Ù…Ø«Ù†Ù‰"  # Ù‡Ø°Ø§Ù†/Ù‡Ø°ÙŠÙ†
    NEAR_FEMININE_DUAL = "Ù‚Ø±ÙŠØ¨_Ù…Ø¤Ù†Ø«_Ù…Ø«Ù†Ù‰"  # Ù‡Ø§ØªØ§Ù†/Ù‡Ø§ØªÙŠÙ†
    FAR_MASCULINE_DUAL = "Ø¨Ø¹ÙŠØ¯_Ù…Ø°ÙƒØ±_Ù…Ø«Ù†Ù‰"  # Ø°Ø§Ù†Ùƒ/Ø°ÙŠÙ†Ùƒ
    FAR_FEMININE_DUAL = "Ø¨Ø¹ÙŠØ¯_Ù…Ø¤Ù†Ø«_Ù…Ø«Ù†Ù‰"  # ØªØ§Ù†Ùƒ/ØªÙŠÙ†Ùƒ
    NEAR_PLURAL = "Ù‚Ø±ÙŠØ¨_Ø¬Ù…Ø¹"  # Ù‡Ø¤Ù„Ø§Ø¡
    FAR_PLURAL = "Ø¨Ø¹ÙŠØ¯_Ø¬Ù…Ø¹"  # Ø£ÙˆÙ„Ø¦Ùƒ
    LOCATIVE_NEAR = "Ù…ÙƒØ§Ù†ÙŠ_Ù‚Ø±ÙŠØ¨"  # Ù‡Ù†Ø§/Ù‡Ø§Ù‡Ù†Ø§
    LOCATIVE_FAR = "Ù…ÙƒØ§Ù†ÙŠ_Ø¨Ø¹ÙŠØ¯"  # Ù‡Ù†Ø§Ùƒ/Ù‡Ù†Ø§Ù„Ùƒ


class GrammaticalCase(Enum):
    """Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©"""

    NOMINATIVE = "Ù…Ø±ÙÙˆØ¹"  # Ø§Ù„Ù…Ø¨ØªØ¯Ø£ØŒ Ø§Ù„ÙØ§Ø¹Ù„
    ACCUSATIVE = "Ù…Ù†ØµÙˆØ¨"  # Ø§Ù„Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡ØŒ Ø§Ø³Ù… ÙƒØ§Ù†
    GENITIVE = "Ù…Ø¬Ø±ÙˆØ±"  # Ø§Ù„Ù…Ø¶Ø§Ù Ø¥Ù„ÙŠÙ‡ØŒ Ù…Ø¬Ø±ÙˆØ± Ø¨Ø­Ø±Ù Ø¬Ø±


@dataclass
class DemonstrativePronoun:
    """ØªÙ…Ø«ÙŠÙ„ Ø§Ø³Ù… Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""

    text: str  # Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Ù‡Ø°Ø§ØŒ Ù‡Ø°Ù‡ØŒ Ø¥Ù„Ø®)
    category: DemonstrativeCategory  # Ø§Ù„ÙØ¦Ø©
    syllables: List[str]  # Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©
    phonetic_features: List[str]  # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
    grammatical_case: GrammaticalCase  # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ÙŠØ©
    distance: str  # Ù‚Ø±ÙŠØ¨/Ø¨Ø¹ÙŠØ¯
    gender: str  # Ù…Ø°ÙƒØ±/Ù…Ø¤Ù†Ø«/Ù…Ø­Ø§ÙŠØ¯
    number: str  # Ù…ÙØ±Ø¯/Ù…Ø«Ù†Ù‰/Ø¬Ù…Ø¹
    usage_contexts: List[str]  # Ø³ÙŠØ§Ù‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    frequency_score: float  # Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± (0 1)
    morphological_pattern: str  # Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµØ±ÙÙŠ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATIVE PRONOUNS DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicDemonstrativePronounsDatabase:
    """Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    def __init__(self):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.demonstrative_pronouns: List[DemonstrativePronoun] = []
    self.syllable_patterns: Dict[str, List[str]] = {}
    self._initialize_database()

    def _initialize_database(self):  # type: ignore[no-untyped def]
    """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©"""

    demonstratives_data = [
            # Ù„Ù„Ù‚Ø±ÙŠØ¨ - Ø§Ù„Ù…ÙØ±Ø¯
    {
    "text": "Ù‡Ø°Ø§",
    "category": DemonstrativeCategory.NEAR_MASCULINE_SINGULAR,
    "syllables": ["Ù‡ÙØ§", "Ø°ÙØ§"],
    "phonetic_features": ["h", "aa", "dh", "aa"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø°ÙƒØ±",
    "number": "Ù…ÙØ±Ø¯",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù‚Ø±ÙŠØ¨ Ø§Ù„Ù…Ø°ÙƒØ±", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ØªØ®ØµÙŠØµ"],
    "frequency_score": 0.95,
    "morphological_pattern": "CV CV",
    },
    {
    "text": "Ù‡Ø°Ù‡",
    "category": DemonstrativeCategory.NEAR_FEMININE_SINGULAR,
    "syllables": ["Ù‡ÙØ§", "Ø°ÙÙ‡Ù"],
    "phonetic_features": ["h", "aa", "dh", "i", "h", "i"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø¤Ù†Ø«",
    "number": "Ù…ÙØ±Ø¯",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù‚Ø±ÙŠØ¨Ø© Ø§Ù„Ù…Ø¤Ù†Ø«Ø©", "Ø§Ù„ØªØ¹Ø±ÙŠÙ", "Ø§Ù„ØªØ®ØµÙŠØµ"],
    "frequency_score": 0.93,
    "morphological_pattern": "CV CVC",
    },
            # Ù„Ù„Ø¨Ø¹ÙŠØ¯ - Ø§Ù„Ù…ÙØ±Ø¯
    {
    "text": "Ø°Ù„Ùƒ",
    "category": DemonstrativeCategory.FAR_MASCULINE_SINGULAR,
    "syllables": ["Ø°ÙØ§", "Ù„ÙÙƒÙ"],
    "phonetic_features": ["dh", "aa", "l", "i", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø°ÙƒØ±",
    "number": "Ù…ÙØ±Ø¯",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ø¨Ø¹ÙŠØ¯ Ø§Ù„Ù…Ø°ÙƒØ±", "Ø§Ù„ØªÙØ³ÙŠØ±", "Ø§Ù„Ø¥Ø­Ø§Ù„Ø©"],
    "frequency_score": 0.91,
    "morphological_pattern": "CV CVC",
    },
    {
    "text": "ØªÙ„Ùƒ",
    "category": DemonstrativeCategory.FAR_FEMININE_SINGULAR,
    "syllables": ["ØªÙÙ„", "ÙƒÙ"],
    "phonetic_features": ["t", "i", "l", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø¤Ù†Ø«",
    "number": "Ù…ÙØ±Ø¯",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ø¨Ø¹ÙŠØ¯Ø© Ø§Ù„Ù…Ø¤Ù†Ø«Ø©", "Ø§Ù„ØªÙØ³ÙŠØ±", "Ø§Ù„Ø¥Ø­Ø§Ù„Ø©"],
    "frequency_score": 0.89,
    "morphological_pattern": "CVC CV",
    },
            # Ù„Ù„Ù‚Ø±ÙŠØ¨ - Ø§Ù„Ù…Ø«Ù†Ù‰
    {
    "text": "Ù‡Ø°Ø§Ù†",
    "category": DemonstrativeCategory.NEAR_MASCULINE_DUAL,
    "syllables": ["Ù‡ÙØ§", "Ø°ÙØ§", "Ù†Ù"],
    "phonetic_features": ["h", "aa", "dh", "aa", "n", "i"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø°ÙƒØ±",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ÙŠÙ† Ù‚Ø±ÙŠØ¨ÙŠÙ† Ù…Ø°ÙƒØ±ÙŠÙ†", "Ø§Ù„ØªØ®ØµÙŠØµ Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬"],
    "frequency_score": 0.72,
    "morphological_pattern": "CV-CV CV",
    },
    {
    "text": "Ù‡Ø°ÙŠÙ†",
    "category": DemonstrativeCategory.NEAR_MASCULINE_DUAL,
    "syllables": ["Ù‡ÙØ§", "Ø°ÙÙŠÙ’", "Ù†Ù"],
    "phonetic_features": ["h", "aa", "dh", "ay", "n", "i"],
    "grammatical_case": GrammaticalCase.ACCUSATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø°ÙƒØ±",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ÙŠÙ† Ù‚Ø±ÙŠØ¨ÙŠÙ† Ù…Ø°ÙƒØ±ÙŠÙ† Ù…Ù†ØµÙˆØ¨/Ù…Ø¬Ø±ÙˆØ±"],
    "frequency_score": 0.68,
    "morphological_pattern": "CV-CVC CV",
    },
    {
    "text": "Ù‡Ø§ØªØ§Ù†",
    "category": DemonstrativeCategory.NEAR_FEMININE_DUAL,
    "syllables": ["Ù‡ÙØ§", "ØªÙØ§", "Ù†Ù"],
    "phonetic_features": ["h", "aa", "t", "aa", "n", "i"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø¤Ù†Ø«",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ØªÙŠÙ† Ù‚Ø±ÙŠØ¨ØªÙŠÙ† Ù…Ø¤Ù†Ø«ØªÙŠÙ†"],
    "frequency_score": 0.65,
    "morphological_pattern": "CV-CV CV",
    },
    {
    "text": "Ù‡Ø§ØªÙŠÙ†",
    "category": DemonstrativeCategory.NEAR_FEMININE_DUAL,
    "syllables": ["Ù‡ÙØ§", "ØªÙÙŠÙ’", "Ù†Ù"],
    "phonetic_features": ["h", "aa", "t", "ay", "n", "i"],
    "grammatical_case": GrammaticalCase.ACCUSATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø¤Ù†Ø«",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ØªÙŠÙ† Ù‚Ø±ÙŠØ¨ØªÙŠÙ† Ù…Ø¤Ù†Ø«ØªÙŠÙ† Ù…Ù†ØµÙˆØ¨/Ù…Ø¬Ø±ÙˆØ±"],
    "frequency_score": 0.62,
    "morphological_pattern": "CV-CVC CV",
    },
            # Ù„Ù„Ø¨Ø¹ÙŠØ¯ - Ø§Ù„Ù…Ø«Ù†Ù‰
    {
    "text": "Ø°Ø§Ù†Ùƒ",
    "category": DemonstrativeCategory.FAR_MASCULINE_DUAL,
    "syllables": ["Ø°ÙØ§", "Ù†Ù", "ÙƒÙ"],
    "phonetic_features": ["dh", "aa", "n", "i", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø°ÙƒØ±",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ÙŠÙ† Ø¨Ø¹ÙŠØ¯ÙŠÙ† Ù…Ø°ÙƒØ±ÙŠÙ†"],
    "frequency_score": 0.45,
    "morphological_pattern": "CV-CV CV",
    },
    {
    "text": "ØªØ§Ù†Ùƒ",
    "category": DemonstrativeCategory.FAR_FEMININE_DUAL,
    "syllables": ["ØªÙØ§", "Ù†Ù", "ÙƒÙ"],
    "phonetic_features": ["t", "aa", "n", "i", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø¤Ù†Ø«",
    "number": "Ù…Ø«Ù†Ù‰",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø§Ø«Ù†ØªÙŠÙ† Ø¨Ø¹ÙŠØ¯ØªÙŠÙ† Ù…Ø¤Ù†Ø«ØªÙŠÙ†"],
    "frequency_score": 0.42,
    "morphological_pattern": "CV-CV CV",
    },
            # Ù„Ù„Ø¬Ù…Ø¹
    {
    "text": "Ù‡Ø¤Ù„Ø§Ø¡",
    "category": DemonstrativeCategory.NEAR_PLURAL,
    "syllables": ["Ù‡ÙØ§", "Ø¤Ù", "Ù„ÙØ§", "Ø¡Ù"],
    "phonetic_features": ["h", "aa", "u", "l", "aa", "i"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ø¬Ù…Ø¹",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø¬Ù…Ø¹ Ù‚Ø±ÙŠØ¨", "Ø§Ù„Ø¹Ù…ÙˆÙ… Ø§Ù„Ù‚Ø±ÙŠØ¨"],
    "frequency_score": 0.87,
    "morphological_pattern": "CV-CV-CV CV",
    },
    {
    "text": "Ø£ÙˆÙ„Ø¦Ùƒ",
    "category": DemonstrativeCategory.FAR_PLURAL,
    "syllables": ["Ø£ÙÙˆ", "Ù„ÙØ§", "Ø¦Ù", "ÙƒÙ"],
    "phonetic_features": ["u", "w", "l", "aa", "i", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ø¬Ù…Ø¹",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ø¬Ù…Ø¹ Ø¨Ø¹ÙŠØ¯", "Ø§Ù„Ø¹Ù…ÙˆÙ… Ø§Ù„Ø¨Ø¹ÙŠØ¯"],
    "frequency_score": 0.84,
    "morphological_pattern": "CVC-CV-CV CV",
    },
            # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
    {
    "text": "Ù‡Ù†Ø§",
    "category": DemonstrativeCategory.LOCATIVE_NEAR,
    "syllables": ["Ù‡Ù", "Ù†ÙØ§"],
    "phonetic_features": ["h", "u", "n", "aa"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ù…ÙƒØ§Ù†ÙŠ",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©", "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒØ§Ù†"],
    "frequency_score": 0.92,
    "morphological_pattern": "CV CV",
    },
    {
    "text": "Ù‡Ø§Ù‡Ù†Ø§",
    "category": DemonstrativeCategory.LOCATIVE_NEAR,
    "syllables": ["Ù‡ÙØ§", "Ù‡Ù", "Ù†ÙØ§"],
    "phonetic_features": ["h", "aa", "h", "u", "n", "aa"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ù‚Ø±ÙŠØ¨",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ù…ÙƒØ§Ù†ÙŠ",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ø§Ù„Ù…Ø¤ÙƒØ¯Ø©"],
    "frequency_score": 0.58,
    "morphological_pattern": "CV-CV CV",
    },
    {
    "text": "Ù‡Ù†Ø§Ùƒ",
    "category": DemonstrativeCategory.LOCATIVE_FAR,
    "syllables": ["Ù‡Ù", "Ù†ÙØ§", "ÙƒÙ"],
    "phonetic_features": ["h", "u", "n", "aa", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ù…ÙƒØ§Ù†ÙŠ",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø©", "ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø¨Ø¹ÙŠØ¯"],
    "frequency_score": 0.89,
    "morphological_pattern": "CV-CV CV",
    },
    {
    "text": "Ù‡Ù†Ø§Ù„Ùƒ",
    "category": DemonstrativeCategory.LOCATIVE_FAR,
    "syllables": ["Ù‡Ù", "Ù†ÙØ§", "Ù„Ù", "ÙƒÙ"],
    "phonetic_features": ["h", "u", "n", "aa", "l", "i", "k", "a"],
    "grammatical_case": GrammaticalCase.NOMINATIVE,
    "distance": "Ø¨Ø¹ÙŠØ¯",
    "gender": "Ù…Ø­Ø§ÙŠØ¯",
    "number": "Ù…ÙƒØ§Ù†ÙŠ",
    "usage_contexts": ["Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø© Ø§Ù„Ù…Ø¤ÙƒØ¯Ø©"],
    "frequency_score": 0.73,
    "morphological_pattern": "CV-CV-CV CV",
    },
    ]

        # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù†Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
        for data in demonstratives_data:
    demonstrative = DemonstrativePronoun(**data)
    self.demonstrative_pronouns.append(demonstrative)

        # Ø¨Ù†Ø§Ø¡ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    self._build_syllable_patterns()

    logger.info()
    f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {len(self.demonstrative_pronouns)} Ø§Ø³Ù… Ø¥Ø´Ø§Ø±Ø©"
    )  # noqa: E501

    def _build_syllable_patterns(self):  # type: ignore[no-untyped def]
    """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©"""

        for demonstrative in self.demonstrative_pronouns:
    pattern = ' '.join()
    [self._get_syllable_type(syll) for syll in demonstrative.syllables]
    )

            if pattern not in self.syllable_patterns:
    self.syllable_patterns[pattern] = []

    self.syllable_patterns[pattern].append(demonstrative.text)

    logger.info(f"ğŸ“Š Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©: {len(self.syllable_patterns)} Ù†Ù…Ø·")

    def _get_syllable_type(self, syllable: str) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‚Ø·Ø¹ (CV, CVC, Ø¥Ù„Ø®)"""

    consonants = "Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠØ¡Ø¢Ø£Ø¥Ø¦Ø¤Ø©"
    vowels = "Ø§ÙˆÙŠÙŠØ©ÙÙÙÙ’"

    pattern = ""
        for char in syllable:
            if char in consonants:
    pattern += "C"
            elif char in vowels:
    pattern += "V"

        # ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        if len(pattern) > 4:
    return "COMPLEX"

    return pattern if pattern else "CV"

    def get_by_category()
    self, category: DemonstrativeCategory
    ) -> List[DemonstrativePronoun]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©"""
    return [d for d in self.demonstrative_pronouns if d.category == category]

    def get_by_distance(self, distance: str) -> List[DemonstrativePronoun]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ©"""
    return [d for d in self.demonstrative_pronouns if d.distance == distance]

    def get_by_number(self, number: str) -> List[DemonstrativePronoun]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø¯Ø¯"""
    return [d for d in self.demonstrative_pronouns if d.number == number]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYLLABLE ANALYZER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DemonstrativeSyllableAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©"""

    def __init__(self, database: ArabicDemonstrativePronounsDatabase):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.database = database
    self.phoneme_weights = self._initialize_phoneme_weights()

    def _initialize_phoneme_weights(self) -> Dict[str, float]:
    """ØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ØµÙˆØ§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""

    return {
            # Ø£ØµÙˆØ§Øª Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©
    'h': 0.95,  # Ù‡ (Ù‡Ø°Ø§ØŒ Ù‡Ø°Ù‡ØŒ Ù‡Ù†Ø§)
    'dh': 0.90,  # Ø° (Ù‡Ø°Ø§ØŒ Ù‡Ø°Ù‡ØŒ Ø°Ù„Ùƒ)
    'l': 0.85,  # Ù„ (Ø°Ù„ÙƒØŒ Ù‡Ø¤Ù„Ø§Ø¡ØŒ Ù‡Ù†Ø§Ù„Ùƒ)
    'k': 0.80,  # Ùƒ (Ø°Ù„ÙƒØŒ ØªÙ„ÙƒØŒ Ù‡Ù†Ø§Ùƒ)
    't': 0.75,  # Øª (ØªÙ„ÙƒØŒ Ù‡Ø§ØªØ§Ù†)
    'n': 0.70,  # Ù† (Ù‡Ø°Ø§Ù†ØŒ Ù‡Ø§ØªØ§Ù†ØŒ Ù‡Ù†Ø§)
    'aa': 0.88,  # Ø§ (Ù…Ø¹Ø¸Ù… Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©)
    'a': 0.82,  # Ù
    'i': 0.78,  # Ù
    'u': 0.75,  # Ù
    'ay': 0.70,  # ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø«Ù†Ù‰ Ù…Ù†ØµÙˆØ¨/Ù…Ø¬Ø±ÙˆØ±)
    'w': 0.65,  # Ùˆ (Ø£ÙˆÙ„Ø¦Ùƒ)
    }

    def analyze_syllables(self, syllables: List[str]) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""

    analysis = {
    'syllables_count': len(syllables),
    'syllable_types': [],
    'pattern': None,
    'complexity_score': 0.0,
    'phonetic_features': [],
    'matching_candidates': [],
    }

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù‚Ø·Ø¹
        for syllable in syllables:
    syll_type = self.database._get_syllable_type(syllable)
    analysis['syllable_types'].append(syll_type)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
    phonetic = self._extract_phonetic_features(syllable)
    analysis['phonetic_features'].extend(phonetic)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·
    analysis['pattern'] = ' '.join(analysis['syllable_types'])

        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
    analysis['complexity_score'] = self._calculate_complexity(syllables)

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ÙŠÙ†
    analysis['matching_candidates'] = self._find_matching_candidates(syllables)

    return analysis

    def _extract_phonetic_features(self, syllable: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø·Ø¹"""

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ ØµÙˆØªÙŠ
    phonetic_map = {
    'Ù‡': 'h',
    'Ø°': 'dh',
    'Ù„': 'l',
    'Ùƒ': 'k',
    'Øª': 't',
    'Ù†': 'n',
    'Ø§': 'aa',
    'Ùˆ': 'w',
    'ÙŠ': 'y',
    'Ø¡': 'q',
    'Ø£': 'a',
    'Ø¥': 'i',
    'Ù': 'u',
    'Ù': 'i',
    'Ù': 'a',
    'Ù’': '',
    'Ø©': 'h',
    'Ø¤': 'u',
    'Ø¦': 'i',
    'Ø¢': 'aa',
    }

    features = []
        for char in syllable:
            if char in phonetic_map:
    phonetic = phonetic_map[char]
                if phonetic:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
    features.append(phonetic)

    return features

    def _calculate_complexity(self, syllables: List[str]) -> float:
    """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    complexity = 0.0

        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    complexity += len(syllables) * 0.2

        # ØªÙ†ÙˆØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    types = set(self.database._get_syllable_type(s) for s in syllables)
    complexity += len(types) * 0.3

        # ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø¹Ù‚Ø¯Ø©
        for syllable in syllables:
            if len(syllable) > 3:
    complexity += 0.5

    return min(complexity, 5.0)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5

    def _find_matching_candidates()
    self, input_syllables: List[str]
    ) -> List[Dict[str, Any]]:
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""

    candidates = []

        for demonstrative in self.database.demonstrative_pronouns:
    similarity = self._calculate_similarity()
    input_syllables, demonstrative.syllables
    )

            if similarity > 0.3:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
    candidate = {
    'demonstrative': demonstrative.text,
    'category': demonstrative.category.value,
    'similarity': similarity,
    'syllables': demonstrative.syllables,
    'distance': demonstrative.distance,
    'gender': demonstrative.gender,
    'number': demonstrative.number,
    'frequency_score': demonstrative.frequency_score,
    }
    candidates.append(candidate)

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    candidates.sort(key=lambda x: x['similarity'], reverse=True)

    return candidates[:5]  # Ø£ÙØ¶Ù„ 5 Ù…Ø±Ø´Ø­ÙŠÙ†

    def _calculate_similarity()
    self, syllables1: List[str], syllables2: List[str]
    ) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø¹Ø¯Ø¯
    length_similarity = 1.0 - abs(len(syllables1) - len(syllables2)) / max()
    len(syllables1), len(syllables2)
    )

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    content_similarity = 0.0
    max_length = max(len(syllables1), len(syllables2))

        for i in range(max_length):
            if i < len(syllables1) and i < len(syllables2):
                # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ØªÙ†Ø§Ø¸Ø±Ø©
    syll_sim = self._syllable_similarity(syllables1[i], syllables2[i])
    content_similarity += syll_sim
            else:
                # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
    content_similarity += 0.0

    content_similarity /= max_length

        # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_similarity = (length_similarity * 0.3) + (content_similarity * 0.7)

    return final_similarity

    def _syllable_similarity(self, syll1: str, syll2: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ù‚Ø·Ø¹ÙŠÙ†"""

        if syll1 == syll2:
    return 1.0

        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
    features1 = self._extract_phonetic_features(syll1)
    features2 = self._extract_phonetic_features(syll2)

        if not features1 or not features2:
    return 0.0

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØµÙˆØªÙŠ
    common = set(features1) & set(features2)
    total = set(features1) | set(features2)

        if not total:
    return 0.0

    jaccard_similarity = len(common) / len(total)

        # ØªØ·Ø¨ÙŠÙ‚ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ØµÙˆØ§Øª
    weighted_similarity = 0.0
        for feature in common:
    weight = self.phoneme_weights.get(feature, 0.5)
    weighted_similarity += weight

        if features1:
    weighted_similarity /= len(features1)

        # Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    return (jaccard_similarity + weighted_similarity) / 2


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicDemonstrativePronounsGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""

    def __init__(self, config_path: Optional[str] = None):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.config = self._load_config(config_path)
    self.demonstrative_pronouns_db = ArabicDemonstrativePronounsDatabase()
    self.syllable_analyzer = DemonstrativeSyllableAnalyzer()
    self.demonstrative_pronouns_db
    )

    logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©")

    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""

        default_config = {
    'similarity_threshold': 0.6,
    'max_candidates': 5,
    'phonetic_weight': 0.7,
    'frequency_weight': 0.3,
    'enable_fuzzy_matching': True,
    'case_sensitive': False,
    }

        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf 8') as f:
    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
    logger.warning(f"ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: {e}")

    return default_config

    def generate_demonstrative_pronouns_from_syllables()
    self, syllables: List[str]
    ) -> Dict[str, Any]:
    """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""

    logger.info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹: {syllables}")

        if not syllables:
    return {
    'success': False,
    'error': 'Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙØ§Ø±ØºØ©',
    'syllables': syllables,
    }

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    analysis = self.syllable_analyzer.analyze_syllables(syllables)

        # ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    matches = self._advanced_matching(syllables, analysis)

        if not matches:
    return {
    'success': False,
    'error': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ù…Ù†Ø§Ø³Ø¨',
    'syllables': syllables,
    'analysis': analysis,
    }

        # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚
    best_match = self._select_best_match(matches)

    result = {
    'success': True,
    'best_match': best_match,
    'all_matches': matches,
    'syllables': syllables,
    'analysis': analysis,
    'confidence': best_match['confidence'],
    }

    logger.info()
    f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚: {best_match['demonstrative']} Ø¨Ø«Ù‚Ø© {best_match['confidence']:.2f}}"
    )  # noqa: E501

    return result

    def _advanced_matching()
    self, syllables: List[str], analysis: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
    """Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""

    matches = []

        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
        for candidate in analysis['matching_candidates']:
    confidence = self._calculate_confidence(syllables, candidate)

            if confidence >= self.config['similarity_threshold']:
    match = {
    'demonstrative': candidate['demonstrative'],
    'category': candidate['category'],
    'confidence': confidence,
    'similarity': candidate['similarity'],
    'syllables': candidate['syllables'],
    'distance': candidate['distance'],
    'gender': candidate['gender'],
    'number': candidate['number'],
    'frequency_score': candidate['frequency_score'],
    'matching_method': 'direct_similarity',
    }
    matches.append(match)

        # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠ
    pattern_matches = self._pattern_based_matching(syllables, analysis['pattern'])
    matches.extend(pattern_matches)

        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¹Ù„Ø§Ù‹
        if self.config['enable_fuzzy_matching'] and len(matches) < 3:
    fuzzy_matches = self._fuzzy_matching(syllables)
    matches.extend(fuzzy_matches)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    unique_matches = self._deduplicate_matches(matches)
    unique_matches.sort(key=lambda x: x['confidence'], reverse=True)

    return unique_matches[: self.config['max_candidates']]

    def _calculate_confidence()
    self, syllables: List[str], candidate: Dict[str, Any]
    ) -> float:
    """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù…Ø±Ø´Ø­"""

        # Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø«Ù‚Ø©
    similarity_score = candidate['similarity']
    frequency_score = candidate['frequency_score']

        # ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¯Ø¯ (Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹)
    length_match = 1.0 if len(syllables) == len(candidate['syllables']) else 0.7

        # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    confidence = ()
    similarity_score * self.config['phonetic_weight']
    + frequency_score * self.config['frequency_weight']
    ) * length_match

    return min(confidence, 1.0)

    def _pattern_based_matching()
    self, syllables: List[str], pattern: str
    ) -> List[Dict[str, Any]]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠ"""

    matches = []

        if pattern in self.demonstrative_pronouns_db.syllable_patterns:
            for demonstrative_text in self.demonstrative_pronouns_db.syllable_patterns[
    pattern
    ]:
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø³Ù… Ø§Ù„Ø¥Ø´Ø§Ø±Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    demonstrative = next()
    ()
    d
                        for d in self.demonstrative_pronouns_db.demonstrative_pronouns
                        if d.text == demonstrative_text
    ),
    None)

                if demonstrative:
    confidence = 0.75  # Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù†Ù…Ø·ÙŠ

    match = {
    'demonstrative': demonstrative.text,
    'category': demonstrative.category.value,
    'confidence': confidence,
    'similarity': 0.8,
    'syllables': demonstrative.syllables,
    'distance': demonstrative.distance,
    'gender': demonstrative.gender,
    'number': demonstrative.number,
    'frequency_score': demonstrative.frequency_score,
    'matching_method': 'pattern_based',
    }
    matches.append(match)

    return matches

    def _fuzzy_matching(self, syllables: List[str]) -> List[Dict[str, Any]]:
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠ Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©"""

    matches = []

        # Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ ØªØ³Ø§Ù‡Ù„ Ø£ÙƒØ¨Ø± ÙÙŠ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        for demonstrative in self.demonstrative_pronouns_db.demonstrative_pronouns:
    similarity = self.syllable_analyzer._calculate_similarity()
    syllables, demonstrative.syllables
    )

            if similarity > 0.4:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù…Ù†Ø®ÙØ¶ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠ
    confidence = similarity * 0.8  # Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠ

    match = {
    'demonstrative': demonstrative.text,
    'category': demonstrative.category.value,
    'confidence': confidence,
    'similarity': similarity,
    'syllables': demonstrative.syllables,
    'distance': demonstrative.distance,
    'gender': demonstrative.gender,
    'number': demonstrative.number,
    'frequency_score': demonstrative.frequency_score,
    'matching_method': 'fuzzy_matching',
    }
    matches.append(match)

    return matches

    def _deduplicate_matches()
    self, matches: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""

    seen = set()
    unique_matches = []

        for match in matches:
    demonstrative = match['demonstrative']
            if demonstrative not in seen:
    seen.add(demonstrative)
    unique_matches.append(match)

    return unique_matches

    def _select_best_match(self, matches: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚"""

        if not matches:
    return {}

        # Ø§Ù„ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø«Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
    matches.sort()
    key=lambda x: (x['confidence'], x['frequency_score']), reverse=True
    )

    return matches[0]

    def get_statistics(self) -> Dict[str, Any]:
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""

    stats = {
    'total_demonstratives': len()
    self.demonstrative_pronouns_db.demonstrative_pronouns
    ),
    'categories': {},
    'distances': {},
    'numbers': {},
    'genders': {},
    'syllable_patterns': len(self.demonstrative_pronouns_db.syllable_patterns),
    }

        for demonstrative in self.demonstrative_pronouns_db.demonstrative_pronouns:
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª
    cat = demonstrative.category.value
    stats['categories'][cat] = stats['categories'].get(cat, 0) + 1

            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    dist = demonstrative.distance
    stats['distances'][dist] = stats['distances'].get(dist, 0) + 1

            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯
    num = demonstrative.number
    stats['numbers'][num] = stats['numbers'].get(num, 0) + 1

            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¬Ù†Ø§Ø³
    gen = demonstrative.gender
    stats['genders'][gen] = stats['genders'].get(gen, 0) + 1

    return stats

    def save_database(self, output_path: str = "arabic_demonstrative_pronouns_database.json"):  # type: ignore[no-untyped def]
    """Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""

    data = {
    'demonstrative_pronouns': [
    asdict(d) for d in self.demonstrative_pronouns_db.demonstrative_pronouns
    ],
    'syllable_patterns': self.demonstrative_pronouns_db.syllable_patterns,
    'statistics': self.get_statistics(),
    }

        with open(output_path, 'w', encoding='utf 8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2, default=str)

    logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© ÙÙŠ: {output_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION AND TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():  # type: ignore[no-untyped def]
    """ØªØ´ØºÙŠÙ„ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…"""

    print("ğŸ¯ Ù…ÙˆÙ„Ø¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©")
    print("=" * 50)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯
    generator = ArabicDemonstrativePronounsGenerator()

    # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    test_cases = [
        # Ù„Ù„Ù‚Ø±ÙŠØ¨
    ["Ù‡ÙØ§", "Ø°ÙØ§"],  # Ù‡Ø°Ø§
    ["Ù‡ÙØ§", "Ø°ÙÙ‡Ù"],  # Ù‡Ø°Ù‡
    ["Ù‡ÙØ§", "Ø°ÙØ§", "Ù†Ù"],  # Ù‡Ø°Ø§Ù†
    ["Ù‡ÙØ§", "ØªÙØ§", "Ù†Ù"],  # Ù‡Ø§ØªØ§Ù†
    ["Ù‡ÙØ§", "Ø¤Ù", "Ù„ÙØ§", "Ø¡Ù"],  # Ù‡Ø¤Ù„Ø§Ø¡
        # Ù„Ù„Ø¨Ø¹ÙŠØ¯
    ["Ø°ÙØ§", "Ù„ÙÙƒÙ"],  # Ø°Ù„Ùƒ
    ["ØªÙÙ„", "ÙƒÙ"],  # ØªÙ„Ùƒ
    ["Ø£ÙÙˆ", "Ù„ÙØ§", "Ø¦Ù", "ÙƒÙ"],  # Ø£ÙˆÙ„Ø¦Ùƒ
        # Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©
    ["Ù‡Ù", "Ù†ÙØ§"],  # Ù‡Ù†Ø§
    ["Ù‡Ù", "Ù†ÙØ§", "ÙƒÙ"],  # Ù‡Ù†Ø§Ùƒ
        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø®Ø§Ø·Ø¦Ø©
    ["Ø¨ÙØ§", "Ø±ÙØ¯"],  # ØºÙŠØ± ØµØ­ÙŠØ­
    ["ÙƒÙ", "ØªÙØ§", "Ø¨"],  # ØºÙŠØ± ØµØ­ÙŠØ­
    ]

    print(f"\nğŸ”¬ ØªØ´ØºÙŠÙ„ {len(test_cases)} Ø§Ø®ØªØ¨Ø§Ø±:")
    print(" " * 40)

    successful = 0

    for i, syllables in enumerate(test_cases, 1):
    print(f"\n{i}. Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {syllables}")

    result = generator.generate_demonstrative_pronouns_from_syllables(syllables)

        if result['success']:
    best = result['best_match']
    print(f"   âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©: {best['demonstrative']}")
    print(f"   ğŸ“Š Ø§Ù„ÙØ¦Ø©: {best['category']}")
    print()
    f"   ğŸ“ Ø§Ù„Ù…Ø³Ø§ÙØ©: {best['distance']} | Ø§Ù„Ù†ÙˆØ¹: {best['gender']} | Ø§Ù„Ø¹Ø¯Ø¯: {best['number']}"
    )  # noqa: E501
    print(f"   ğŸ¯ Ø§Ù„Ø«Ù‚Ø©: {best['confidence']:.2f}")
    successful += 1
        else:
    print(f"   âŒ ÙØ´Ù„: {result['error']}")

    print("\nğŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print()
    f"   Ø§Ù„Ù†Ø§Ø¬Ø­: {successful}/{len(test_cases)} ({successful/len(test_cases)*100:.1f%)}"
    )  # noqa: E501

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    stats = generator.get_statistics()
    print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:")
    print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¥Ø´Ø§Ø±Ø©: {stats['total_demonstratives']}")
    print(f"   Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©: {stats['syllable_patterns']}")
    print(f"   Ø§Ù„ÙØ¦Ø§Øª: {len(stats['categories'])}")

    # Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    generator.save_database()

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ!")


if __name__ == "__main__":
    main()

