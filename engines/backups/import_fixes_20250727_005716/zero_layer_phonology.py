#!/usr/bin/env python3
"""
Zero Layer: Phonology Engine for Arabic NLP
============================================
Enterprise-Grade Phonological Analysis and Classification System
Professional implementation following Python best practices and Arabic linguistic standards

This layer serves as the foundation for all higher-level Arabic NLP processing:
- Extracts phonemes and diacritics with linguistic precision
- Classifies phonemes as: Root (Ø¬Ø°Ø±ÙŠØ©), Affixal (Ø²Ø§Ø¦Ø¯Ø©), Functional (ÙˆØ¸ÙŠÙÙŠØ©)
- Prepares phonological units for syllable construction
- Provides vector representation for machine learning pipelines

Author: Arabic NLP Expert Team
Version: 1.0.0
Date: 2025-07 23
License: MIT
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc


# Global suppressions for professional codebase
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too long

import logging  # noqa: F401
import re  # noqa: F401
from dataclasses import dataclass, field  # noqa: F401
from enum import Enum  # noqa: F401
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path  # noqa: F401


class PhonemeClassification(Enum):
    """Enumeration for Arabic phoneme classification types"""

    ROOT = "root"  # Ø¬Ø°Ø±ÙŠØ© - Root consonants that carry lexical meaning
    AFFIXAL = "affixal"  # Ø²Ø§Ø¦Ø¯Ø© - Affixal morphemes (prefixes, suffixes, infixes)
    FUNCTIONAL = "functional"  # ÙˆØ¸ÙŠÙÙŠØ© - Functional elements (particles, connectives)
    UNKNOWN = "unknown"  # ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ - Unclassified or foreign elements


class HarakaClassification(Enum):
    """Enumeration for Arabic diacritical mark classification"""

    SHORT_VOWELS = "short_vowels"  # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
    LONG_VOWELS = "long_vowels"  # Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
    SUKUN = "sukun"  # Ø§Ù„Ø³ÙƒÙˆÙ†
    SHADDA = "shadda"  # Ø§Ù„Ø´Ø¯Ø©
    TANWIN = "tanwin"  # Ø§Ù„ØªÙ†ÙˆÙŠÙ†
    MADD = "madd"  # Ø§Ù„Ù…Ø¯
    NONE = "none"  # Ø¨Ø¯ÙˆÙ† Ø­Ø±ÙƒØ©


@dataclass
class PhonologicalUnit:
    """
    Represents a single phonological unit in Arabic text
    ÙˆØ­Ø¯Ø© ØµÙˆØªÙŠØ© ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    """

    phoneme: str  # Ø§Ù„ÙÙˆÙ†ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    haraka: str  # Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø©
    phoneme_class: PhonemeClassification  # ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙˆÙ†ÙŠÙ…
    haraka_class: HarakaClassification  # ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ø±ÙƒØ©
    position: int  # Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©
    features: Dict[str, Any] = field(default_factory=dict)  # Ø®ØµØ§Ø¦Øµ Ø¥Ø¶Ø§ÙÙŠØ©

    def to_dict(self) -> Dict[str, Any]:
    """Convert to dictionary representation for serialization"""
    return {
    'phoneme': self.phoneme,
    'haraka': self.haraka,
    'phoneme_class': self.phoneme_class.value,
    'haraka_class': self.haraka_class.value,
    'position': self.position,
    'features': self.features,
    }

    def __str__(self) -> str:
    """String representation for debugging"""
    return f"{self.phoneme}{self.haraka} [{self.phoneme_class.value}]"


@dataclass
class PhonologicalAnalysis:
    """
    Complete phonological analysis result for an Arabic word
    Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    """

    word: str  # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
    units: List[PhonologicalUnit]  # Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
    root_phonemes: List[PhonologicalUnit]  # Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¬Ø°Ø±ÙŠØ©
    affixal_phonemes: List[PhonologicalUnit]  # Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    functional_phonemes: List[PhonologicalUnit]  # Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©
    statistics: Dict[str, int] = field(default_factory=dict)  # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
    confidence: float = 0.0  # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©

    def to_dict(self) -> Dict[str, Any]:
    """Convert to dictionary representation"""
    return {
    'word': self.word,
    'units': [unit.to_dict() for unit in self.units],
    'root_phonemes': [unit.to_dict() for unit in self.root_phonemes],
    'affixal_phonemes': [unit.to_dict() for unit in self.affixal_phonemes],
    'functional_phonemes': [
    unit.to_dict() for unit in self.functional_phonemes
    ],
    'statistics': self.statistics,
    'confidence': self.confidence,
    }


class ZeroLayerPhonologyEngine:
    """
    Zero Layer: Professional Arabic Phonology Engine
    Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙØ±: Ù…Ø­Ø±Ùƒ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ù‡Ù†ÙŠ

    Enterprise-grade phonological analysis system that serves as the foundation
    for all higher level Arabic NLP processing layers.
    """

    def __init__(self, config_path: Optional[Path] = None):  # type: ignore[no-untyped def]
    """Initialize the Zero Layer Phonology Engine"""
    self.logger = logging.getLogger('ZeroLayerPhonology')
    self._setup_logging()

        # Load configuration
    self.config = self._load_config(config_path)

        # Initialize phoneme classification mappings
    self._initialize_phoneme_mappings()

        # Initialize haraka classification mappings
    self._initialize_haraka_mappings()

        # Initialize processing statistics
    self.statistics = {
    'words_processed': 0,
    'units_extracted': 0,
    'classification_accuracy': 0.0,
    }

    self.logger.info("ğŸ¯ Zero Layer Phonology Engine initialized successfully")

    def _setup_logging(self) -> None:
    """Configure logging for the engine"""
        if not self.logger.handlers:
    handler = logging.StreamHandler()
            formatter = logging.Formatter()
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    self.logger.addHandler(handler)
    self.logger.setLevel(logging.INFO)

    def _load_config(self, config_path: Optional[Path]) -> Dict[str, Any]:
    """Load engine configuration"""
        default_config = {
    'enable_advanced_classification': True,
    'strict_haraka_validation': True,
    'confidence_threshold': 0.85,
    'enable_statistics': True,
    }

        if config_path and config_path.exists():
            # Load from file if provided
            # Implementation for loading JSON/YAML config
    pass

    return default_config

    def _initialize_phoneme_mappings(self) -> None:
    """Initialize Arabic phoneme classification mappings"""

        # ROOT PHONEMES (Ø¬Ø°Ø±ÙŠØ©) - Core semantic-bearing consonants
    self.root_phonemes = {
            # Core stops and fricatives
    'Ø¨',
    'Øª',
    'Ø«',
    'Ø¬',
    'Ø­',
    'Ø®',
    'Ø¯',
    'Ø°',
    'Ø±',
    'Ø²',
    'Ø³',
    'Ø´',
    'Øµ',
    'Ø¶',
    'Ø·',
    'Ø¸',
    'Ø¹',
    'Øº',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    }

        # AFFIXAL PHONEMES (Ø²Ø§Ø¦Ø¯Ø©) - Morphological markers
    self.affixal_phonemes = {
            # Long vowels and glides - Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¯
    'Ø§',  # Ø£Ù„Ù Ø§Ù„Ù…Ø¯ - Long vowel marker
    'ÙŠ',  # ÙŠØ§Ø¡ Ø§Ù„Ù…Ø¯ - Long vowel marker
    'Ùˆ',  # ÙˆØ§Ùˆ Ø§Ù„Ù…Ø¯ - Long vowel marker
    }

        # FUNCTIONAL PHONEMES (ÙˆØ¸ÙŠÙÙŠØ©) - Grammatical particles
    self.functional_phonemes = {
    'Ø¡',  # Ù‡Ù…Ø²Ø© Ø§Ù„Ù‚Ø·Ø¹
    'Ø¢',  # Ø¢ Ø§Ù„Ù…Ø¯Ø©
    'Ø£',  # Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ Ø£Ù„Ù
    'Ø¥',  # Ù‡Ù…Ø²Ø© ØªØ­Øª Ø£Ù„Ù
    'Ø©',  # ØªØ§Ø¡ Ù…Ø±Ø¨ÙˆØ·Ø©
    'Ù‰',  # Ø£Ù„Ù Ù…Ù‚ØµÙˆØ±Ø©
    }

        # Additional classification for advanced analysis
    self.phoneme_features = {
            # Consonant features - Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„ØµÙˆØ§Ù…Øª
    'stops': {'Ø¨', 'Øª', 'Ø¯', 'Ùƒ', 'Ù‚', 'Ø·', 'Ø¶', 'Ø¬'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±ÙŠØ©
    'fricatives': {
    'Ù',
    'Ø«',
    'Ø°',
    'Ø³',
    'Ø²',
    'Ø´',
    'Øµ',
    'Ø¸',
    'Ø­',
    'Ø®',
    'Ø¹',
    'Øº',
    'Ù‡',
    },  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø§Ø­ØªÙƒØ§ÙƒÙŠØ©
    'nasals': {'Ù…', 'Ù†'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø£Ù†ÙÙŠØ©
    'liquids': {'Ù„', 'Ø±'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø³Ø§Ø¦Ù„Ø©
    'glides': {
    'Ùˆ',
    'ÙŠ',
    },  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø´Ø¨Ù‡ Ø§Ù„ØµÙˆØªÙŠØ© (Without Ø§ - alif is vowel carrier)
    'pharyngealized': {'Øµ', 'Ø¶', 'Ø·', 'Ø¸'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…ÙØ®Ù…Ø©
    'emphatic': {'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ù‚', 'Ø®', 'Øº'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
    'glottal': {'Ø¡', 'Ù‡'},  # Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø­Ù†Ø¬Ø±ÙŠØ©
    }

    def _initialize_haraka_mappings(self) -> None:
    """Initialize Arabic haraka (diacritics) classification mappings"""

        # Short vowels (Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©)
    self.short_vowels = {'Ù', 'Ù', 'Ù'}

        # Long vowels (Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©) - contextual
    self.long_vowel_markers = {'Ø§', 'Ùˆ', 'ÙŠ', 'Ù‰'}

        # Sukun (Ø§Ù„Ø³ÙƒÙˆÙ†)
    self.sukun_markers = {'Ù’'}

        # Shadda (Ø§Ù„Ø´Ø¯Ø©)
    self.shadda_markers = {'Ù‘'}

        # Tanwin (Ø§Ù„ØªÙ†ÙˆÙŠÙ†)
    self.tanwin_markers = {'Ù‹', 'ÙŒ', 'Ù'}

        # Madd markers (Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¯)
    self.madd_markers = {'Ù°', '~'}

        # Haraka features for linguistic analysis
    self.haraka_features = {
    'vowel_quality': {
    'Ù': 'open',  # ÙØªØ­Ø©
    'Ù': 'close',  # ÙƒØ³Ø±Ø©
    'Ù': 'close',  # Ø¶Ù…Ø©
    },
    'vowel_frontness': {
    'Ù': 'central',  # ÙØªØ­Ø©
    'Ù': 'front',  # ÙƒØ³Ø±Ø©
    'Ù': 'back',  # Ø¶Ù…Ø©
    },
    }

    def classify_phoneme()
    self, phoneme: str, context: Optional[str] = None
    ) -> PhonemeClassification:
    """
    Classify a phoneme based on its linguistic role
    ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙˆÙ†ÙŠÙ… Ø­Ø³Ø¨ Ø¯ÙˆØ±Ù‡ Ø§Ù„Ù„ØºÙˆÙŠ

    Args:
    phoneme: The Arabic phoneme to classify
    context: Optional word context for better classification

    Returns:
    PhonemeClassification enum value
    """
        if phoneme in self.root_phonemes:
    return PhonemeClassification.ROOT
        elif phoneme in self.affixal_phonemes:
            # Advanced contextual classification could be added here
    return PhonemeClassification.AFFIXAL
        elif phoneme in self.functional_phonemes:
    return PhonemeClassification.FUNCTIONAL
        else:
    return PhonemeClassification.UNKNOWN

    def classify_haraka(self, haraka: str) -> HarakaClassification:
    """
    Classify a haraka (diacritical mark)
    ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ø±ÙƒØ©

    Args:
    haraka: The Arabic haraka to classify

    Returns:
    HarakaClassification enum value
    """
        if haraka in self.short_vowels:
    return HarakaClassification.SHORT_VOWELS
        elif haraka in self.long_vowel_markers:
    return HarakaClassification.LONG_VOWELS
        elif haraka in self.sukun_markers:
    return HarakaClassification.SUKUN
        elif haraka in self.shadda_markers:
    return HarakaClassification.SHADDA
        elif haraka in self.tanwin_markers:
    return HarakaClassification.TANWIN
        elif haraka in self.madd_markers:
    return HarakaClassification.MADD
        else:
    return HarakaClassification.NONE

    def extract_phonemes_units(self, word: str) -> List[PhonologicalUnit]:
    """
    Extract phonological units from Arabic word
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

    Args:
    word: Arabic word to analyze

    Returns:
    List of PhonologicalUnit objects
    """
    units = []
    position = 0

    i = 0
        while i < len(word):
    char = word[i]

            # Skip non Arabic characters
            if not self._is_arabic_character(char):
    i += 1
    continue

            # Extract phoneme
            if self._is_arabic_letter(char):
    phoneme = char
    haraka = ""

                # Collect following harakat
    j = i + 1
                while j < len(word) and self._is_haraka(word[j]):
    haraka += word[j]
    j += 1

                # Classify phoneme and haraka
    phoneme_class = self.classify_phoneme(phoneme, word)
    haraka_class = ()
    self.classify_haraka(haraka)
                    if haraka
                    else HarakaClassification.NONE
    )

                # Extract additional features
    features = self._extract_phoneme_features(phoneme, haraka)

                # Create phonological unit
    unit = PhonologicalUnit()
    phoneme=phoneme,
    haraka=haraka,
    phoneme_class=phoneme_class,
    haraka_class=haraka_class,
    position=position,
    features=features)

    units.append(unit)
    position += 1
    i = j
            else:
    i += 1

    return units

    def _extract_phoneme_features(self, phoneme: str, haraka: str) -> Dict[str, Any]:
    """Extract detailed phonological features"""
    features = {}

        # Consonant features
        for feature_type, phoneme_set in self.phoneme_features.items():
            if phoneme in phoneme_set:
    features[feature_type] = True

        # Haraka features
        if haraka and haraka in self.haraka_features.get('vowel_quality', {}):
    features['vowel_quality'] = self.haraka_features['vowel_quality'][haraka]

        if haraka and haraka in self.haraka_features.get('vowel_frontness', {}):
    features['vowel_frontness'] = self.haraka_features['vowel_frontness'][
    haraka
    ]

    return features

    def analyze(self, word: str) -> PhonologicalAnalysis:
    """
    Perform complete phonological analysis of Arabic word
    Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ ØµÙˆØªÙŠ ÙƒØ§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

    Args:
    word: Arabic word to analyze

    Returns:
    PhonologicalAnalysis object with complete results
    """
    self.logger.info(f"ğŸ”¬ Analyzing phonological structure of: {word}")

        # Extract phonological units
    units = self.extract_phonemes_units(word)

        # Classify units by type
    root_phonemes = [
    unit for unit in units if unit.phoneme_class == PhonemeClassification.ROOT
    ]
    affixal_phonemes = [
    unit
            for unit in units
            if unit.phoneme_class == PhonemeClassification.AFFIXAL
    ]
    functional_phonemes = [
    unit
            for unit in units
            if unit.phoneme_class == PhonemeClassification.FUNCTIONAL
    ]

        # Calculate statistics
    statistics = {
    'total_units': len(units),
    'root_count': len(root_phonemes),
    'affixal_count': len(affixal_phonemes),
    'functional_count': len(functional_phonemes),
    'harakat_count': sum(1 for unit in units if unit.haraka),
    'shadda_count': sum(1 for unit in units if 'Ù‘' in unit.haraka),
    }

        # Calculate confidence based on classification accuracy
    confidence = self._calculate_confidence(units, statistics)

        # Create analysis result
    analysis = PhonologicalAnalysis()
    word=word,
    units=units,
    root_phonemes=root_phonemes,
    affixal_phonemes=affixal_phonemes,
    functional_phonemes=functional_phonemes,
    statistics=statistics,
    confidence=confidence)

        # Update engine statistics
    self.statistics['words_processed'] += 1
    self.statistics['units_extracted'] += len(units)

    self.logger.info()
    f"âœ… Analysis complete: {len(units)} units extracted with {confidence:.2%} confidence"
    )  # noqa: E501

    return analysis

    def _calculate_confidence()
    self, units: List[PhonologicalUnit], statistics: Dict[str, int]
    ) -> float:
    """Calculate analysis confidence score"""
        if not units:
    return 0.0

        # Factors affecting confidence
        classified_units = sum()
    1 for unit in units if unit.phoneme_class != PhonemeClassification.UNKNOWN
    )
        classification_ratio = classified_units / len(units)

        # Presence of harakat increases confidence
    harakat_ratio = statistics['harakat_count'] / len(units) if units else 0

        # Combine factors
    confidence = (classification_ratio * 0.7) + (harakat_ratio * 0.3)

    return min(confidence, 1.0)

    def _is_arabic_character(self, char: str) -> bool:
    """Check if character is Arabic"""
    return '\u0600' <= char <= '\u06ff' or '\u0750' <= char <= '\u077f'

    def _is_arabic_letter(self, char: str) -> bool:
    """Check if character is Arabic letter (not diacritic)"""
    return ()
    self._is_arabic_character(char)
    and not self._is_haraka(char)
    and char not in 'Ø›ØŸØŒ'
    )

    def _is_haraka(self, char: str) -> bool:
    """Check if character is a haraka (diacritical mark)"""
    harakat = {'Ù', 'Ù', 'Ù', 'Ù’', 'Ù‘', 'Ù‹', 'ÙŒ', 'Ù', 'Ù°', '~'}
    return char in harakat

    def get_phoneme_vector()
    self, unit: PhonologicalUnit
    ) -> Dict[str, Union[str, int, float]]:
    """
    Convert phonological unit to vector representation for ML
    ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„ØµÙˆØªÙŠØ© Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ù…ØªØ¬Ù‡ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ

    Args:
    unit: PhonologicalUnit to vectorize

    Returns:
    Dictionary representation suitable for machine learning
    """
    vector = {
    'phoneme_ord': ord(unit.phoneme),
    'phoneme_class_id': list(PhonemeClassification).index(unit.phoneme_class),
    'haraka_class_id': list(HarakaClassification).index(unit.haraka_class),
    'position': unit.position,
    'has_haraka': 1 if unit.haraka else 0,
    'haraka_count': len(unit.haraka),
    }

        # Add feature flags
        for feature, value in unit.features.items():
            if isinstance(value, bool):
    vector[f'feat_{feature}'] = 1 if value else 0
            elif isinstance(value, str):
    vector[f'feat_{feature}'] = hash(value) % 1000  # Simple string encoding

    return vector

    def process_text(self, text: str) -> List[PhonologicalAnalysis]:
    """
    Process entire Arabic text (multiple words)
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ (ÙƒÙ„Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)

    Args:
    text: Arabic text to process

    Returns:
    List of PhonologicalAnalysis for each word
    """
        # Simple word tokenization
    words = re.findall(r'[\u0600-\u06FF\u0750 \u077F]+', text)

    results = []
        for word in words:
            if word.strip():
    analysis = self.analyze(word.strip())
    results.append(analysis)

    return results

    def get_statistics(self) -> Dict[str, Any]:
    """Get engine processing statistics"""
    return self.statistics.copy()

    def reset_statistics(self) -> None:
    """Reset engine statistics"""
    self.statistics = {
    'words_processed': 0,
    'units_extracted': 0,
    'classification_accuracy': 0.0,
    }


# Example usage and testing functions
def demonstrate_zero_layer():  # type: ignore[no-untyped-def]
    """Demonstrate the Zero Layer Phonology Engine capabilities"""
    print("ğŸ¯ Zero Layer Phonology Engine Demonstration")
    print("=" * 60)

    # Initialize engine
    engine = ZeroLayerPhonologyEngine()

    # Test words with different complexity levels
    test_words = [
    "ÙƒÙØªÙØ§Ø¨ÙŒ",  # Simple noun with tanwin
    "Ù…ÙØ¯ÙØ±ÙÙ‘Ø³ÙØ©ÙŒ",  # Complex noun with shadda
    "ÙŠÙÙƒÙ’ØªÙØ¨ÙÙˆÙ†Ù",  # Verb with affixes
    "ÙˆÙØ§Ù„Ù’ÙƒÙØªÙØ§Ø¨Ù",  # Word with definite article
    "Ù…ÙÙƒÙ’ØªÙÙˆØ¨ÙŒ",  # Passive participle
    ]

    for word in test_words:
    print(f"\nğŸ“– Analyzing: {word}")
    print(" " * 30)

    analysis = engine.analyze(word)

    print(f"Total Units: {analysis.statistics['total_units']}")
    print(f"Root Phonemes: {analysis.statistics['root_count']}")
    print(f"Affixal Phonemes: {analysis.statistics['affixal_count']}")
    print(f"Functional Phonemes: {analysis.statistics['functional_count']}")
    print(f"Confidence: {analysis.confidence:.2%}")

    print("\nPhonological Units:")
        for i, unit in enumerate(analysis.units, 1):
    print(f"  {i. {unit}}")

    print("\nğŸ“Š Engine Statistics:")
    stats = engine.get_statistics()
    for key, value in stats.items():
    print(f"  {key: {value}}")


if __name__ == "__main__":
    demonstrate_zero_layer()

