#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Arabic Verb and Source Syllable Pattern Generator
==============================================================
Ù…ÙˆÙ„Ø¯ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

ÙŠØºØ·ÙŠ:
- Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ù…Ø¬Ø±Ø¯Ø© (Ø«Ù„Ø§Ø«ÙŠØ© ÙˆØ±Ø¨Ø§Ø¹ÙŠØ©)
- Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ù…Ø²ÙŠØ¯Ø© (Ù‚ÙŠØ§Ø³ÙŠØ© ÙˆØºÙŠØ± Ù‚ÙŠØ§Ø³ÙŠØ©)
- Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ© ÙˆØ§Ù„Ø³Ù…Ø§Ø¹ÙŠØ©
- Ø§Ù„Ø¸ÙˆØ§Ù‡Ø± Ø§Ù„ÙÙˆÙ†ÙˆÙ„ÙˆØ¬ÙŠØ© (Ø¥Ø¯ØºØ§Ù…ØŒ Ø¥Ø¹Ù„Ø§Ù„ØŒ Ø¥Ø¨Ø¯Ø§Ù„)
- Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© (CV, CVC, CVCC, CVV, CVVC, CVVCV)

Author: GitHub Copilot Arabic NLP Expert
Version: 3.0.0 - COMPREHENSIVE VERB SYSTEM
Date: 2025-07-26
Encoding: UTF 8
"""

from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum
import re
import json
from collections import Counter
import logging

# Configure logging
logging.basicConfig()
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC VERB MORPHOLOGY SYSTEM - Ù†Ø¸Ø§Ù… ØµØ±Ù Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class VerbType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    TRILATERAL_SIMPLE = "trilateral_simple"  # Ø«Ù„Ø§Ø«ÙŠ Ù…Ø¬Ø±Ø¯
    QUADRILATERAL_SIMPLE = "quadrilateral_simple"  # Ø±Ø¨Ø§Ø¹ÙŠ Ù…Ø¬Ø±Ø¯
    TRILATERAL_AUGMENTED = "trilateral_augmented"  # Ø«Ù„Ø§Ø«ÙŠ Ù…Ø²ÙŠØ¯
    QUADRILATERAL_AUGMENTED = "quadrilateral_augmented"  # Ø±Ø¨Ø§Ø¹ÙŠ Ù…Ø²ÙŠØ¯


class SyllableType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""

    V = "V"  # ØµØ§Ø¦Øª Ù…Ù†ÙØ±Ø¯
    CV = "CV"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª
    CVC = "CVC"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª + ØµØ§Ù…Øª
    CVV = "CVV"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª Ø·ÙˆÙŠÙ„
    CVVC = "CVVC"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª Ø·ÙˆÙŠÙ„ + ØµØ§Ù…Øª
    CVCC = "CVCC"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª + ØµØ§Ù…ØªØ§Ù†
    CCV = "CCV"  # ØµØ§Ù…ØªØ§Ù† + ØµØ§Ø¦Øª (Ù†Ø§Ø¯Ø± ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
    CVCCC = "CVCCC"  # ØµØ§Ù…Øª + ØµØ§Ø¦Øª + Ø«Ù„Ø§Ø«Ø© ØµÙˆØ§Ù…Øª (ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø©)
    CVVCV = "CVVCV"  # Ù†Ù…Ø· Ù…Ø±ÙƒØ¨
    CVVCVC = "CVVCVC"  # Ù†Ù…Ø· Ù…Ø±ÙƒØ¨


@dataclass
class VerbForm:
    """ØµÙŠØºØ© Ø§Ù„ÙØ¹Ù„"""

    form_number: str  # Ø±Ù‚Ù… Ø§Ù„ØµÙŠØºØ© (I, II, III, ...)
    form_name: str  # Ø§Ø³Ù… Ø§Ù„ØµÙŠØºØ©
    pattern: str  # Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ
    meaning: str  # Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¹Ø§Ù…
    syllable_pattern: List[str]  # Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    morphemes: List[str]  # Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª


@dataclass
class SourcePattern:
    """Ù†Ù…Ø· Ø§Ù„Ù…ØµØ¯Ø±"""

    source_word: str  # Ø§Ù„Ù…ØµØ¯Ø±
    verb_form: str  # ØµÙŠØºØ© Ø§Ù„ÙØ¹Ù„
    syllable_pattern: str  # Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    is_standard: bool  # Ù‚ÙŠØ§Ø³ÙŠ Ø£Ù… Ø³Ù…Ø§Ø¹ÙŠ
    phonological_features: List[str]  # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©


class ArabicVerbMorphologySystem:
    """Ù†Ø¸Ø§Ù… ØµØ±Ù Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„"""

    def __init__(self):

        # Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    self.root_consonants = [
    'Ùƒ',
    'Øª',
    'Ø¨',
    'Ø³',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    'Ø±',
    'Ø¬',
    'Ø¯',
    'Ø¹',
    ]
    self.all_consonants = [
    'Ø¡',
    'Ø¨',
    'Øª',
    'Ø«',
    'Ø¬',
    'Ø­',
    'Ø®',
    'Ø¯',
    'Ø°',
    'Ø±',
    'Ø²',
    'Ø³',
    'Ø´',
    'Øµ',
    'Ø¶',
    'Ø·',
    'Ø¸',
    'Ø¹',
    'Øº',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    'Ùˆ',
    'ÙŠ',
    ]

        # Ø§Ù„ØµÙˆØ§Ø¦Øª ÙˆØ§Ù„Ø­Ø±ÙƒØ§Øª
    self.short_vowels = ['Ù', 'Ù', 'Ù']
    self.long_vowels = ['Ø§', 'ÙŠ', 'Ùˆ']
    self.diacritics = ['Ù‹', 'ÙŒ', 'Ù', 'Ù’', 'Ù‘', 'Ù°']

        # ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
    self.verb_forms = self._load_verb_forms()
    self.source_patterns = self._load_source_patterns()
    self.phonological_rules = self._load_phonological_rules()

    logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… ØµØ±Ù Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„")

    def _load_verb_forms(self) -> Dict[str, VerbForm]:
    """ØªØ­Ù…ÙŠÙ„ ØµÙŠØº Ø§Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    return {
            # Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…Ø¬Ø±Ø¯
    "I": VerbForm()
                form_number="I",
                form_name="Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…Ø¬Ø±Ø¯",
    pattern="ÙÙØ¹ÙÙ„Ù",
    meaning="basic_action",
    syllable_pattern=["CV", "CV", "CV"],  # ÙÙ-Ø¹Ù Ù„Ù
    morphemes=["Ù", "Ø¹", "Ù„"]),
            # Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…Ø²ÙŠØ¯
    "II": VerbForm()
                form_number="II",
                form_name="ÙÙØ¹ÙÙ‘Ù„Ù",
    pattern="ÙÙØ¹ÙÙ‘Ù„Ù",
    meaning="intensive/causative",
    syllable_pattern=["CV", "CVC", "CV"],  # ÙÙ-Ø¹ÙÙ‘ Ù„Ù
    morphemes=["Ù", "Ø¹Ù‘", "Ù„"]),
    "III": VerbForm()
                form_number="III",
                form_name="ÙÙØ§Ø¹ÙÙ„Ù",
    pattern="ÙÙØ§Ø¹ÙÙ„Ù",
    meaning="reciprocal/attempt",
    syllable_pattern=["CV", "CV", "CV", "CV"],  # ÙÙØ§-Ø¹Ù Ù„Ù
    morphemes=["Ù", "Ø§", "Ø¹", "Ù„"]),
    "IV": VerbForm()
                form_number="IV",
                form_name="Ø£ÙÙÙ’Ø¹ÙÙ„Ù",
    pattern="Ø£ÙÙÙ’Ø¹ÙÙ„Ù",
    meaning="causative",
    syllable_pattern=["V", "CVC", "CV"],  # Ø£Ù-ÙÙ’Ø¹ Ù„Ù
    morphemes=["Ø£", "ÙØ¹", "Ù„"]),
    "V": VerbForm()
                form_number="V",
                form_name="ØªÙÙÙØ¹ÙÙ‘Ù„Ù",
    pattern="ØªÙÙÙØ¹ÙÙ‘Ù„Ù",
    meaning="reflexive",
    syllable_pattern=["CV", "CV", "CVC", "CV"],  # ØªÙ-ÙÙ-Ø¹ÙÙ‘ Ù„Ù
    morphemes=["Øª", "Ù", "Ø¹Ù‘", "Ù„"]),
    "VI": VerbForm()
                form_number="VI",
                form_name="ØªÙÙÙØ§Ø¹ÙÙ„Ù",
    pattern="ØªÙÙÙØ§Ø¹ÙÙ„Ù",
    meaning="mutual_action",
    syllable_pattern=["CV", "CV", "CV", "CV", "CV"],  # ØªÙ-ÙÙØ§-Ø¹Ù Ù„Ù
    morphemes=["Øª", "Ù", "Ø§", "Ø¹", "Ù„"]),
    "VII": VerbForm()
                form_number="VII",
                form_name="Ø§Ù†Ù’ÙÙØ¹ÙÙ„Ù",
    pattern="Ø§Ù†Ù’ÙÙØ¹ÙÙ„Ù",
    meaning="passive/reflexive",
    syllable_pattern=["V", "CVC", "CV", "CV"],  # Ø§Ù†-ÙÙØ¹ Ù„Ù
    morphemes=["Ø§Ù†", "ÙØ¹", "Ù„"]),
    "VIII": VerbForm()
                form_number="VIII",
                form_name="Ø§ÙÙ’ØªÙØ¹ÙÙ„Ù",
    pattern="Ø§ÙÙ’ØªÙØ¹ÙÙ„Ù",
    meaning="reflexive",
    syllable_pattern=["VC", "CV", "CV", "CV"],  # Ø§Ù-ØªÙØ¹ Ù„Ù
    morphemes=["Ø§Ù", "Øª", "Ø¹", "Ù„"]),
    "IX": VerbForm()
                form_number="IX",
                form_name="Ø§ÙÙ’Ø¹ÙÙ„ÙÙ‘",
    pattern="Ø§ÙÙ’Ø¹ÙÙ„ÙÙ‘",
    meaning="colors/defects",
    syllable_pattern=["VC", "CV", "CVC"],  # Ø§Ù-Ø¹Ù Ù„ÙÙ‘
    morphemes=["Ø§Ù", "Ø¹", "Ù„Ù‘"]),
    "X": VerbForm()
                form_number="X",
                form_name="Ø§Ø³Ù’ØªÙÙÙ’Ø¹ÙÙ„Ù",
    pattern="Ø§Ø³Ù’ØªÙÙÙ’Ø¹ÙÙ„Ù",
    meaning="seeking/requesting",
    syllable_pattern=["VC", "CV", "CVC", "CV"],  # Ø§Ø³-ØªÙ-ÙÙ’Ø¹ Ù„Ù
    morphemes=["Ø§Ø³Øª", "ÙØ¹", "Ù„"]),
            # Ø§Ù„Ø±Ø¨Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¬Ø±Ø¯
    "Q1": VerbForm()
                form_number="Q1",
                form_name="ÙÙØ¹Ù’Ù„ÙÙ„Ù",
    pattern="ÙÙØ¹Ù’Ù„ÙÙ„Ù",
    meaning="quadrilateral_basic",
    syllable_pattern=["CVC", "CV", "CV"],  # ÙÙØ¹Ù’-Ù„Ù Ù„Ù
    morphemes=["ÙØ¹", "Ù„", "Ù„"]),
            # Ø§Ù„Ø±Ø¨Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø²ÙŠØ¯
    "Q2": VerbForm()
                form_number="Q2",
                form_name="ØªÙÙÙØ¹Ù’Ù„ÙÙ„Ù",
    pattern="ØªÙÙÙØ¹Ù’Ù„ÙÙ„Ù",
    meaning="quadrilateral_reflexive",
    syllable_pattern=["CV", "CVC", "CV", "CV"],  # ØªÙ-ÙÙØ¹Ù’-Ù„Ù Ù„Ù
    morphemes=["Øª", "ÙØ¹", "Ù„", "Ù„"]),
    "Q3": VerbForm()
                form_number="Q3",
                form_name="Ø§ÙÙ’Ø¹ÙÙ†Ù’Ù„ÙÙ„Ù",
    pattern="Ø§ÙÙ’Ø¹ÙÙ†Ù’Ù„ÙÙ„Ù",
    meaning="quadrilateral_augmented",
    syllable_pattern=["VC", "CV", "CVC", "CV", "CV"],  # Ø§Ù-Ø¹Ù-Ù†Ù’Ù„ Ù„Ù
    morphemes=["Ø§Ù", "Ø¹", "Ù†Ù„", "Ù„"]),
    }

    def _load_source_patterns(self) -> Dict[str, List[SourcePattern]]:
    """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
    return {
            # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…Ø¬Ø±Ø¯
    "I": [
    SourcePattern("ÙÙØ¹Ù’Ù„", "I", "CVC", True, ["Ù…Ù‚Ø·Ø¹_Ù…ØºÙ„Ù‚"]),
    SourcePattern("ÙÙØ¹ÙÙˆÙ„", "I", "CV CVC", True, ["Ù…Ù‚Ø·Ø¹_Ù…Ø±ÙƒØ¨"]),
    SourcePattern("ÙÙØ¹ÙØ§Ù„", "I", "CV CVC", True, ["ØµØ§Ø¦Øª_Ø·ÙˆÙŠÙ„"]),
    SourcePattern("ÙÙØ¹ÙØ§Ù„ÙØ©", "I", "CV-CV CV", True, ["ØªØ§Ø¡_Ø§Ù„ØªØ£Ù†ÙŠØ«"]),
    SourcePattern("Ù…ÙÙÙ’Ø¹ÙÙ„", "I", "CVC CV", True, ["Ù…ÙŠÙ…_Ø²Ø§Ø¦Ø¯Ø©"]),
    ],
            # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…Ø²ÙŠØ¯
    "II": [
    SourcePattern("ØªÙÙÙ’Ø¹ÙÙŠÙ„", "II", "CVC CVVC", True, ["ØªÙ†Ø´ÙŠØ·", "Ù…Ù‚Ø·Ø¹_Ø·ÙˆÙŠÙ„"]),
    SourcePattern("ØªÙÙÙ’Ø¹ÙÙ„ÙØ©", "II", "CVC-CV CV", True, ["ØªØ§Ø¡_Ø§Ù„ØªØ£Ù†ÙŠØ«"]),
    ],
    "III": [
    SourcePattern("Ù…ÙÙÙØ§Ø¹ÙÙ„ÙØ©", "III", "CV-CV-CV CV", True, ["Ù…Ø´Ø§Ø±ÙƒØ©"]),
    SourcePattern("ÙÙØ¹ÙØ§Ù„", "III", "CV CVC", True, ["Ù…Ø®ØªØµØ±"]),
    ],
    "IV": [
    SourcePattern("Ø¥ÙÙÙ’Ø¹ÙØ§Ù„", "IV", "VC CVC", True, ["Ù‡Ù…Ø²Ø©_ÙˆØµÙ„"]),
    ],
    "V": [
    SourcePattern("ØªÙÙÙØ¹ÙÙ‘Ù„", "V", "CV-CV CVC", True, ["ØªØ´Ø¯ÙŠØ¯"]),
    ],
    "VI": [
    SourcePattern("ØªÙÙÙØ§Ø¹ÙÙ„", "VI", "CV-CV-CV CVC", True, ["ØªØ¨Ø§Ø¯Ù„"]),
    ],
    "VII": [
    SourcePattern("Ø§Ù†Ù’ÙÙØ¹ÙØ§Ù„", "VII", "VC-CV CVC", True, ["Ø§Ù†ÙØ¹Ø§Ù„"]),
    ],
    "VIII": [
    SourcePattern("Ø§ÙÙ’ØªÙØ¹ÙØ§Ù„", "VIII", "VC-CV CVC", True, ["Ø§ÙƒØªØ³Ø§Ø¨"]),
    ],
    "IX": [
    SourcePattern("Ø§ÙÙ’Ø¹ÙÙ„ÙØ§Ù„", "IX", "VC-CV CVC", True, ["Ø£Ù„ÙˆØ§Ù†_ÙˆØ¹ÙŠÙˆØ¨"]),
    ],
    "X": [
    SourcePattern("Ø§Ø³Ù’ØªÙÙÙ’Ø¹ÙØ§Ù„", "X", "VC-CVC CVC", True, ["Ø·Ù„Ø¨"]),
    ],
            # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø±Ø¨Ø§Ø¹ÙŠ
    "Q1": [
    SourcePattern("ÙÙØ¹Ù’Ù„ÙÙ„ÙØ©", "Q1", "CVC-CV CV", True, ["ØªØ§Ø¡_Ø§Ù„ØªØ£Ù†ÙŠØ«"]),
    SourcePattern("ÙÙØ¹Ù’Ù„ÙØ§Ù„", "Q1", "CVC CVC", True, ["Ù…Ù‚Ø·Ø¹_Ù…Ø¶Ø§Ø¹Ù"]),
    ],
    "Q2": [
    SourcePattern("ØªÙÙÙØ¹Ù’Ù„ÙÙ„", "Q2", "CV-CVC CVC", True, ["ØªØ¯Ø±Ø¬"]),
    ],
            # Ù…ØµØ§Ø¯Ø± Ø³Ù…Ø§Ø¹ÙŠØ© (ØºÙŠØ± Ù‚ÙŠØ§Ø³ÙŠØ©)
    "irregular": [
    SourcePattern("Ù…ÙØ¬ÙÙŠØ¡", "I", "CV CVVC", False, ["Ù‡Ù…Ø²Ø©_Ù†Ù‡Ø§Ø¦ÙŠØ©", "ÙŠØ§Ø¡_Ù…Ø¯"]),
    SourcePattern("ÙˆÙØ¶ÙÙˆØ¡", "I", "CV-CV CVC", False, ["ÙˆØ§Ùˆ_Ø¶Ù…Ø©", "Ù‡Ù…Ø²Ø©"]),
    SourcePattern("Ø³ÙØ¤ÙØ§Ù„", "I", "CV-CV CVC", False, ["Ù‡Ù…Ø²Ø©_Ù…ØªÙˆØ³Ø·Ø©"]),
    SourcePattern("Ø¨ÙÙ†ÙØ§Ø¡", "I", "CV-CV CVC", False, ["Ù‡Ù…Ø²Ø©_Ù…Ù…Ø¯ÙˆØ¯Ø©"]),
    ],
    }

    def _load_phonological_rules(self) -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©"""
    return {
    'assimilation': {
                # Ø§Ù„Ø¥Ø¯ØºØ§Ù…
    'consonant_clusters': {
    'ØªØ¯': 'Ø¯Ø¯',  # Ø§Ù„ØªØ§Ø¡ ØªÙØ¯ØºÙ… ÙÙŠ Ø§Ù„Ø¯Ø§Ù„
    'ØªØ²': 'Ø²Ø²',  # Ø§Ù„ØªØ§Ø¡ ØªÙØ¯ØºÙ… ÙÙŠ Ø§Ù„Ø²Ø§ÙŠ
    'Ù†Ø¨': 'Ù…Ø¨',  # Ø§Ù„Ù†ÙˆÙ† ØªÙ‚Ù„Ø¨ Ù…ÙŠÙ…Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø§Ø¡
    'Ù†Ù…': 'Ù…Ù…',  # Ø§Ù„Ù†ÙˆÙ† ØªÙØ¯ØºÙ… ÙÙŠ Ø§Ù„Ù…ÙŠÙ…
    },
    'vowel_harmony': {
    'ÙÙˆ': 'ÙÙˆ',  # Ø§Ù„Ø¶Ù…Ø© Ù…Ø¹ Ø§Ù„ÙˆØ§Ùˆ
    'ÙÙŠ': 'ÙÙŠ',  # Ø§Ù„ÙƒØ³Ø±Ø© Ù…Ø¹ Ø§Ù„ÙŠØ§Ø¡
    'ÙØ§': 'ÙØ§',  # Ø§Ù„ÙØªØ­Ø© Ù…Ø¹ Ø§Ù„Ø£Ù„Ù
    },
    },
    'weakening': {
                # Ø§Ù„Ø¥Ø¹Ù„Ø§Ù„
    'waw_alif': {
    'Ù‚ÙˆÙ„': 'Ù‚Ø§Ù„',  # Ù‚ÙˆÙÙ„  > Ù‚Ø§Ù„
    'Ù†ÙˆÙ…': 'Ù†Ø§Ù…',  # Ù†ÙˆÙÙ…  > Ù†Ø§Ù…
    },
    'ya_alif': {
    'Ø¨ÙŠØ¹': 'Ø¨Ø§Ø¹',  # Ø¨ÙŠÙØ¹  > Ø¨Ø§Ø¹
    'Ø³ÙŠØ±': 'Ø³Ø§Ø±',  # Ø³ÙŠÙØ±  > Ø³Ø§Ø±
    },
    },
    'epenthesis': {
                # Ø§Ù„Ø¥Ø´Ø¨Ø§Ø¹ ÙˆÙƒØ³Ø± Ø§Ù„ØªÙ‚Ø§Ø¡ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†
    'consonant_clusters': {
    'CC': 'CiC',  # Ø¥Ø¯Ø®Ø§Ù„ ÙƒØ³Ø±Ø© Ø¨ÙŠÙ† Ø³Ø§ÙƒÙ†ÙŠÙ†
    },
    'word_initial': {
    'CC': 'iCC',  # Ù‡Ù…Ø²Ø© ÙˆØµÙ„ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    },
    },
    'metathesis': {
                # Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ
    'specific_contexts': {
    'Ø§ØµØ·Ø¨Ø±': 'Ø§ØµØ¨Ø±',  # Ù‚Ù„Ø¨ Ø§Ù„ØªØ§Ø¡ ÙˆØ§Ù„ØµØ§Ø¯
    'Ø§Ø¯Ù‘Ø§Ø±Ùƒ': 'Ø§Ø¯Ø±Ùƒ',  # Ù‚Ù„Ø¨ Ø§Ù„ØªØ§Ø¡ ÙˆØ§Ù„Ø¯Ø§Ù„
    }
    },
    }

    def generate_verb_syllable_patterns()
    self, root: List[str], verb_form: str, include_pronouns: bool = False
    ) -> Dict[str, Any]:
    """
    ØªÙˆÙ„ÙŠØ¯ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„ÙØ¹Ù„

    Args:
    root: Ø§Ù„Ø¬Ø°Ø± (Ù…Ø«Ù„ ['Ùƒ', 'Øª', 'Ø¨'])
    verb_form: ØµÙŠØºØ© Ø§Ù„ÙØ¹Ù„ (I, II, III, ...)
    include_pronouns: ØªØ´Ù…Ù„ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©

    Returns:
    Dict: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„ÙØ¹Ù„
    """

        if verb_form not in self.verb_forms:
    raise ValueError(f"ØµÙŠØºØ© Ø§Ù„ÙØ¹Ù„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {verb_form}")

        form_data = self.verb_forms[verb_form]

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙØ¹Ù„ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„ÙˆØ²Ù†
    verb_word = self._construct_verb(root, form_data)

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©
    phonologically_adjusted = self._apply_phonological_rules(verb_word, root)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    syllable_analysis = self._analyze_syllables(phonologically_adjusted)

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨Øª
        if include_pronouns:
    pronoun_variants = self._add_pronoun_variants(phonologically_adjusted)
        else:
    pronoun_variants = []

    return {
    'root': root,
    'verb_form': verb_form,
    'pattern': form_data.pattern,
    'constructed_verb': verb_word,
    'phonologically_adjusted': phonologically_adjusted,
    'syllable_structure': syllable_analysis['patterns'],
    'syllable_count': syllable_analysis['count'],
    'complexity_score': syllable_analysis['complexity'],
    'phonological_processes': syllable_analysis['processes'],
    'pronoun_variants': pronoun_variants,
    'morphological_analysis': {
    'morphemes': form_data.morphemes,
    'meaning': form_data.meaning,
    'type': self._classify_verb_type(verb_form),
    },
    }

    def generate_source_syllable_patterns()
    self, root: List[str], verb_form: str, source_type: str = "standard"
    ) -> List[Dict[str, Any]]:
    """
    ØªÙˆÙ„ÙŠØ¯ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ù…ØµØ§Ø¯Ø±

    Args:
    root: Ø§Ù„Ø¬Ø°Ø±
    verb_form: ØµÙŠØºØ© Ø§Ù„ÙØ¹Ù„
    source_type: Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø± (standard, irregular)

    Returns:
    List[Dict]: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…Ø¹ ØªØ­Ù„ÙŠÙ„Ù‡Ø§
    """

        if verb_form not in self.source_patterns and source_type != "irregular":
    raise ValueError(f"Ù…ØµØ§Ø¯Ø± Ø§Ù„ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {verb_form}")

    sources_data = self.source_patterns.get(verb_form, [])
        if source_type == "irregular":
    sources_data.extend(self.source_patterns.get("irregular", []))

    results = []

        for source_pattern in sources_data:
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ù† Ø§Ù„Ø¬Ø°Ø±
    source_word = self._construct_source(root, source_pattern)

            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©
    phonologically_adjusted = self._apply_phonological_rules(source_word, root)

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    syllable_analysis = self._analyze_syllables(phonologically_adjusted)

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
    phonological_features = self._analyze_phonological_features()
    phonologically_adjusted, source_pattern.phonological_features
    )

    results.append()
    {
    'root': root,
    'verb_form': verb_form,
    'source_word': source_word,
    'phonologically_adjusted': phonologically_adjusted,
    'syllable_pattern': source_pattern.syllable_pattern,
    'analyzed_patterns': syllable_analysis['patterns'],
    'syllable_count': syllable_analysis['count'],
    'is_standard': source_pattern.is_standard,
    'phonological_features': phonological_features,
    'complexity_score': syllable_analysis['complexity'],
    'morphological_analysis': {
    'source_type': ()
    'Ù‚ÙŠØ§Ø³ÙŠ' if source_pattern.is_standard else 'Ø³Ù…Ø§Ø¹ÙŠ'
    ),
    'semantic_field': self._determine_semantic_field(verb_form),
    },
    }
    )

    return results

    def _construct_verb(self, root: List[str], form_data: VerbForm) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙØ¹Ù„ Ù…Ù† Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„ÙˆØ²Ù†"""

    pattern = form_data.pattern

        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø±Ù…ÙˆØ² Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ø§Ù„ÙˆØ²Ù†
        if len(root) == 3:  # Ø¬Ø°Ø± Ø«Ù„Ø§Ø«ÙŠ
    result = ()
    pattern.replace('Ù', root[0])
    .replace('Ø¹', root[1])
    .replace('Ù„', root[2])
    )
        elif len(root) == 4:  # Ø¬Ø°Ø± Ø±Ø¨Ø§Ø¹ÙŠ
            # Ù†Ø­ØªØ§Ø¬ Ù„Ù†Ù…Ø· Ø®Ø§Øµ Ù„Ù„Ø±Ø¨Ø§Ø¹ÙŠ
            if form_data.form_number.startswith('Q'):
    result = ()
    pattern.replace('Ù', root[0])
    .replace('Ø¹', root[1])
    .replace('Ù„', root[2])
    .replace('Ù„', root[3])
    )
            else:
    result = pattern  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ù„Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø®Ø§ØµØ©
        else:
    raise ValueError(f"Ø·ÙˆÙ„ Ø§Ù„Ø¬Ø°Ø± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {len(root)}")

    return result

    def _construct_source(self, root: List[str], source_pattern: SourcePattern) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ù† Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„Ù†Ù…Ø·"""

    pattern = source_pattern.source_word

        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø±Ù…ÙˆØ² Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ù†Ù…Ø· Ø§Ù„Ù…ØµØ¯Ø±
        if len(root) == 3:
    result = ()
    pattern.replace('Ù', root[0])
    .replace('Ø¹', root[1])
    .replace('Ù„', root[2])
    )
        elif len(root) == 4:
            # Ù„Ù„Ø±Ø¨Ø§Ø¹ÙŠ
    result = ()
    pattern.replace('Ù', root[0])
    .replace('Ø¹', root[1])
    .replace('Ù„', root[2])
    .replace('Ù„', root[3])
    )
        else:
    result = pattern

    return result

    def _apply_phonological_rules(self, word: str, root: List[str]) -> str:
    """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠØ©"""

    result = word
    applied_processes = []

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¯ØºØ§Ù…
        for cluster, replacement in self.phonological_rules['assimilation'][
    'consonant_clusters'
    ].items():
            if cluster in result:
    result = result.replace(cluster, replacement)
    applied_processes.append(f"Ø¥Ø¯ØºØ§Ù…: {cluster } > {replacement}}")

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù„
        for original, changed in self.phonological_rules['weakening'][
    'waw_alif'
    ].items():
            if any(char in original for char in root):
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚
                if 'Ùˆ' in word and 'Ù' in word:
    result = result.replace('ÙˆÙ', 'Ø§')
    applied_processes.append("Ø¥Ø¹Ù„Ø§Ù„: ÙˆÙ  > Ø§")

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙ‚Ø§Ø¡ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†
    result = self._resolve_consonant_clusters(result)

    return result

    def _resolve_consonant_clusters(self, word: str) -> str:
    """Ø­Ù„ Ø§Ù„ØªÙ‚Ø§Ø¡ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†"""

        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¨Ø³Ø·Ø© Ù„ÙƒØ³Ø± Ø§Ù„ØªÙ‚Ø§Ø¡ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ†
    result = word

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ‚Ø§Ø¡ Ø§Ù„Ø³Ø§ÙƒÙ†ÙŠÙ† ÙˆØ¥Ø¯Ø®Ø§Ù„ Ø­Ø±ÙƒØ©
    consonant_patterns = re.findall(r'[Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ]{2,}', result)

        for pattern in consonant_patterns:
            if len(pattern) >= 2:
                # Ø¥Ø¯Ø®Ø§Ù„ ÙƒØ³Ø±Ø© Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø³Ø§ÙƒÙ†Ø©
    modified = pattern[0] + 'Ù' + pattern[1:]
    result = result.replace(pattern, modified, 1)

    return result

    def _analyze_syllables(self, word: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø©"""

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹
    syllables = self._segment_into_syllables(word)

        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· ÙƒÙ„ Ù…Ù‚Ø·Ø¹
    patterns = []
        for syllable in syllables:
    pattern = self._determine_syllable_pattern(syllable)
    patterns.append(pattern)

        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
    complexity = self._calculate_syllable_complexity(patterns)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
    processes = self._identify_phonological_processes(syllables)

    return {
    'syllables': syllables,
    'patterns': patterns,
    'count': len(syllables),
    'complexity': complexity,
    'processes': processes,
    }

    def _segment_into_syllables(self, word: str) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹"""

    syllables = []
    current_syllable = ""

    i = 0
        while i < len(word):
    char = word[i]

            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
    current_syllable += char

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹
            if self._is_vowel(char) or char in self.short_vowels:
                # Ù†ÙˆØ§Ø© Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù†Ù†Ø¸Ø± Ù„Ù„Ø­Ø±Ù Ø§Ù„ØªØ§Ù„ÙŠ
                if i + 1 < len(word):
    next_char = word[i + 1]
                    if self._is_consonant(next_char):
                        # ØµØ§Ù…Øª Ø¨Ø¹Ø¯ Ø§Ù„ØµØ§Ø¦Øª
                        if i + 2 < len(word) and self._is_vowel(word[i + 2]):
                            # ØµØ§Ù…Øª + ØµØ§Ø¦Øª Ø¨Ø¹Ø¯Ù‡Ø§  > Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
    syllables.append(current_syllable)
    current_syllable = ""
                        else:
                            # ØµØ§Ù…Øª ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø© -> Ù†Ø¶ÙŠÙÙ‡ Ù„Ù„Ù…Ù‚Ø·Ø¹
                            if i + 1 == len(word) - 1:
    current_syllable += next_char
    syllables.append(current_syllable)
    current_syllable = ""
    i += 1  # ØªØ®Ø·ÙŠ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ù…Ø¶Ø§Ù
                else:
                    # Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø©
    syllables.append(current_syllable)
    current_syllable = ""

    i += 1

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ø£Ø®ÙŠØ± Ø¥Ø°Ø§ Ø¨Ù‚ÙŠ Ø´ÙŠØ¡
        if current_syllable:
    syllables.append(current_syllable)

    return syllables

    def _determine_syllable_pattern(self, syllable: str) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ"""

    pattern = ""

        for char in syllable:
            if self._is_consonant(char):
    pattern += "C"
            elif self._is_vowel(char) or char in self.short_vowels:
    pattern += "V"
            elif char in self.diacritics:
                # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ´ÙƒÙŠÙ„
                if char == 'Ù‘':  # Ø§Ù„Ø´Ø¯Ø©
    pattern += "C"  # ØªØ¶Ø¹ÙŠÙ Ø§Ù„ØµØ§Ù…Øª
                # Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ø§ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

    return pattern

    def _is_consonant(self, char: str) -> bool:
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø±Ù ØµØ§Ù…ØªØ§Ù‹"""
    return char in self.all_consonants

    def _is_vowel(self, char: str) -> bool:
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø±Ù ØµØ§Ø¦ØªØ§Ù‹"""
    return char in self.long_vowels or char in self.short_vowels

    def _calculate_syllable_complexity(self, patterns: List[str]) -> float:
    """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    complexity_weights = {
    'V': 1.0,
    'CV': 1.2,
    'CVC': 1.5,
    'CVV': 1.8,
    'CVVC': 2.0,
    'CVCC': 2.5,
    'CCV': 3.0,
    'CVCCC': 3.5,
    }

    total_complexity = sum()
    complexity_weights.get(pattern, 1.0) for pattern in patterns
    )
    average_complexity = total_complexity / len(patterns) if patterns else 0.0

    return round(average_complexity, 2)

    def _identify_phonological_processes(self, syllables: List[str]) -> List[str]:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©"""

    processes = []

        for syllable in syllables:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø´Ø¯Ø© (Ø§Ù„Ø¥Ø¯ØºØ§Ù…)
            if 'Ù‘' in syllable:
    processes.append("Ø¥Ø¯ØºØ§Ù…")

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¯
            if any(vowel in syllable for vowel in self.long_vowels):
    processes.append("Ù…Ø¯_ØµÙˆØªÙŠ")

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙ†ÙˆÙŠÙ†
            if any(diac in syllable for diac in ['Ù‹', 'ÙŒ', 'Ù']):
    processes.append("ØªÙ†ÙˆÙŠÙ†")

    return list(set(processes))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±

    def _add_pronoun_variants(self, verb: str) -> List[Dict[str, Any]]:
    """Ø¥Ø¶Ø§ÙØ© Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©"""

    pronouns = {
    'ÙˆÙ†': {'type': 'plural_masculine', 'meaning': 'they_masc'},
    'ÙŠÙ†': {'type': 'plural_feminine', 'meaning': 'they_fem'},
    'Øª': {'type': 'second_person', 'meaning': 'you'},
    'Ù†Ø§': {'type': 'first_person_plural', 'meaning': 'we'},
    'Ù‡Ø§': {'type': 'attached_feminine', 'meaning': 'her/it_fem'},
    'Ù‡': {'type': 'attached_masculine', 'meaning': 'him/it_masc'},
    }

    variants = []

        for pronoun, data in pronouns.items():
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¶Ù…ÙŠØ± Ù„Ù„ÙØ¹Ù„
    extended_verb = verb + pronoun

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    syllable_analysis = self._analyze_syllables(extended_verb)

    variants.append()
    {
    'verb_with_pronoun': extended_verb,
    'pronoun': pronoun,
    'pronoun_type': data['type'],
    'meaning': data['meaning'],
    'syllable_patterns': syllable_analysis['patterns'],
    'syllable_count': syllable_analysis['count'],
    'complexity_score': syllable_analysis['complexity'],
    }
    )

    return variants

    def _classify_verb_type(self, verb_form: str) -> str:
    """ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„ÙØ¹Ù„"""

        if verb_form == "I":
    return "Ø«Ù„Ø§Ø«ÙŠ_Ù…Ø¬Ø±Ø¯"
        elif verb_form in ["II", "III", "IV", "V", "VI", "VII", "VIII", "IX", "X"]:
    return "Ø«Ù„Ø§Ø«ÙŠ_Ù…Ø²ÙŠØ¯"
        elif verb_form == "Q1":
    return "Ø±Ø¨Ø§Ø¹ÙŠ_Ù…Ø¬Ø±Ø¯"
        elif verb_form.startswith("Q"):
    return "Ø±Ø¨Ø§Ø¹ÙŠ_Ù…Ø²ÙŠØ¯"
        else:
    return "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯"

    def _analyze_phonological_features()
    self, word: str, base_features: List[str]
    ) -> List[str]:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©"""

    features = base_features.copy()

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‡Ù…Ø²Ø©
        if 'Ø¡' in word:
    features.append("Ù‡Ù…Ø²Ø©")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙØ®Ù…Ø©
    emphatic_letters = ['Øµ', 'Ø¶', 'Ø·', 'Ø¸']
        if any(letter in word for letter in emphatic_letters):
    features.append("ØªÙØ®ÙŠÙ…")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ø±ÙˆÙ Ø§Ù„Ù‚Ù„Ù‚Ù„Ø©
    qalqala_letters = ['Ù‚', 'Ø·', 'Ø¨', 'Ø¬', 'Ø¯']
        if any(letter in word for letter in qalqala_letters):
    features.append("Ù‚Ù„Ù‚Ù„Ø©")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¯
        if any(vowel in word for vowel in self.long_vowels):
    features.append("Ù…Ø¯_Ø·Ø¨ÙŠØ¹ÙŠ")

    return features

    def _determine_semantic_field(self, verb_form: str) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""

    semantic_fields = {
    "I": "Ø§Ù„Ø£ÙØ¹Ø§Ù„_Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©",
    "II": "Ø§Ù„ØªÙƒØ«ÙŠØ±_ÙˆØ§Ù„Ø³Ø¨Ø¨ÙŠØ©",
    "III": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©_ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©",
    "IV": "Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©_ÙˆØ§Ù„ØªØ¹Ø¯ÙŠØ©",
    "V": "Ø§Ù„ØªØ¯Ø±Ø¬_ÙˆØ§Ù„Ø§Ù†ÙØ¹Ø§Ù„",
    "VI": "Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©_Ø§Ù„Ù…ØªØ¨Ø§Ø¯Ù„Ø©",
    "VII": "Ø§Ù„Ø§Ù†ÙØ¹Ø§Ù„_ÙˆØ§Ù„ØªØ£Ø«Ø±",
    "VIII": "Ø§Ù„Ø§ÙƒØªØ³Ø§Ø¨_ÙˆØ§Ù„Ø·Ù„Ø¨",
    "IX": "Ø§Ù„Ø£Ù„ÙˆØ§Ù†_ÙˆØ§Ù„Ø¹ÙŠÙˆØ¨",
    "X": "Ø§Ù„Ø·Ù„Ø¨_ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡",
    "Q1": "Ø§Ù„Ø­Ø±ÙƒØ©_ÙˆØ§Ù„ØµÙˆØª",
    "Q2": "Ø§Ù„ØªØ¯Ø±Ø¬_Ø§Ù„Ø±Ø¨Ø§Ø¹ÙŠ",
    }

    return semantic_fields.get(verb_form, "ØºÙŠØ±_Ù…Ø­Ø¯Ø¯")

    def generate_comprehensive_analysis()
    self,
    roots: List[List[str]],
    verb_forms: List[str] = None,
    include_sources: bool = True,
    include_pronouns: bool = False) -> Dict[str, Any]:
    """
    ØªÙˆÙ„ÙŠØ¯ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ø£ÙØ¹Ø§Ù„

    Args:
    roots: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø°ÙˆØ±
    verb_forms: Ù‚Ø§Ø¦Ù…Ø© ØµÙŠØº Ø§Ù„Ø£ÙØ¹Ø§Ù„ (Ø¥Ø°Ø§ Ù„Ù… ØªÙØ­Ø¯Ø¯ØŒ Ø³ØªÙØ³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙŠØº)
    include_sources: ØªØ´Ù…Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±
    include_pronouns: ØªØ´Ù…Ù„ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©

    Returns:
    Dict: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
    """

        if verb_forms is None:
    verb_forms = list(self.verb_forms.keys())

    results = {
    'total_combinations': 0,
    'verb_analysis': [],
    'source_analysis': [],
    'statistics': {
    'syllable_patterns': Counter(),
    'complexity_distribution': [],
    'phonological_processes': Counter(),
    'verb_types': Counter(),
    },
    'coverage_analysis': {
    'covered_patterns': set(),
    'new_patterns': set(),
    'complexity_range': {'min': float('inf'), 'max': 0},
    },
    }

    logger.info(f"Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù€ {len(roots)} Ø¬Ø°Ø± Ùˆ {len(verb_forms)} ØµÙŠØºØ©")

        for root in roots:
            for verb_form in verb_forms:
                try:
                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¹Ù„
    verb_analysis = self.generate_verb_syllable_patterns()
    root, verb_form, include_pronouns
    )
    results['verb_analysis'].append(verb_analysis)

                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    self._update_statistics(results['statistics'], verb_analysis)
    self._update_coverage_analysis()
    results['coverage_analysis'], verb_analysis
    )

                    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±
                    if include_sources and verb_form in self.source_patterns:
    source_analyses = self.generate_source_syllable_patterns()
    root, verb_form
    )
    results['source_analysis'].extend(source_analyses)

                        for source_analysis in source_analyses:
    self._update_statistics()
    results['statistics'], source_analysis, is_source=True
    )

    results['total_combinations'] += 1

                except Exception as e:
    logger.warning()
    f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø± {root} Ù…Ø¹ Ø§Ù„ØµÙŠØºØ© {verb_form: {str(e)}}"
    )

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    results['final_statistics'] = self._calculate_final_statistics(results)

    logger.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„: {results['total_combinations']} ØªÙˆØ§ÙÙŠÙ‚")

    return results

    def _update_statistics(self, stats: Dict, analysis: Dict, is_source: bool = False):
    """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""

        # ØªØ­Ø¯ÙŠØ« Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    patterns = ()
    analysis.get('syllable_structure', [])
            if not is_source
            else analysis.get('analyzed_patterns', [])
    )
        for pattern in patterns:
    stats['syllable_patterns'][pattern] += 1

        # ØªØ­Ø¯ÙŠØ« ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
    complexity = analysis.get('complexity_score', 0)
    stats['complexity_distribution'].append(complexity)

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©
    processes = analysis.get('phonological_processes', [])
        for process in processes:
    stats['phonological_processes'][process] += 1

        # ØªØ­Ø¯ÙŠØ« Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£ÙØ¹Ø§Ù„
        if not is_source:
    verb_type = analysis.get('morphological_analysis', {}).get()
    'type', 'ØºÙŠØ±_Ù…Ø­Ø¯Ø¯'
    )
    stats['verb_types'][verb_type] += 1

    def _update_coverage_analysis(self, coverage: Dict, analysis: Dict):
    """ØªØ­Ø¯ÙŠØ« ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØºØ·ÙŠØ©"""

    patterns = analysis.get('syllable_structure', [])
        for pattern in patterns:
    coverage['covered_patterns'].add(pattern)

    complexity = analysis.get('complexity_score', 0)
    coverage['complexity_range']['min'] = min()
    coverage['complexity_range']['min'], complexity
    )
    coverage['complexity_range']['max'] = max()
    coverage['complexity_range']['max'], complexity
    )

    def _calculate_final_statistics(self, results: Dict) -> Dict:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""

    stats = results['statistics']

    return {
    'total_unique_patterns': len(stats['syllable_patterns']),
    'most_common_patterns': stats['syllable_patterns'].most_common(10),
    'average_complexity': ()
    sum(stats['complexity_distribution'])
    / len(stats['complexity_distribution'])
                if stats['complexity_distribution']
                else 0
    ),
    'complexity_range': {
    'min': ()
    min(stats['complexity_distribution'])
                    if stats['complexity_distribution']
                    else 0
    ),
    'max': ()
    max(stats['complexity_distribution'])
                    if stats['complexity_distribution']
                    else 0
    ),
    },
    'most_common_processes': stats['phonological_processes'].most_common(5),
    'verb_type_distribution': dict(stats['verb_types']),
    'coverage_percentage': len(results['coverage_analysis']['covered_patterns'])
    / 14
    * 100,  # Ù…Ù† Ø£ØµÙ„ 14 Ù†Ù…Ø· Ù…Ù‚Ø·Ø¹ÙŠ
    }

    def export_analysis_report()
    self, results: Dict, filename: str = "arabic_verb_analysis_report.json"
    ):
    """ØªØµØ¯ÙŠØ± ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„"""

        # ØªØ­ÙˆÙŠÙ„ Counter Ùˆ set Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„
    exportable_results = self._make_serializable(results)

        with open(filename, 'w', encoding='utf 8') as f:
    json.dump(exportable_results, f, ensure_ascii=False, indent=2)

    logger.info(f"ØªÙ… ØªØµØ¯ÙŠØ± ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¥Ù„Ù‰ {filename}")

    def _make_serializable(self, obj):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ³Ù„Ø³Ù„"""

        if isinstance(obj, dict):
    return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
    return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, Counter):
    return dict(obj)
        elif isinstance(obj, set):
    return list(obj)
        else:
    return obj


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION AND TESTING - Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…"""

    print("ğŸ”¤ Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ© Ù„Ù„Ø£ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 80)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    system = ArabicVerbMorphologySystem()

    # Ø¬Ø°ÙˆØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_roots = [
    ['Ùƒ', 'Øª', 'Ø¨'],  # ÙƒØªØ¨
    ['Ø¯', 'Ø±', 'Ø³'],  # Ø¯Ø±Ø³
    ['Ø¹', 'Ù„', 'Ù…'],  # Ø¹Ù„Ù…
    ['Ø³', 'Ø£', 'Ù„'],  # Ø³Ø£Ù„ (Ù…Ø¹ Ù‡Ù…Ø²Ø©)
    ['Ù‚', 'Ùˆ', 'Ù„'],  # Ù‚ÙˆÙ„ (Ù…Ø¹ØªÙ„)
    ['Ø¯', 'Ø­', 'Ø±', 'Ø¬'],  # Ø¯Ø­Ø±Ø¬ (Ø±Ø¨Ø§Ø¹ÙŠ)
    ]

    # ØµÙŠØº Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_forms = ['I', 'II', 'IV', 'V', 'X', 'Q1']

    print("ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£ÙØ¹Ø§Ù„:")
    print(" " * 40)

    # Ø§Ø®ØªØ¨Ø§Ø± ÙØ¹Ù„ ÙˆØ§Ø­Ø¯ Ù…ÙØµÙ„
    sample_analysis = system.generate_verb_syllable_patterns()
    ['Ùƒ', 'Øª', 'Ø¨'], 'X', include_pronouns=True
    )

    print(f"ğŸ“ Ù…Ø«Ø§Ù„ ØªÙØµÙŠÙ„ÙŠ: {sample_analysis['constructed_verb']}")
    print(f"   Ø§Ù„Ø¬Ø°Ø±: {'} - '.join(sample_analysis['root'])}")
    print(f"   Ø§Ù„ØµÙŠØºØ©: {sample_analysis['verb_form']} ({sample_analysis['pattern'])}")
    print(f"   Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {'} - '.join(sample_analysis['syllable_structure'])}")
    print(f"   Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {sample_analysis['complexity_score']}")
    print()
    f"   Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØµÙˆØªÙŠØ©: {', '.join(sample_analysis['phonological_processes'])}"
    )

    if sample_analysis['pronoun_variants']:
    print("   ğŸ”¸ Ù…ØªØºÙŠØ±Ø§Øª Ù…Ø¹ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±:")
        for variant in sample_analysis['pronoun_variants'][:3]:  # Ø£ÙˆÙ„ 3 ÙÙ‚Ø·
    print()
    f"      {variant['verb_with_pronoun']} ({variant['pronoun_type']}) - {' '.join(variant['syllable_patterns'])}"
    )

    print("\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø±:")
    print(" " * 40)

    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø±
    source_analyses = system.generate_source_syllable_patterns(['Ø¹', 'Ù„', 'Ù…'], 'II')

    for source in source_analyses:
    print(f"ğŸ“š Ø§Ù„Ù…ØµØ¯Ø±: {source['source_word']}")
    print(f"   Ø§Ù„Ù†Ù…Ø·: {source['syllable_pattern']}")
    print(f"   Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø­Ù„Ù„Ø©: {'} - '.join(source['analyzed_patterns'])}")
    print(f"   Ø§Ù„Ù†ÙˆØ¹: {source['morphological_analysis']['source_type']}")
    print(f"   Ø§Ù„Ø®ØµØ§Ø¦Øµ: {', '.join(source['phonological_features'])}")
    print()

    print("ğŸ”¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„:")
    print(" " * 40)

    # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
    comprehensive_results = system.generate_comprehensive_analysis()
    test_roots[:3],  # Ø£ÙˆÙ„ 3 Ø¬Ø°ÙˆØ± ÙÙ‚Ø· Ù„Ù„Ø³Ø±Ø¹Ø©
    test_forms[:4],  # Ø£ÙˆÙ„ 4 ØµÙŠØº
    include_sources=True,
    include_pronouns=False)

    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙˆØ§ÙÙŠÙ‚: {comprehensive_results['total_combinations']}")
    print()
    f"ğŸ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙØ±Ø¯Ø©: {comprehensive_results['final_statistics']['total_unique_patterns']}"
    )
    print()
    f"ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {comprehensive_results['final_statistics']['average_complexity']:.2f}"
    )
    print()
    f"ğŸ“‹ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØºØ·ÙŠØ©: {comprehensive_results['final_statistics']['coverage_percentage']:.1f}%"
    )

    print("\nğŸ† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹:")
    for pattern, count in comprehensive_results['final_statistics'][
    'most_common_patterns'
    ]:
    print(f"   {pattern}: {count} Ù…Ø±Ø©")

    print("\nğŸ”§ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹:")
    for process, count in comprehensive_results['final_statistics'][
    'most_common_processes'
    ]:
    print(f"   {process}: {count} Ù…Ø±Ø©")

    # ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    system.export_analysis_report(comprehensive_results)

    print("\nâœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±!")
    print("ğŸ“„ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ±: arabic_verb_analysis_report.json")


if __name__ == "__main__":
    main()

