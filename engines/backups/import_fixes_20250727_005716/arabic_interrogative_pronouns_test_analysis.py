#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Interrogative Pronouns Testing & Analysis System
======================================================
Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ÙˆØªØ­Ù„ÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Comprehensive testing, evaluation, and performance analysis system for
Arabic interrogative pronouns deep learning models.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 1.0.0 - TESTING & ANALYSIS
Date: 2025-07-24
Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc


import numpy as np  # noqa: F401
import pandas as pd  # noqa: F401
import matplotlib.pyplot as plt  # noqa: F401
import seaborn as sns  # noqa: F401
from sklearn.metrics import ()
    classification_report,
    confusion_matrix,
    accuracy_score,
    f1_score)  # noqa: F401
from sklearn.model_selection import StratifiedKFold  # noqa: F401
import torch  # noqa: F401
import json  # noqa: F401
import time  # noqa: F401
from typing import Dict, List, Tuple, Any
import warnings  # noqa: F401

warnings.filterwarnings('ignore')

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
from arabic_interrogative_pronouns_deep_model import (  # noqa: F401
    InterrogativePronounInference,
    INTERROGATIVE_PRONOUNS,
    PRONOUN_TO_ID)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMPREHENSIVE TEST SUITE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class InterrogativePronounTestSuite:
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.inference_system = InterrogativePronounInference()
    self.test_results = {}

        # Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„Ø©
    self.test_cases = {
    'basic': [
    (["Ù…ÙÙ†Ù’"], "Ù…ÙÙ†", "Ø´Ø®Øµ"),
    (["Ù…ÙØ§"], "Ù…ÙØ§", "Ø´ÙŠØ¡"),
    (["Ù…Ù", "ØªÙÙ‰"], "Ù…ÙØªÙÙ‰", "Ø²Ù…Ø§Ù†"),
    (["Ø£ÙÙŠÙ’", "Ù†Ù"], "Ø£ÙÙŠÙ’Ù†Ù", "Ù…ÙƒØ§Ù†"),
    (["ÙƒÙÙŠÙ’", "ÙÙ"], "ÙƒÙÙŠÙ’ÙÙ", "ÙƒÙŠÙÙŠØ©"),
    (["ÙƒÙÙ…Ù’"], "ÙƒÙÙ…Ù’", "ÙƒÙ…ÙŠØ©"),
    (["Ø£ÙÙŠÙ‘"], "Ø£ÙÙŠÙ‘", "Ø§Ø®ØªÙŠØ§Ø±"),
    (["Ù„Ù", "Ù…Ù"], "Ù„ÙÙ…Ù", "Ø³Ø¨Ø¨"),
    ],
    'intermediate': [
    (["Ù…ÙØ§", "Ø°ÙØ§"], "Ù…ÙØ§Ø°ÙØ§", "Ø´ÙŠØ¡"),
    (["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"], "Ù„ÙÙ…ÙØ§Ø°ÙØ§", "Ø³Ø¨Ø¨"),
    (["Ø£ÙÙŠÙÙ‘", "Ø§", "Ù†Ù"], "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù", "Ø²Ù…Ø§Ù†"),
    (["Ø£ÙÙ†ÙÙ‘", "Ù‰"], "Ø£ÙÙ†ÙÙ‘Ù‰", "Ù…ÙƒØ§Ù†"),
    (["ÙƒÙØ£ÙÙŠÙ’", "ÙŠÙÙ†Ù’"], "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’", "ÙƒÙ…ÙŠØ©"),
    ],
    'advanced': [
    (["Ø£ÙÙŠÙÙ‘", "Ù‡ÙØ§"], "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§", "Ø§Ø®ØªÙŠØ§Ø±"),
    (["Ù…ÙÙ‡Ù’", "Ù…ÙØ§"], "Ù…ÙÙ‡Ù’Ù…ÙØ§", "Ø´ÙŠØ¡"),
    (["Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"], "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§", "Ù…ÙƒØ§Ù†"),
    (["ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"], "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§", "ÙƒÙŠÙÙŠØ©"),
    (["Ù…ÙÙ†Ù’", "Ø°ÙØ§"], "Ù…ÙÙ†Ù’ Ø°ÙØ§", "Ø´Ø®Øµ"),
    ],
    'variations': [
                # ØªÙ†ÙˆÙŠØ¹Ø§Øª ØµÙˆØªÙŠØ©
    (["Ù…ÙÙ†Ù’"], "Ù…ÙÙ†", "Ø´Ø®Øµ"),  # ØªØºÙŠÙŠØ± Ø­Ø±ÙƒØ©
    (["Ù…ÙØ§"], "Ù…ÙØ§", "Ø´ÙŠØ¡"),  # ØªØºÙŠÙŠØ± Ø­Ø±ÙƒØ©
    (["Ù…Ù", "ØªÙÙ‰"], "Ù…ÙØªÙÙ‰", "Ø²Ù…Ø§Ù†"),  # ØªØºÙŠÙŠØ± Ø­Ø±ÙƒØ©
    (["Ø£ÙÙŠÙ’", "Ù†Ù"], "Ø£ÙÙŠÙ’Ù†Ù", "Ù…ÙƒØ§Ù†"),  # ØªØºÙŠÙŠØ± Ø­Ø±ÙƒØ©
    ],
    }

    def run_basic_tests(self, model_types: List[str] = None) -> Dict[str, Any]:
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""

        if model_types is None:
    model_types = ['lstm', 'gru', 'transformer']

    print("ğŸ§ª ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©...")

    results = {}

        for model_type in model_types:
    print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ {model_type.upper()}...")

    model_results = {
    'correct': 0,
    'total': 0,
    'details': [],
    'accuracy_by_category': {},
    'errors': [],
    }


            # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø§Ù„Ø§Øª
            for category, cases in self.test_cases.items():
    category_correct = 0
    category_total = len(cases)

                for syllables, expected, semantic_category in cases:
    prediction = self.inference_system.predict_syllables()
    syllables, model_type
    )
    is_correct = prediction == expected

    model_results['details'].append()
    {
    'syllables': syllables,
    'expected': expected,
    'predicted': prediction,
    'correct': is_correct,
    'category': category,
    'semantic': semantic_category,
    }
    )

                    if is_correct:
    model_results['correct'] += 1
    category_correct += 1
                    else:
    model_results['errors'].append()
    {
    'syllables': syllables,
    'expected': expected,
    'predicted': prediction,
    'category': category,
    }
    )

    model_results['total'] += 1

    category_accuracy = category_correct / category_total
    model_results['accuracy_by_category'][category] = category_accuracy

    print()
    f"   ğŸ“Š {category}: {category_accuracy:.1%} ({category_correct}/{category_total)}"
    )  # noqa: E501

    overall_accuracy = model_results['correct'] / model_results['total']
    model_results['overall_accuracy'] = overall_accuracy

    results[model_type] = model_results
    print(f"   âœ… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {overall_accuracy:.1%}")

    return results

    def run_stress_tests()
    self, model_type: str = 'transformer', num_tests: int = 500
    ) -> Dict[str, Any]:
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡"""

    print(f"âš¡ ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶ØºØ· ({num_tests} Ø§Ø®ØªØ¨Ø§Ø±)...")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        from arabic_interrogative_pronouns_deep_model import ()
    create_synthetic_data)  # noqa: F401

    X, y = create_synthetic_data(self.inference_system.processor, num_tests)

        # Ù‚ÙŠØ§Ø³ Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©
    start_time = time.time()
    memory_before = ()
    torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
    )

    correct_predictions = 0
    prediction_times = []

        for i, (features, true_label) in enumerate(zip(X, y)):
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØªÙ‚Ø±ÙŠØ¨ÙŠØ© (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
    syllables = ["Ù…ÙÙ†Ù’"] if true_label == 0 else ["Ù…ÙØ§"]  # Ù…Ø¨Ø³Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±

    pred_start = time.time()
    predicted_pronoun = self.inference_system.predict_syllables()
    syllables, model_type
    )
    pred_time = time.time() - pred_start
    prediction_times.append(pred_time)

            if predicted_pronoun == INTERROGATIVE_PRONOUNS[true_label]:
    correct_predictions += 1

    end_time = time.time()
    memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0

    total_time = end_time - start_time
    avg_prediction_time = np.mean(prediction_times)
    accuracy = correct_predictions / num_tests

    results = {
    'total_tests': num_tests,
    'accuracy': accuracy,
    'total_time': total_time,
    'avg_prediction_time': avg_prediction_time,
    'predictions_per_second': num_tests / total_time,
    'memory_usage_mb': ()
    (memory_after - memory_before) / (1024 * 1024)
                if torch.cuda.is_available()
                else 0
    ),
    'prediction_times': prediction_times,
    }

    print(f"   âš¡ Ø§Ù„Ø¯Ù‚Ø©: {accuracy:.1%}")
    print(f"   â±ï¸  Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.2fs}")
    print(f"   ğŸ“ˆ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª/Ø«Ø§Ù†ÙŠØ©: {results['predictions_per_second']:.1f}")
    print(f"   â±ï¸  Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ†Ø¨Ø¤: {avg_prediction_time*1000:.2f}ms")

    return results

    def run_cross_validation()
    self, num_folds: int = 5, samples_per_fold: int = 200
    ) -> Dict[str, Any]:
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹"""

    print(f"ğŸ”„ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ ({num_folds} Ø·ÙŠØ§Øª)...")

        from arabic_interrogative_pronouns_deep_model import ()
    create_synthetic_data,
    train_model)  # noqa: F401

        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
    X, y = create_synthetic_data()
    self.inference_system.processor, num_folds * samples_per_fold
    )

    X_array = np.array([x.flatten() for x in X])
    y_array = np.array(y)

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø§Ù„Ø·Ø¨Ù‚ÙŠ
    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

    cv_results = {
    'fold_accuracies': [],
    'fold_f1_scores': [],
    'mean_accuracy': 0,
    'std_accuracy': 0,
    'mean_f1': 0,
    'std_f1': 0,
    }

        for fold, (train_idx, test_idx) in enumerate(skf.split(X_array, y_array)):
    print(f"   ğŸ“‚ Ø§Ù„Ø·ÙŠØ© {fold} + 1}/{num_folds}...")

            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_train = [X[i] for i in train_idx]
    y_train = [y[i] for i in train_idx]
    X_test = [X[i] for i in test_idx]
    y_test = [y[i] for i in test_idx]

            # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯
            from arabic_interrogative_pronouns_deep_model import ()
    InterrogativeTransformer)  # noqa: F401

    model = InterrogativeTransformer()
    input_size=self.inference_system.processor.vocab_size + 12,
    d_model=128,
    nhead=8,
    num_layers=3,  # Ø£Ù‚Ù„ Ù„Ù„Ø³Ø±Ø¹Ø©
    num_classes=len(INTERROGATIVE_PRONOUNS),
    dropout=0.1)

            # ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹
    train_model(model, X_train, y_train, epochs=15, batch_size=32)

            # ØªÙ‚ÙŠÙŠÙ…
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    predictions = []
            with torch.no_grad():
                for x in X_test:
    input_tensor = torch.FloatTensor(x).unsqueeze(0).to(device)
    outputs = model(input_tensor)
    pred = torch.argmax(outputs, dim=1).item()
    predictions.append(pred)

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    accuracy = accuracy_score(y_test, predictions)
    f1 = f1_score(y_test, predictions, average='weighted')

    cv_results['fold_accuracies'].append(accuracy)
    cv_results['fold_f1_scores'].append(f1)

    print(f"      Ø¯Ù‚Ø© Ø§Ù„Ø·ÙŠØ©: {accuracy:.3f,} F1: {f1:.3f}}")

        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    cv_results['mean_accuracy'] = np.mean(cv_results['fold_accuracies'])
    cv_results['std_accuracy'] = np.std(cv_results['fold_accuracies'])
    cv_results['mean_f1'] = np.mean(cv_results['fold_f1_scores'])
    cv_results['std_f1'] = np.std(cv_results['fold_f1_scores'])

    print()
    f"   ğŸ“Š Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ù‚Ø©: {cv_results['mean_accuracy']:.3f} Â± {cv_results['std_accuracy']:.3f}}"
    )  # noqa: E501
    print()
    f"   ğŸ“Š Ù…ØªÙˆØ³Ø· F1: {cv_results['mean_f1']:.3f} Â± {cv_results['std_f1']:.3f}}"
    )  # noqa: E501

    return cv_results

    def generate_confusion_matrix()
    self, model_type: str = 'transformer', num_samples: int = 300
    ) -> Dict[str, Any]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø·"""

    print(f"ğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø· Ù„Ù†Ù…ÙˆØ°Ø¬ {model_type.upper()}...")

        from arabic_interrogative_pronouns_deep_model import ()
    create_synthetic_data)  # noqa: F401

        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    X, y_true = create_synthetic_data(self.inference_system.processor, num_samples)

        # Ø§Ù„ØªÙ†Ø¨Ø¤
    y_pred = []
        for x in X:
            # ØªØ­ÙˆÙŠÙ„ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    syllables = ["Ù…ÙÙ†Ù’"]  # Ù…Ø¨Ø³Ø·
    prediction = self.inference_system.predict_syllables(syllables, model_type)
    pred_id = PRONOUN_TO_ID.get(prediction, 0)
    y_pred.append(pred_id)

        # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø·
    cm = confusion_matrix(y_true, y_pred)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    labels = list(INTERROGATIVE_PRONOUNS.values())
    report = classification_report()
    y_true, y_pred, target_names=labels, output_dict=True
    )

        # Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø·
    plt.figure(figsize=(12, 10))
    sns.heatmap()
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=labels,
    yticklabels=labels)
    plt.title(f'Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø· - Ù†Ù…ÙˆØ°Ø¬ {model_type.upper()}')
    plt.xlabel('Ø§Ù„ØªÙ†Ø¨Ø¤')
    plt.ylabel('Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©')
    plt.tight_layout()
    plt.savefig(f'confusion_matrix_{model_type}.png', dpi=300, bbox_inches='tight')
    plt.show()

    return {
    'confusion_matrix': cm,
    'classification_report': report,
    'accuracy': accuracy_score(y_true, y_pred),
    }

    def benchmark_models(self) -> Dict[str, Any]:
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

    print("ğŸ† Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")

        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    model_results = self.inference_system.train_all_models(num_samples=1000)

        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡
    benchmark_results = {}

        for model_type in ['lstm', 'gru', 'transformer']:
    print(f"\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… {model_type.upper()}...")

            # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    basic_results = self.run_basic_tests([model_type])

            # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶ØºØ·
    stress_results = self.run_stress_tests(model_type, num_tests=100)

    benchmark_results[model_type] = {
    'training_accuracy': model_results[model_type]['final_train_acc'],
    'test_accuracy': model_results[model_type]['final_test_acc'],
    'basic_test_accuracy': basic_results[model_type]['overall_accuracy'],
    'stress_test_accuracy': stress_results['accuracy'],
    'prediction_speed': stress_results['predictions_per_second'],
    'avg_prediction_time': stress_results['avg_prediction_time'],
    }

        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    df = pd.DataFrame(benchmark_results).T
    print("\nğŸ“ˆ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:")
    print(df.round(4))

        # Ø±Ø³Ù… Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    axes[0, 0].bar()
    benchmark_results.keys(),
    [r['training_accuracy'] for r in benchmark_results.values()])
    axes[0, 0].set_title('Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨')
    axes[0, 0].set_ylim(0.8, 1.0)

        # Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    axes[0, 1].bar()
    benchmark_results.keys(),
    [r['test_accuracy'] for r in benchmark_results.values()])
    axes[0, 1].set_title('Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±')
    axes[0, 1].set_ylim(0.8, 1.0)

        # Ø³Ø±Ø¹Ø© Ø§Ù„ØªÙ†Ø¨Ø¤
    axes[1, 0].bar()
    benchmark_results.keys(),
    [r['prediction_speed'] for r in benchmark_results.values()])
    axes[1, 0].set_title('Ø³Ø±Ø¹Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ (ØªÙ†Ø¨Ø¤/Ø«Ø§Ù†ÙŠØ©)')

        # Ø²Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…ØªÙˆØ³Ø·
    axes[1, 1].bar()
    benchmark_results.keys(),
    [r['avg_prediction_time'] * 1000 for r in benchmark_results.values()])
    axes[1, 1].set_title('Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤ (Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©)')

    plt.tight_layout()
    plt.savefig('model_benchmark.png', dpi=300, bbox_inches='tight')
    plt.show()

    return benchmark_results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYSIS SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class InterrogativeAnalysisSystem:
    """Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):  # type: ignore[no-untyped def]
    """TODO: Add docstring."""
    self.test_suite = InterrogativePronounTestSuite()

    def full_analysis_report(self) -> Dict[str, Any]:
    """ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„"""

    print("ğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„...")

    report = {
    'timestamp': time.strftime('%Y-%m %d %H:%M:%S'),
    'system_info': self._get_system_info(),
    'basic_tests': {},
    'stress_tests': {},
    'cross_validation': {},
    'model_benchmark': {},
    'confusion_matrices': {},
    'recommendations': [],
    }

        # Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    report['basic_tests'] = self.test_suite.run_basic_tests()

        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶ØºØ·
        for model_type in ['transformer']:  # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    report['stress_tests'][model_type] = self.test_suite.run_stress_tests()
    model_type, 200
    )

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
    report['cross_validation'] = self.test_suite.run_cross_validation(num_folds=3)

        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    report['model_benchmark'] = self.test_suite.benchmark_models()

        # Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø®Ù„Ø·
        for model_type in ['transformer']:
    report['confusion_matrices'][model_type] = ()
    self.test_suite.generate_confusion_matrix(model_type, 200)
    )

        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
    report['recommendations'] = self._generate_recommendations(report)

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open()
    'interrogative_pronouns_analysis_report.json', 'w', encoding='utf 8'
    ) as f:
    json.dump(report, f, ensure_ascii=False, indent=2, default=str)

    print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: interrogative_pronouns_analysis_report.json")

    return report

    def _get_system_info(self) -> Dict[str, Any]:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""

    return {
    'torch_version': torch.__version__,
    'cuda_available': torch.cuda.is_available(),
    'device_count': ()
    torch.cuda.device_count() if torch.cuda.is_available() else 0
    ),
    'python_version': '3.13',
    'interrogative_pronouns_count': len(INTERROGATIVE_PRONOUNS),
    'phoneme_vocab_size': 56,
    }

    def _generate_recommendations(self, report: Dict[str, Any]) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""

    recommendations = []

        # ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    benchmark = report['model_benchmark']
    best_model = max(benchmark.keys(), key=lambda k: benchmark[k]['test_accuracy'])

    recommendations.append(f"ğŸ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„: {best_model.upper()}")

        # ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    basic_tests = report['basic_tests']
        for model, results in basic_tests.items():
            if results['overall_accuracy'] < 0.95:
    recommendations.append()
    f"âš ï¸ {model.upper()}: ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† (Ø¯Ù‚Ø©: {results['overall_accuracy']:.1%})"
    )
            elif results['overall_accuracy'] > 0.98:
    recommendations.append()
    f"âœ… {model.upper()}: Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² (Ø¯Ù‚Ø©: {results['overall_accuracy']:.1%})"
    )

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹
    cv_results = report['cross_validation']
        if cv_results['std_accuracy'] > 0.05:
    recommendations.append("ğŸ“Š Ø§Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠ - ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±Ø¹Ø©
        if best_model in benchmark:
    speed = benchmark[best_model]['prediction_speed']
            if speed > 100:
    recommendations.append(f"âš¡ Ø³Ø±Ø¹Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ù…ØªØ§Ø²Ø©: {speed:.1f} ØªÙ†Ø¨Ø¤/Ø«Ø§Ù†ÙŠØ©")
            elif speed < 50:
    recommendations.append("ğŸŒ Ø§Ù„Ø³Ø±Ø¹Ø© ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡Ø§")

    return recommendations


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():  # type: ignore[no-untyped def]
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""

    print("ğŸ§ª Ù†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± ÙˆØªØ­Ù„ÙŠÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 60)

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„
    analysis_system = InterrogativeAnalysisSystem()

    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„...")
    report = analysis_system.full_analysis_report()

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ
    print("\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print(f"   ğŸ¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {len(report['basic_tests'])} Ù†Ù…Ø§Ø°Ø¬")
    print("   âš¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¶ØºØ·: Ù…ÙƒØªÙ…Ù„Ø©")
    print()
    f"   ğŸ”„ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹: {report['cross_validation']['mean_accuracy']:.1%} Â± {report['cross_validation']['std_accuracy']:.1%}"
    )

    # Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØµÙŠØ§Øª
    print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
    for rec in report['recommendations']:
    print(f"   {rec}")

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„!")
    print("ğŸ“„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ø­ÙÙˆØ¸ ÙÙŠ: interrogative_pronouns_analysis_report.json")


if __name__ == "__main__":
    main()

