#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ”¥ Advanced Arabic Digital Vector Generator for Single Words
============================================================

A comprehensive algorithmic system for generating digital vectors for Arabic singular words
with advanced linguistic features including definiteness, case marking, gender agreement,
diminutive forms, prosodic patterns, irregular inflections, and semantic roles.

Ø§Ù„Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©
Ù†Ø¸Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠ Ø´Ø§Ù…Ù„ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©
Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

Author: GitHub Copilot (Advanced Arabic NLP Expert)
Version: 3.0 (Comprehensive Linguistic Analysis)
Date: 2024
"""
# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line too long


import re
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import logging

# Configure logging
logging.basicConfig()
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DefinitenesType(Enum):
    """ØªØµÙ†ÙŠÙ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ"""

    DEFINITE = 0  # Ø§Ù„ÙƒØªØ§Ø¨ - Ù…Ø¹Ø±ÙØ©
    INDEFINITE = 1  # ÙƒØªØ§Ø¨ - Ù†ÙƒØ±Ø©
    PROPER_NOUN = 2  # Ù…Ø­Ù…Ø¯ - Ø¹Ù„Ù…
    PRONOUN = 3  # Ù‡Ùˆ - Ø¶Ù…ÙŠØ±


class CaseMarking(Enum):
    """Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨"""

    NOMINATIVE = 0  # Ø§Ù„ÙØ§Ø¹Ù„ - Ù…Ø±ÙÙˆØ¹
    ACCUSATIVE = 1  # Ø§Ù„Ù…ÙØ¹ÙˆÙ„ - Ù…Ù†ØµÙˆØ¨
    GENITIVE = 2  # Ø§Ù„Ù…Ø¶Ø§Ù Ø¥Ù„ÙŠÙ‡ - Ù…Ø¬Ø±ÙˆØ±
    UNDEFINED = 3  # Ø¨Ø¯ÙˆÙ† Ø¥Ø¹Ø±Ø§Ø¨ ÙˆØ§Ø¶Ø­


class Gender(Enum):
    """Ø§Ù„Ø¬Ù†Ø¯Ø± Ø§Ù„Ù†Ø­ÙˆÙŠ"""

    MASCULINE = 0  # Ù…Ø°ÙƒØ±
    FEMININE = 1  # Ù…Ø¤Ù†Ø«
    COMMON = 2  # Ù…Ø´ØªØ±Ùƒ


class Number(Enum):
    """Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø­ÙˆÙŠ"""

    SINGULAR = 0  # Ù…ÙØ±Ø¯
    DUAL = 1  # Ù…Ø«Ù†Ù‰
    PLURAL = 2  # Ø¬Ù…Ø¹


class GenitiveType(Enum):
    """Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø¶Ø§ÙØ©"""

    NO_GENITIVE = 0  # Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ©
    TRUE_GENITIVE = 1  # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© - Ø¨ÙŠØª Ø§Ù„Ø·Ø§Ù„Ø¨
    FALSE_GENITIVE = 2  # Ø¥Ø¶Ø§ÙØ© Ù…Ø¬Ø§Ø²ÙŠØ© - ÙƒØ«ÙŠØ± Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡


class DiminutiveForm(Enum):
    """Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªØµØºÙŠØ±"""

    NO_DIMINUTIVE = 0  # Ø¨Ø¯ÙˆÙ† ØªØµØºÙŠØ±
    FUAIL = 1  # ÙÙØ¹ÙÙŠÙ’Ù„ - ÙƒÙØªÙÙŠÙ’Ø¨
    FUAILA = 2  # ÙÙØ¹ÙÙŠÙ’Ù„ÙØ© - Ø¨ÙÙ†ÙÙŠÙÙ‘Ø©
    FUAIIL = 3  # ÙÙØ¹ÙÙŠÙ’Ø¹ÙÙ„ - Ø¯ÙØ±ÙÙŠÙ’Ù‡ÙÙ…


class SemanticRole(Enum):
    """Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©"""

    AGENT = 0  # ÙØ§Ø¹Ù„ Ø¯Ù„Ø§Ù„ÙŠ - Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„ÙØ¹Ù„
    PATIENT = 1  # Ù…ÙØ¹ÙˆÙ„ Ø¯Ù„Ø§Ù„ÙŠ - Ø§Ù„Ø°ÙŠ ÙŠØªØ£Ø«Ø± Ø¨Ø§Ù„ÙØ¹Ù„
    INSTRUMENT = 2  # Ø£Ø¯Ø§Ø© - ÙˆØ³ÙŠÙ„Ø© Ø§Ù„ÙØ¹Ù„
    LOCATION = 3  # Ù…ÙƒØ§Ù† - Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø¯Ø«
    TIME = 4  # Ø²Ù…Ø§Ù† - ÙˆÙ‚Øª Ø§Ù„Ø­Ø¯Ø«
    MANNER = 5  # Ø·Ø±ÙŠÙ‚Ø© - ÙƒÙŠÙÙŠØ© Ø§Ù„ÙØ¹Ù„


@dataclass
class PhonologicalVector:
    """Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""

    phonemes: List[str]  # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª
    syllabic_structure: List[str]  # Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ© CV
    stress_pattern: List[int]  # Ù†Ù…Ø· Ø§Ù„Ù†Ø¨Ø± (0=ØºÙŠØ± Ù…Ù†Ø¨ÙˆØ±ØŒ 1=Ù…Ù†Ø¨ÙˆØ± Ø«Ø§Ù†ÙˆÙŠØŒ 2=Ù…Ù†Ø¨ÙˆØ± Ø£Ø³Ø§Ø³ÙŠ)
    emphatic_spreading: List[bool]  # Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙØ®ÙŠÙ…
    length_pattern: List[int]  # Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª (1=Ù‚ØµÙŠØ±ØŒ 2=Ø·ÙˆÙŠÙ„)


@dataclass
class MorphologicalVector:
    """Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„ØµØ±ÙÙŠ"""

    root: str  # Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø£Ùˆ Ø§Ù„Ø±Ø¨Ø§Ø¹ÙŠ
    pattern: str  # Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ
    prefixes: List[str]  # Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
    suffixes: List[str]  # Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
    stem: str  # Ø§Ù„Ø¬Ø°Ø¹
    derivational_morphemes: List[str]  # Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ÙŠØ©


@dataclass
class SyntacticVector:
    """Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ù†Ø­ÙˆÙŠ"""

    definiteness: DefinitenesType  # Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ
    case_marking: CaseMarking  # Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨
    gender: Gender  # Ø§Ù„Ø¬Ù†Ø¯Ø±
    number: Number  # Ø§Ù„Ø¹Ø¯Ø¯
    genitive_type: GenitiveType  # Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø¶Ø§ÙØ©
    is_vocative: bool  # Ø§Ù„Ù…Ù†Ø§Ø¯Ù‰
    construct_state: bool  # Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ©


@dataclass
class SemanticVector:
    """Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""

    semantic_role: SemanticRole  # Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    semantic_class: str  # Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© (concrete/abstract)
    animacy: str  # Ø§Ù„Ø­ÙŠÙˆÙŠØ© (animate/inanimate)
    countability: str  # Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ø¹Ø¯ (count/mass)
    semantic_features: Dict[str, float]  # Ù…ÙŠØ²Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©


@dataclass
class AdvancedFeatures:
    """Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""

    diminutive_form: DiminutiveForm  # Ø´ÙƒÙ„ Ø§Ù„ØªØµØºÙŠØ±
    irregular_inflection: bool  # Ø§Ù„ØªØµØ±ÙŠÙ Ø§Ù„Ø´Ø§Ø°
    hamza_type: Optional[str]  # Ù†ÙˆØ¹ Ø§Ù„Ù‡Ù…Ø²Ø© (ÙˆØµÙ„/Ù‚Ø·Ø¹)
    assimilation_effects: List[str]  # ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¯ØºØ§Ù…
    prosodic_breaks: List[int]  # Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©


class ArabicDigitalVectorGenerator:
    """
    ğŸ¯ Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©

    Features Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (INCLUDED):
    âœ… Ø§Ù„ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø¹Ø±ÙÙŠ (definiteness) - Ø§Ù„Ù€ØŒ Ù†ÙƒØ±Ø©ØŒ Ø¹Ù„Ù…ØŒ Ø¶Ù…ÙŠØ±
    âœ… Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ø¥Ø¹Ø±Ø§Ø¨ - Ù…Ø±ÙÙˆØ¹ØŒ Ù…Ù†ØµÙˆØ¨ØŒ Ù…Ø¬Ø±ÙˆØ±
    âœ… Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù„Ø§Ù… - Ø­Ø±ÙˆÙ Ø´Ù…Ø³ÙŠØ© ÙˆÙ‚Ù…Ø±ÙŠØ©
    âœ… Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ø­ÙˆÙŠØ© - Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ…Ø¬Ø§Ø²ÙŠØ©
    âœ… Ø§Ù„Ø¬Ù†Ø¯Ø± ÙˆØ§Ù„Ø§ØªÙØ§Ù‚ Ø§Ù„ØµØ±ÙÙŠ - Ù…Ø°ÙƒØ±/Ù…Ø¤Ù†Ø«/Ù…Ø´ØªØ±Ùƒ
    âœ… Ø§Ù„ØªØµØºÙŠØ± - ÙÙØ¹ÙÙŠÙ’Ù„ØŒ ÙÙØ¹ÙÙŠÙ’Ù„ÙØ©ØŒ ÙÙØ¹ÙÙŠÙ’Ø¹ÙÙ„
    âœ… Ø§Ù„Ù†Ø¨Ø± ÙˆØ§Ù„Ø¹Ø±ÙˆØ¶ - stress patternsØŒ Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
    âœ… Ø§Ù„ØªØµØ±ÙŠÙ Ø§Ù„Ø´Ø§Ø° - Ø§Ù„Ø£ÙØ¹Ø§Ù„ ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø´Ø§Ø°Ø©
    âœ… Ø§Ù„ØªØ«Ù†ÙŠØ© ÙˆØ§Ù„Ø¬Ù…Ø¹ - ÙƒØ§Ù…ØªØ¯Ø§Ø¯ Ù„Ù„Ù…ÙØ±Ø¯
    âœ… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© - Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    âœ… Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© - Ù‡Ù…Ø² Ø§Ù„ÙˆØµÙ„ØŒ Ø§Ù„Ø¥Ø¯ØºØ§Ù…ØŒ Ø§Ù„Ø­Ø°Ù
    âœ… Ø§Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠØ© - Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ML Ù„Ù„ØªÙ†Ø¨Ø¤

    Features Ø§Ù„Ù…Ø³ØªØ«Ù†Ø§Ø© (EXCLUDED Ù…Ù† Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ):
    âŒ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ - ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©
    âŒ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù…Ù„ - Ø®Ø§Ø±Ø¬ Ù†Ø·Ø§Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØ±Ø¯Ø©
    âŒ Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© Ø§Ù„Ù…ØªØºÙŠØ±Ø© - ØªØ­ØªØ§Ø¬ corpus analysis
    âŒ Ø§Ù„ØªÙ†ØºÙŠÙ… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ - ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚
    """

    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ"""
        self._initialize_linguistic_resources()
        self._initialize_ml_models()
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©")

    def _initialize_linguistic_resources(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù„ØºÙˆÙŠØ©"""

        # 1. Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
        # Replaced with unified_phonemes
            "Ø¨": {
                "place": "bilabial",
                "manner": "stop",
                "voice": True,
                "emphatic": False,
            },
            "Øª": {
                "place": "alveolar",
                "manner": "stop",
                "voice": False,
                "emphatic": False,
            },
            "Ø«": {
                "place": "dental",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ø¬": {
                "place": "postalveolar",
                "manner": "affricate",
                "voice": True,
                "emphatic": False,
            },
            "Ø­": {
                "place": "pharyngeal",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ø®": {
                "place": "uvular",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ø¯": {
                "place": "alveolar",
                "manner": "stop",
                "voice": True,
                "emphatic": False,
            },
            "Ø°": {
                "place": "dental",
                "manner": "fricative",
                "voice": True,
                "emphatic": False,
            },
            "Ø±": {
                "place": "alveolar",
                "manner": "trill",
                "voice": True,
                "emphatic": False,
            },
            "Ø²": {
                "place": "alveolar",
                "manner": "fricative",
                "voice": True,
                "emphatic": False,
            },
            "Ø³": {
                "place": "alveolar",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ø´": {
                "place": "postalveolar",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Øµ": {
                "place": "alveolar",
                "manner": "fricative",
                "voice": False,
                "emphatic": True,
            },
            "Ø¶": {
                "place": "alveolar",
                "manner": "stop",
                "voice": True,
                "emphatic": True,
            },
            "Ø·": {
                "place": "alveolar",
                "manner": "stop",
                "voice": False,
                "emphatic": True,
            },
            "Ø¸": {
                "place": "dental",
                "manner": "fricative",
                "voice": True,
                "emphatic": True,
            },
            "Ø¹": {
                "place": "pharyngeal",
                "manner": "fricative",
                "voice": True,
                "emphatic": False,
            },
            "Øº": {
                "place": "uvular",
                "manner": "fricative",
                "voice": True,
                "emphatic": False,
            },
            "Ù": {
                "place": "labiodental",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ù‚": {
                "place": "uvular",
                "manner": "stop",
                "voice": False,
                "emphatic": False,
            },
            "Ùƒ": {
                "place": "velar",
                "manner": "stop",
                "voice": False,
                "emphatic": False,
            },
            "Ù„": {
                "place": "alveolar",
                "manner": "lateral",
                "voice": True,
                "emphatic": False,
            },
            "Ù…": {
                "place": "bilabial",
                "manner": "nasal",
                "voice": True,
                "emphatic": False,
            },
            "Ù†": {
                "place": "alveolar",
                "manner": "nasal",
                "voice": True,
                "emphatic": False,
            },
            "Ù‡": {
                "place": "glottal",
                "manner": "fricative",
                "voice": False,
                "emphatic": False,
            },
            "Ùˆ": {
                "place": "labiovelar",
                "manner": "approximant",
                "voice": True,
                "emphatic": False,
            },
            "ÙŠ": {
                "place": "palatal",
                "manner": "approximant",
                "voice": True,
                "emphatic": False,
            },
            "Ø¡": {
                "place": "glottal",
                "manner": "stop",
                "voice": False,
                "emphatic": False,
            },
        }

        # 2. Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø´Ù…Ø³ÙŠØ© ÙˆØ§Ù„Ù‚Ù…Ø±ÙŠØ©
        self.sun_letters = {
            "Øª",
            "Ø«",
            "Ø¯",
            "Ø°",
            "Ø±",
            "Ø²",
            "Ø³",
            "Ø´",
            "Øµ",
            "Ø¶",
            "Ø·",
            "Ø¸",
            "Ù„",
            "Ù†",
        }
        self.moon_letters = {
            "Ø¡",
            "Ø¨",
            "Ø¬",
            "Ø­",
            "Ø®",
            "Ø¹",
            "Øº",
            "Ù",
            "Ù‚",
            "Ùƒ",
            "Ù…",
            "Ù‡",
            "Ùˆ",
            "ÙŠ",
        }

        # 3. Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØµØºÙŠØ±
        self.diminutive_patterns = {
            "ÙÙØ¹ÙÙŠÙ’Ù„": r"^(.)(.)(.?)$",  # ÙƒØªØ§Ø¨ â†’ ÙƒÙØªÙÙŠÙ’Ø¨
            "ÙÙØ¹ÙÙŠÙ’Ù„ÙØ©": r"^(.)(.)(.?)Ø©$",  # Ø¨Ù†Øª â†’ Ø¨ÙÙ†ÙÙŠÙÙ‘Ø©
            "ÙÙØ¹ÙÙŠÙ’Ø¹ÙÙ„": r"^(.)(.)(.)(.)$",  # Ø¯Ø±Ù‡Ù… â†’ Ø¯ÙØ±ÙÙŠÙ’Ù‡ÙÙ…
        }

        # 4. Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        self.common_roots = {
            "ÙƒØªØ¨": {"meaning": "writing", "type": "trilateral"},
            "Ø¯Ø±Ø³": {"meaning": "studying", "type": "trilateral"},
            "Ø¹Ù„Ù…": {"meaning": "knowledge", "type": "trilateral"},
            "Ù‚Ø±Ø£": {"meaning": "reading", "type": "trilateral"},
        }

        # 5. Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØµØ±ÙÙŠØ©
        self.morphological_patterns = {
            "ÙØ§Ø¹Ù„": {"type": "active_participle", "form": "I"},
            "Ù…ÙØ¹ÙˆÙ„": {"type": "passive_participle", "form": "I"},
            "Ù…ÙÙØ§Ø¹ÙÙ„": {"type": "active_participle", "form": "III"},
            "Ù…ÙØªÙÙØ§Ø¹ÙÙ„": {"type": "active_participle", "form": "VI"},
            "Ù…ÙØ³ØªÙÙØ¹ÙÙ„": {"type": "active_participle", "form": "X"},
        }

        # 6. Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„ØªÙ†ÙˆÙŠÙ†
        self.diacritics = {
            "Ù": {"name": "fatha", "length": 1, "type": "short"},
            "Ù": {"name": "kasra", "length": 1, "type": "short"},
            "Ù": {"name": "damma", "length": 1, "type": "short"},
            "Ø§": {"name": "alif", "length": 2, "type": "long"},
            "Ùˆ": {"name": "waw", "length": 2, "type": "long"},
            "ÙŠ": {"name": "ya", "length": 2, "type": "long"},
            "Ù‹": {"name": "tanween_fath", "length": 2, "type": "nunation"},
            "Ù": {"name": "tanween_kasr", "length": 2, "type": "nunation"},
            "ÙŒ": {"name": "tanween_damm", "length": 2, "type": "nunation"},
        }

    def _initialize_ml_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤"""
        # Ø³ÙŠØªÙ… ØªØ·ÙˆÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§Ø­Ù‚Ø§Ù‹
        self.stress_predictor = None
        self.gender_predictor = None
        self.semantic_classifier = None
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ (ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©)")

    def generate_digital_vector()
        self, word: str, context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©

        Args:
            word: Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡Ø§
            context: Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

        Returns:
            Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ
        """
        logger.info(f"Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}")

        try:
            # 1. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            phonological_vector = self._analyze_phonology(word)

            # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ
            morphological_vector = self._analyze_morphology(word)

            # 3. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ
            syntactic_vector = self._analyze_syntax(word, context)

            # 4. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
            semantic_vector = self._analyze_semantics(word, context)

            # 5. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            advanced_features = self._analyze_advanced_features(word)

            # 6. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ Ø±Ù‚Ù…ÙŠ Ù…ÙˆØ­Ø¯
            numerical_vector = self._convert_to_numerical_vector()
                phonological_vector,
                morphological_vector,
                syntactic_vector,
                semantic_vector,
                advanced_features)

            # 7. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
            comprehensive_analysis = {
                "word": word,
                "timestamp": datetime.now().isoformat(),
                "phonological_vector": phonological_vector,
                "morphological_vector": morphological_vector,
                "syntactic_vector": syntactic_vector,
                "semantic_vector": semantic_vector,
                "advanced_features": advanced_features,
                "numerical_vector": numerical_vector,
                "vector_dimensions": len(numerical_vector),
                "processing_status": "success",
            }

            logger.info()
                f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø¨Ù†Ø¬Ø§Ø­ - Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯: {len(numerical_vector)}"
            )
            return comprehensive_analysis

        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø© {word: {str(e)}}")
            return {"word": word, "error": str(e), "processing_status": "error"}

    def _analyze_phonology(self, word: str) -> PhonologicalVector:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª
        phonemes = self._extract_phonemes(word)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©
        syllabic_structure = self._analyze_syllabic_structure(word)

        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„Ù†Ø¨Ø±
        stress_pattern = self._predict_stress_pattern(syllabic_structure)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙØ®ÙŠÙ…
        emphatic_spreading = self._analyze_emphatic_spreading(phonemes)

        # ØªØ­Ø¯ÙŠØ¯ Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª
        length_pattern = self._analyze_length_pattern(word)

        return PhonologicalVector()
            phonemes=phonemes,
            syllabic_structure=syllabic_structure,
            stress_pattern=stress_pattern,
            emphatic_spreading=emphatic_spreading,
            length_pattern=length_pattern)

    def _analyze_morphology(self, word: str) -> MorphologicalVector:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        root = self._extract_root(word)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù†
        pattern = self._identify_pattern(word, root)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚
        prefixes, stem, suffixes = self._analyze_affixes(word)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ÙŠØ©
        derivational_morphemes = self._extract_derivational_morphemes(word)

        return MorphologicalVector()
            root=root,
            pattern=pattern,
            prefixes=prefixes,
            suffixes=suffixes,
            stem=stem,
            derivational_morphemes=derivational_morphemes)

    def _analyze_syntax()
        self, word: str, context: Optional[Dict] = None
    ) -> SyntacticVector:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø­ÙˆÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""

        # ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ
        definiteness = self._determine_definiteness(word)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨ (Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¥Ù† Ø£Ù…ÙƒÙ†)
        case_marking = self._determine_case_marking(word, context)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø¯Ø±
        gender = self._determine_gender(word)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯
        number = self._determine_number(word)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¶Ø§ÙØ©
        genitive_type = self._analyze_genitive_construction(word, context)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†Ø§Ø¯Ù‰
        is_vocative = self._is_vocative(word, context)

        # Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ©
        construct_state = self._is_construct_state(word, context)

        return SyntacticVector()
            definiteness=definiteness,
            case_marking=case_marking,
            gender=gender,
            number=number,
            genitive_type=genitive_type,
            is_vocative=is_vocative,
            construct_state=construct_state)

    def _analyze_semantics()
        self, word: str, context: Optional[Dict] = None
    ) -> SemanticVector:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_role = self._determine_semantic_role(word, context)

        # ØªØµÙ†ÙŠÙ Ø¯Ù„Ø§Ù„ÙŠ
        semantic_class = self._classify_semantically(word)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­ÙŠÙˆÙŠØ©
        animacy = self._determine_animacy(word)

        # Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ø¹Ø¯
        countability = self._determine_countability(word)

        # Ù…ÙŠØ²Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©
        semantic_features = self._extract_semantic_features(word)

        return SemanticVector()
            semantic_role=semantic_role,
            semantic_class=semantic_class,
            animacy=animacy,
            countability=countability,
            semantic_features=semantic_features)

    def _analyze_advanced_features(self, word: str) -> AdvancedFeatures:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""

        # ØªØ­Ø¯ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„ØªØµØºÙŠØ±
        diminutive_form = self._identify_diminutive_form(word)

        # ÙƒØ´Ù Ø§Ù„ØªØµØ±ÙŠÙ Ø§Ù„Ø´Ø§Ø°
        irregular_inflection = self._is_irregular_inflection(word)

        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù‡Ù…Ø²Ø©
        hamza_type = self._analyze_hamza_type(word)

        # ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¯ØºØ§Ù…
        assimilation_effects = self._analyze_assimilation(word)

        # Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©
        prosodic_breaks = self._analyze_prosodic_breaks(word)

        return AdvancedFeatures()
            diminutive_form=diminutive_form,
            irregular_inflection=irregular_inflection,
            hamza_type=hamza_type,
            assimilation_effects=assimilation_effects,
            prosodic_breaks=prosodic_breaks)

    def _convert_to_numerical_vector()
        self,
        phonological: PhonologicalVector,
        morphological: MorphologicalVector,
        syntactic: SyntacticVector,
        semantic: SemanticVector,
        advanced: AdvancedFeatures) -> List[float]:
        """ØªØ­ÙˆÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡ Ø±Ù‚Ù…ÙŠ Ù…ÙˆØ­Ø¯"""

        vector = []

        # 1. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© (40 Ø¨ÙØ¹Ø¯)
        vector.extend(self._encode_phonological_features(phonological))

        # 2. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ© (30 Ø¨ÙØ¹Ø¯)
        vector.extend(self._encode_morphological_features(morphological))

        # 3. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ© (20 Ø¨ÙØ¹Ø¯)
        vector.extend(self._encode_syntactic_features(syntactic))

        # 4. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© (25 Ø¨ÙØ¹Ø¯)
        vector.extend(self._encode_semantic_features(semantic))

        # 5. Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (15 Ø¨ÙØ¹Ø¯)
        vector.extend(self._encode_advanced_features(advanced))

        return vector

    # ============== Helper Methods ==============

    def _extract_phonemes(self, word: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©"""
        phonemes = []
        for char in word:
            if self.unified_phonemes.get_phoneme(char) is not None:
                phonemes.append(char)
        return phonemes

    def _analyze_syllabic_structure(self, word: str) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©"""
        # ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø· - ÙŠØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ± Ø£ÙƒØ«Ø±
        syllabic_units = []
        current_syllable = ""

        for char in word:
            if self.unified_phonemes.get_phoneme(char) is not None:
                if self.get_phoneme(char].get("manner") in [
                    "stop",
                    "fricative",
                    "affricate",
                ]:
                    current_syllable += "C"
                else:
                    current_syllable += "V"
            elif char in ["Ù", "Ù", "Ù"]:
                current_syllable += "V"
            elif char in ["Ø§", "Ùˆ", "ÙŠ"]:
                current_syllable += "V"

        if current_syllable:
            syllabic_units.append(current_syllable)

        return syllabic_units

    def _predict_stress_pattern(self, syllabic_structure: List[str]) -> List[int]:
        """ØªÙ†Ø¨Ø¤ Ù†Ù…Ø· Ø§Ù„Ù†Ø¨Ø±"""
        # Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¨Ø³Ø·Ø©: Ø§Ù„Ù†Ø¨Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ø£Ø®ÙŠØ± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø«Ù‚ÙŠÙ„Ø§Ù‹ØŒ ÙˆØ¥Ù„Ø§ Ø¹Ù„Ù‰ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø®ÙŠØ±
        stress = [0] * len(syllabic_structure)

        if syllabic_structure:
            if len(syllabic_structure[-1]) > 2:  # Ù…Ù‚Ø·Ø¹ Ø«Ù‚ÙŠÙ„
                stress[-1] = 2  # Ù†Ø¨Ø± Ø£Ø³Ø§Ø³ÙŠ
            elif len(len(syllabic_structure)  > 1) > 1:
                stress[-2] = 2  # Ù†Ø¨Ø± Ø¹Ù„Ù‰ Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø®ÙŠØ±
            else:
                stress[ 1] = 2

        return stress

    def _analyze_emphatic_spreading(self, phonemes: List[str]) -> List[bool]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙØ®ÙŠÙ…"""
        spreading = [False] * len(phonemes)

        for i, phoneme in enumerate(phonemes):
            if phoneme in self.phonemes and self.get_phoneme(phoneme].get())
                "emphatic", False
            ):
                # Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØªÙØ®ÙŠÙ… Ø¥Ù„Ù‰ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø©
                spreading[i] = True
                if i > 0:
                    spreading[i - 1] = True
                if i < len(phonemes) - 1:
                    spreading[i + 1] = True

        return spreading

    def _analyze_length_pattern(self, word: str) -> List[int]:
        """ØªØ­Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª"""
        lengths = []
        for char in word:
            if char in self.diacritics:
                lengths.append(self.diacritics[char]["length"])
            else:
                lengths.append(1)  # Ø·ÙˆÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return lengths

    def _extract_root(self, word: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±"""
        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø¨Ø³Ø·Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        # Ø¥Ø²Ø§Ù„Ø© Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ
        clean_word = word
        if clean_word.startswith("Ø§Ù„"):
            clean_word = clean_word[2:]

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        suffixes = ["Ø©", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "ÙˆÙ†"]
        for suffix in suffixes:
            if clean_word.endswith(suffix):
                clean_word = clean_word[:  len(suffix)]
                break

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ© (ØªØ¨Ø³ÙŠØ·)
        consonants = [
            c
            for c in clean_word
            if c in self.phonemes
            and self.get_phoneme(c].get("manner")
            in ["stop", "fricative", "affricate", "nasal"]
        ]

        return "".join(consonants[:3])  # Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ

    def _identify_pattern(self, word: str, root: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµØ±ÙÙŠ"""
        # ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø·
        if word.startswith("Ù…"):
            return "Ù…ÙØ¹ÙˆÙ„"
        elif "Ø§" in word:
            return "ÙØ§Ø¹Ù„"
        else:
            return "ÙØ¹Ù„"

    def _analyze_affixes(self, word: str) -> Tuple[List[str], str, List[str]]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª ÙˆØ§Ù„Ø¬Ø°Ø¹ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚"""
        prefixes = []
        suffixes = []
        stem = word

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
        if word.startswith("Ø§Ù„"):
            prefixes.append("Ø§Ù„")
            stem = stem[2:]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        common_suffixes = ["Ø©", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "ÙˆÙ†", "Ù‡Ø§", "ÙƒÙ…", "Ù‡Ù…"]
        for suffix in common_suffixes:
            if stem.endswith(suffix):
                suffixes.append(suffix)
                stem = stem[:  len(suffix)]
                break

        return prefixes, stem, suffixes

    def _extract_derivational_morphemes(self, word: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ÙŠØ©"""
        morphemes = []
        if word.startswith("Ù…Ù"):
            morphemes.append("Ù…Ù ")  # Ù…ÙˆØ±ÙÙŠÙ… Ø§Ø³Ù… Ø§Ù„ÙØ§Ø¹Ù„
        if word.startswith("Ù…Ù"):
            morphemes.append("Ù…Ù ")  # Ù…ÙˆØ±ÙÙŠÙ… Ø§Ø³Ù… Ø§Ù„Ù…ÙØ¹ÙˆÙ„
        return morphemes

    def _determine_definiteness(self, word: str) -> DefinitenesType:
        """ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙ"""
        if word.startswith("Ø§Ù„"):
            return DefinitenesType.DEFINITE
        elif word in ["Ù‡Ùˆ", "Ù‡ÙŠ", "Ø£Ù†Øª", "Ø£Ù†Ø§", "Ù†Ø­Ù†"]:
            return DefinitenesType.PRONOUN
        elif word[0].isupper():  # Ø§Ø³Ù… Ø¹Ù„Ù… (ØªØ¨Ø³ÙŠØ·)
            return DefinitenesType.PROPER_NOUN
        else:
            return DefinitenesType.INDEFINITE

    def _determine_case_marking()
        self, word: str, context: Optional[Dict] = None
    ) -> CaseMarking:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨"""
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†ÙˆÙŠÙ† ÙˆØ§Ù„Ø³ÙŠØ§Ù‚
        if word.endswith("ÙŒ") or word.endswith("Ù"):
            return CaseMarking.NOMINATIVE
        elif word.endswith("Ù‹") or word.endswith("Ù"):
            return CaseMarking.ACCUSATIVE
        elif word.endswith("Ù") or word.endswith("Ù"):
            return CaseMarking.GENITIVE
        else:
            return CaseMarking.UNDEFINED

    def _determine_gender(self, word: str) -> Gender:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø¯Ø±"""
        if word.endswith("Ø©") or word.endswith("Ø§Ø¡"):
            return Gender.FEMININE
        else:
            return Gender.MASCULINE  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

    def _determine_number(self, word: str) -> Number:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯"""
        if word.endswith("Ø§Ù†") or word.endswith("ÙŠÙ†"):
            return Number.DUAL
        elif word.endswith("ÙˆÙ†") or word.endswith("Ø§Øª"):
            return Number.PLURAL
        else:
            return Number.SINGULAR

    def _analyze_genitive_construction()
        self, word: str, context: Optional[Dict] = None
    ) -> GenitiveType:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¶Ø§ÙØ©"""
        # ÙŠØ­ØªØ§Ø¬ Ø³ÙŠØ§Ù‚ Ù„Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
        return GenitiveType.NO_GENITIVE

    def _is_vocative(self, word: str, context: Optional[Dict] = None) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†Ø§Ø¯Ù‰"""
        if context and context.get("preceded_by_ya", False):
            return True
        return False

    def _is_construct_state(self, word: str, context: Optional[Dict] = None) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ©"""
        if context and context.get("followed_by_genitive", False):
            return True
        return False

    def _determine_semantic_role()
        self, word: str, context: Optional[Dict] = None
    ) -> SemanticRole:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø·
        if context:
            if context.get("position") == "subject":
                return SemanticRole.AGENT
            elif context.get("position") == "object":
                return SemanticRole.PATIENT
        return SemanticRole.AGENT  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

    def _classify_semantically(self, word: str) -> str:
        """Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        abstract_indicators = ["ÙÙƒØ±", "Ø¹Ù„Ù…", "Ø­Ø¨", "Ø®ÙˆÙ"]
        if any(indicator in word for indicator in abstract_indicators):
            return "abstract"
        return "concrete"

    def _determine_animacy(self, word: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­ÙŠÙˆÙŠØ©"""
        animate_words = ["Ø±Ø¬Ù„", "Ø§Ù…Ø±Ø£Ø©", "Ø·ÙÙ„", "Ø­ÙŠÙˆØ§Ù†"]
        if word in animate_words:
            return "animate"
        return "inanimate"

    def _determine_countability(self, word: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ø¹Ø¯"""
        mass_words = ["Ù…Ø§Ø¡", "Ù‡ÙˆØ§Ø¡", "ØªØ±Ø§Ø¨"]
        if word in mass_words:
            return "mass"
        return "count"

    def _extract_semantic_features(self, word: str) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©"""
        return {
            "concreteness": 0.7,
            "imageability": 0.6,
            "familiarity": 0.8,
            "age_of_acquisition": 0.5,
        }

    def _identify_diminutive_form(self, word: str) -> DiminutiveForm:
        """ØªØ­Ø¯ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„ØªØµØºÙŠØ±"""
        for form, pattern in self.diminutive_patterns.items():
            if re.match(pattern, word):
                if form == "ÙÙØ¹ÙÙŠÙ’Ù„":
                    return DiminutiveForm.FUAIL
                elif form == "ÙÙØ¹ÙÙŠÙ’Ù„ÙØ©":
                    return DiminutiveForm.FUAILA
                elif form == "ÙÙØ¹ÙÙŠÙ’Ø¹ÙÙ„":
                    return DiminutiveForm.FUAIIL
        return DiminutiveForm.NO_DIMINUTIVE

    def _is_irregular_inflection(self, word: str) -> bool:
        """ÙƒØ´Ù Ø§Ù„ØªØµØ±ÙŠÙ Ø§Ù„Ø´Ø§Ø°"""
        irregular_words = ["Ø£Ø¨", "Ø£Ø®", "Ø­Ù…", "ÙÙ…"]
        return word in irregular_words

    def _analyze_hamza_type(self, word: str) -> Optional[str]:
        """ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù‡Ù…Ø²Ø©"""
        if word.startswith("Ø§"):
            return "ÙˆØµÙ„"
        elif "Ø¡" in word:
            return "Ù‚Ø·Ø¹"
        return None

    def _analyze_assimilation(self, word: str) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¯ØºØ§Ù…"""
        effects = []
        if word.startswith("Ø§Ù„"):
            first_letter = word[2] if len(word) > 2 else ""
            if first_letter in self.sun_letters:
                effects.append(f"Ø¥Ø¯ØºØ§Ù… Ø§Ù„Ù„Ø§Ù… ÙÙŠ {first_letter}")
        return effects

    def _analyze_prosodic_breaks(self, word: str) -> List[int]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©"""
        # ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        breaks = []
        syllable_count = len(self._analyze_syllabic_structure(word))
        if syllable_count > 2:
            breaks.append(syllable_count // 2)  # ÙˆÙ‚ÙØ© ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
        return breaks

    def _encode_phonological_features()
        self, phonological: PhonologicalVector
    ) -> List[float]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©"""
        features = []

        # Ø¹Ø¯Ø¯ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª
        features.append(len(phonological.phonemes))

        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙØ®Ù…Ø©
        emphatic_ratio = ()
            sum(phonological.emphatic_spreading) / len(phonological.emphatic_spreading)
            if phonological.emphatic_spreading
            else 0
        )
        features.append(emphatic_ratio)

        # Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙˆØ§Øª
        avg_length = ()
            sum(phonological.length_pattern) / len(phonological.length_pattern)
            if phonological.length_pattern
            else 0
        )
        features.append(avg_length)

        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
        features.append(len(phonological.syllabic_structure))

        # Ù†Ù…Ø· Ø§Ù„Ù†Ø¨Ø± (Ù…Ø±Ù…Ø²)
        stress_encoded = [0] * 5  # Ø£Ù‚ØµÙ‰ 5 Ù…Ù‚Ø§Ø·Ø¹
        for i, stress in enumerate(phonological.stress_pattern[:5]):
            stress_encoded[i] = stress
        features.extend(stress_encoded)

        # Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø¥ÙƒÙ…Ø§Ù„ 40 Ø¨ÙØ¹Ø¯
        features.extend([0] * (40 - len(features)))

        return features[:40]

    def _encode_morphological_features()
        self, morphological: MorphologicalVector
    ) -> List[float]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©"""
        features = []

        # Ø·ÙˆÙ„ Ø§Ù„Ø¬Ø°Ø±
        features.append(len(morphological.root))

        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
        features.append(len(morphological.prefixes))

        # Ø¹Ø¯Ø¯ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        features.append(len(morphological.suffixes))

        # Ø·ÙˆÙ„ Ø§Ù„Ø¬Ø°Ø¹
        features.append(len(morphological.stem))

        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ±ÙÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚ÙŠØ©
        features.append(len(morphological.derivational_morphemes))

        # ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ²Ù† (one hot Ù…Ø¨Ø³Ø·)
        common_patterns = ["ÙØ¹Ù„", "ÙØ§Ø¹Ù„", "Ù…ÙØ¹ÙˆÙ„", "Ù…ÙÙØ§Ø¹ÙÙ„"]
        pattern_encoded = [
            1 if morphological.pattern == pattern else 0 for pattern in common_patterns
        ]
        features.extend(pattern_encoded)

        # Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø¥ÙƒÙ…Ø§Ù„ 30 Ø¨ÙØ¹Ø¯
        features.extend([0] * (30 - len(features)))

        return features[:30]

    def _encode_syntactic_features(self, syntactic: SyntacticVector) -> List[float]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø­ÙˆÙŠØ©"""
        features = []

        # ØªØ±Ù…ÙŠØ² Ø§Ù„ØªØ¹Ø±ÙŠÙ
        definiteness_encoded = [0] * 4
        definiteness_encoded[syntactic.definiteness.value] = 1
        features.extend(definiteness_encoded)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨
        case_encoded = [0] * 4
        case_encoded[syntactic.case_marking.value] = 1
        features.extend(case_encoded)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¬Ù†Ø¯Ø±
        gender_encoded = [0] * 3
        gender_encoded[syntactic.gender.value] = 1
        features.extend(gender_encoded)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¹Ø¯Ø¯
        number_encoded = [0] * 3
        number_encoded[syntactic.number.value] = 1
        features.extend(number_encoded)

        # Ù…ÙŠØ²Ø§Øª Ø«Ù†Ø§Ø¦ÙŠØ©
        features.append(1 if syntactic.is_vocative else 0)
        features.append(1 if syntactic.construct_state else 0)

        # Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø¥ÙƒÙ…Ø§Ù„ 20 Ø¨ÙØ¹Ø¯
        features.extend([0] * (20 - len(features)))

        return features[:20]

    def _encode_semantic_features(self, semantic: SemanticVector) -> List[float]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©"""
        features = []

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¯ÙˆØ± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        role_encoded = [0] * 6  # 6 Ø£Ø¯ÙˆØ§Ø± Ø¯Ù„Ø§Ù„ÙŠØ©
        role_encoded[semantic.semantic_role.value] = 1
        features.extend(role_encoded)

        # ØªØ±Ù…ÙŠØ² Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        features.append(1 if semantic.semantic_class == "concrete" else 0)
        features.append(1 if semantic.animacy == "animate" else 0)
        features.append(1 if semantic.countability == "count" else 0)

        # Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        features.extend(list(semantic.semantic_features.values()))

        # Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø¥ÙƒÙ…Ø§Ù„ 25 Ø¨ÙØ¹Ø¯
        features.extend([0] * (25 - len(features)))

        return features[:25]

    def _encode_advanced_features(self, advanced: AdvancedFeatures) -> List[float]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        features = []

        # ØªØ±Ù…ÙŠØ² Ø§Ù„ØªØµØºÙŠØ±
        diminutive_encoded = [0] * 4
        diminutive_encoded[advanced.diminutive_form.value] = 1
        features.extend(diminutive_encoded)

        # Ù…ÙŠØ²Ø§Øª Ø«Ù†Ø§Ø¦ÙŠØ©
        features.append(1 if advanced.irregular_inflection else 0)
        features.append(1 if advanced.hamza_type else 0)

        # Ø¹Ø¯Ø¯ ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¥Ø¯ØºØ§Ù…
        features.append(len(advanced.assimilation_effects))

        # Ø¹Ø¯Ø¯ Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¹Ø±ÙˆØ¶ÙŠØ©
        features.append(len(advanced.prosodic_breaks))

        # Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ø¥ÙƒÙ…Ø§Ù„ 15 Ø¨ÙØ¹Ø¯
        features.extend([0] * (15 - len(features)))

        return features[:15]


def main():
    """Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ
    generator = ArabicDigitalVectorGenerator()

    # ÙƒÙ„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    test_words = [
        "Ø§Ù„ÙƒØªØ§Ø¨",  # Ø§Ø³Ù… Ù…Ø¹Ø±Ù‘Ù
        "Ù…Ø¯Ø±Ø³Ø©",  # Ø§Ø³Ù… Ù…Ø¤Ù†Ø«
        "ÙƒÙØªÙÙŠÙ’Ø¨",  # ØªØµØºÙŠØ±
        "Ù…ÙØ¯Ø±ÙÙ‘Ø³",  # Ø§Ø³Ù… ÙØ§Ø¹Ù„
        "Ù…ÙƒØªÙˆØ¨",  # Ø§Ø³Ù… Ù…ÙØ¹ÙˆÙ„
    ]

    print("ğŸ”¥ Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø©")
    print("=" * 60)

    for word in test_words:
        print(f"\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©: {word}")
        print(" " * 40)

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ
        analysis = generator.generate_digital_vector(word)

        if analysis["processing_status"] == "success":
            print(f"âœ… Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù„ÙŠÙ„ - Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØªØ¬Ù‡: {analysis['vector_dimensions']}")
            print(f"ğŸ”¤ Ø§Ù„ÙÙˆÙ†ÙŠÙ…Ø§Øª: {analysis['phonological_vector'].phonemes}")
            print()
                f"ğŸ“ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ÙŠØ©: {analysis['phonological_vector'].syllabic_structure}"
            )
            print(f"ğŸŒ³ Ø§Ù„Ø¬Ø°Ø±: {analysis['morphological_vector'].root}")
            print(f"ğŸ¯ Ø§Ù„ÙˆØ²Ù†: {analysis['morphological_vector'].pattern}")
            print(f"ğŸ‘¤ Ø§Ù„Ø¬Ù†Ø¯Ø±: {analysis['syntactic_vector'].gender.value}")
            print(f"ğŸ“ Ø§Ù„ØªØ¹Ø±ÙŠÙ: {analysis['syntactic_vector'].definiteness.value}")

            # Ø¹Ø±Ø¶ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ
            vector = analysis["numerical_vector"]
            print(f"ğŸ”¢ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ø±Ù‚Ù…ÙŠ (Ø£ÙˆÙ„ 10 Ø¹Ù†Ø§ØµØ±): {vector[:10]}")

        else:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {analysis['error']}")


if __name__ == "__main__":
    from datetime import datetime

    main()

