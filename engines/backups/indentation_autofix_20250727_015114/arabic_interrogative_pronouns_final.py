#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Final Arabic Interrogative Pronouns Deep Learning System
=======================================================
Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Simplified and optimized version for Arabic interrogative pronouns classification.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 3.0.0 - FINAL SYSTEM
Date: 2025-07-24
Encoding: UTF 8
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import random
import time
from typing import Dict, List, Tuple, Any
import json

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø°Ø±Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


INTERROGATIVE_PRONOUNS = {
    0: "Ù…ÙÙ†",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ
    1: "Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡
    2: "Ù…ÙØªÙÙ‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù†
    3: "Ø£ÙÙŠÙ’Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù†
    4: "ÙƒÙÙŠÙ’ÙÙ",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙŠÙÙŠØ©
    5: "ÙƒÙÙ…Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙƒÙ…ÙŠØ©
    6: "Ø£ÙÙŠÙ‘",  # Ù„Ù„Ø§Ø®ØªÙŠØ§Ø±
    7: "Ù„ÙÙ…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨
    8: "Ù…ÙØ§Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ (Ù…Ø¨Ø§Ø´Ø±)
    9: "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø²Ù…Ø§Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
    10: "Ø£ÙÙ†ÙÙ‘Ù‰",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø·Ø±ÙŠÙ‚Ø©
    11: "Ù„ÙÙ…Ù",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨ (Ù…Ø®ØªØµØ±)
    12: "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ«ÙŠØ±
    13: "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§",  # Ø§Ù„Ù†Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…ÙŠ
    14: "Ù…ÙÙ‡Ù’Ù…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡
    15: "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ù…ÙƒØ§Ù†
    16: "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø©
    17: "Ù…ÙÙ†Ù’ Ø°ÙØ§",  # Ù„Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø¨ØªØ£ÙƒÙŠØ¯
}

PRONOUN_TO_ID = {v: k for k, v in INTERROGATIVE_PRONOUNS.items()}

# Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
SYLLABLE_TO_PRONOUN = {
    ("Ù…ÙÙ†Ù’"): "Ù…ÙÙ†",
    ("Ù…ÙØ§"): "Ù…ÙØ§",
    ("Ù…Ù", "ØªÙÙ‰"): "Ù…ÙØªÙÙ‰",
    ("Ø£ÙÙŠÙ’", "Ù†Ù"): "Ø£ÙÙŠÙ’Ù†Ù",
    ("ÙƒÙÙŠÙ’", "ÙÙ"): "ÙƒÙÙŠÙ’ÙÙ",
    ("ÙƒÙÙ…Ù’"): "ÙƒÙÙ…Ù’",
    ("Ø£ÙÙŠÙ‘"): "Ø£ÙÙŠÙ‘",
    ("Ù„Ù", "Ù…ÙØ§Ø°ÙØ§"): "Ù„ÙÙ…ÙØ§Ø°ÙØ§",
    ("Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"): "Ù„ÙÙ…ÙØ§Ø°ÙØ§",
    ("Ù…ÙØ§", "Ø°ÙØ§"): "Ù…ÙØ§Ø°ÙØ§",
    ("Ø£ÙÙŠÙÙ‘", "Ø§", "Ù†Ù"): "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù",
    ("Ø£ÙÙ†ÙÙ‘", "Ù‰"): "Ø£ÙÙ†ÙÙ‘Ù‰",
    ("Ù„Ù", "Ù…Ù"): "Ù„ÙÙ…Ù",
    ("ÙƒÙØ£ÙÙŠÙ’", "ÙŠÙÙ†Ù’"): "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’",
    ("Ø£ÙÙŠÙÙ‘", "Ù‡ÙØ§"): "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§",
    ("Ù…ÙÙ‡Ù’", "Ù…ÙØ§"): "Ù…ÙÙ‡Ù’Ù…ÙØ§",
    ("Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"): "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§",
    ("ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"): "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§",
    ("Ù…ÙÙ†Ù’", "Ø°ÙØ§"): "Ù…ÙÙ†Ù’ Ø°ÙØ§",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED LSTM MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class OptimizedLSTM(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ LSTM Ù…Ø­Ø³Ù† Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
        self, vocab_size: int = 60, hidden_size: int = 128, num_classes: int = 18
    ):
        super(OptimizedLSTM, self).__init__()

        self.hidden_size = hidden_size

        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        self.embedding = nn.Embedding(vocab_size, 64)

        # Ø·Ø¨Ù‚Ø© LSTM Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        self.lstm = nn.LSTM()
            input_size=64,
            hidden_size=hidden_size,
            num_layers=3,
            batch_first=True,
            bidirectional=True,
            dropout=0.3)

        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        self.attention = nn.Sequential()
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Softmax(dim=1))

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
        self.classifier = nn.Sequential()
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size // 2, num_classes))

    def forward(self, x):
    def forward(self, x):
        # x shape: [batch_size, seq_len]
        embedded = self.embedding(x)

        # LSTM
        lstm_out, _ = self.lstm(embedded)

        # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attention_weights = self.attention(lstm_out)
        attended_output = torch.sum(attention_weights * lstm_out, dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
        output = self.classifier(attended_output)

        return output


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED GRU MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class OptimizedGRU(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ GRU Ù…Ø­Ø³Ù† Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__()
        self, vocab_size: int = 60, hidden_size: int = 128, num_classes: int = 18
    ):
        super(OptimizedGRU, self).__init__()

        self.hidden_size = hidden_size

        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        self.embedding = nn.Embedding(vocab_size, 64)

        # Ø·Ø¨Ù‚Ø© GRU Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡
        self.gru = nn.GRU()
            input_size=64,
            hidden_size=hidden_size,
            num_layers=3,
            batch_first=True,
            bidirectional=True,
            dropout=0.3)

        # Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        self.multihead_attention = nn.MultiheadAttention()
            embed_dim=hidden_size * 2, num_heads=8, dropout=0.2, batch_first=True
        )

        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
        self.classifier = nn.Sequential()
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_size, num_classes))

    def forward(self, x):
    def forward(self, x):
        # x shape: [batch_size, seq_len]
        embedded = self.embedding(x)

        # GRU
        gru_out, _ = self.gru(embedded)

        # Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³
        attn_out, _ = self.multihead_attention(gru_out, gru_out, gru_out)

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        pooled_output = torch.mean(attn_out, dim=1)

        # Ø§Ù„ØªØµÙ†ÙŠÙ
        output = self.classifier(pooled_output)

        return output


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA PROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class SimplePhoneticProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ ØµÙˆØªÙŠ Ù…Ø¨Ø³Ø·"""

    def __init__(self):

        self.char_to_id = {}
        self.id_to_char = {}
        self._build_vocabulary()

    def _build_vocabulary(self):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        chars = set()

        # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
        for syllables in SYLLABLE_TO_PRONOUN.keys():
            for syllable in syllables:
                chars.update(syllable)

        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
        chars.update(['<PAD>', '<UNK>', '<START>', '<END>'])

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ø±Ø§Ø¦Ø·
        for i, char in enumerate(sorted(chars)):
            self.char_to_id[char] = i
            self.id_to_char[i] = char

        self.vocab_size = len(self.char_to_id)

    def encode_syllables(self, syllables: List[str], max_length: int = 10) -> List[int]:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…"""

        encoded = [self.char_to_id.get('<START>', 0)]

        for syllable in syllables:
            for char in syllable:
                char_id = self.char_to_id.get(char, self.char_to_id.get('<UNK>', 1))
                encoded.append(char_id)

        encoded.append(self.char_to_id.get('<END>', 3))

        # padding Ø£Ùˆ Ù‚Øµ
        if len(encoded) < max_length:
            encoded.extend()
                [self.char_to_id.get('<PAD>', 2)] * (max_length - len(encoded))
            )
        else:
            encoded = encoded[:max_length]

        return encoded


def create_training_data()
    processor: SimplePhoneticProcessor, num_samples: int = 3000
) -> Tuple[List[List[int]], List[int]]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""

    X = []
    y = []

    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø¹ ØªÙ†ÙˆÙŠØ¹Ø§Øª
    data_variants = {
        "Ù…ÙÙ†": [["Ù…ÙÙ†Ù’"], ["Ù…Ù", "Ù†Ù’"], ["Ù…ÙÙ†Ù’"], ["Ù…ÙÙ†Ù’"]],
        "Ù…ÙØ§": [["Ù…ÙØ§"], ["Ù…Ù", "Ø§"], ["Ù…ÙØ§"], ["Ù…ÙØ§"]],
        "Ù…ÙØªÙÙ‰": [["Ù…Ù", "ØªÙÙ‰"], ["Ù…Ù", "ØªÙ", "Ù‰"], ["Ù…Ù", "ØªÙÙ‰"]],
        "Ø£ÙÙŠÙ’Ù†Ù": [["Ø£ÙÙŠÙ’", "Ù†Ù"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù"], ["Ø£ÙÙŠÙ’", "Ù†Ù"]],
        "ÙƒÙÙŠÙ’ÙÙ": [["ÙƒÙÙŠÙ’", "ÙÙ"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ"], ["ÙƒÙÙŠÙ’", "ÙÙ"]],
        "ÙƒÙÙ…Ù’": [["ÙƒÙÙ…Ù’"], ["ÙƒÙ", "Ù…Ù’"], ["ÙƒÙÙ…Ù’"]],
        "Ø£ÙÙŠÙ‘": [["Ø£ÙÙŠÙ‘"], ["Ø£Ù", "ÙŠÙ‘"], ["Ø£ÙÙŠÙ‘"]],
        "Ù„ÙÙ…ÙØ§Ø°ÙØ§": [["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"], ["Ù„Ù", "Ù…Ù", "Ø§", "Ø°ÙØ§"]],
        "Ù…ÙØ§Ø°ÙØ§": [["Ù…ÙØ§", "Ø°ÙØ§"], ["Ù…Ù", "Ø§", "Ø°ÙØ§"]],
        "Ø£ÙÙŠÙÙ‘Ø§Ù†Ù": [["Ø£ÙÙŠÙÙ‘", "Ø§", "Ù†Ù"], ["Ø£Ù", "ÙŠÙÙ‘", "Ø§", "Ù†Ù"]],
        "Ø£ÙÙ†ÙÙ‘Ù‰": [["Ø£ÙÙ†ÙÙ‘", "Ù‰"], ["Ø£Ù", "Ù†ÙÙ‘", "Ù‰"]],
        "Ù„ÙÙ…Ù": [["Ù„ÙÙ…Ù"], ["Ù„Ù", "Ù…Ù"]],
        "ÙƒÙØ£ÙÙŠÙÙ‘Ù†Ù’": [["ÙƒÙØ£ÙÙŠÙ’", "ÙŠÙÙ†Ù’"], ["ÙƒÙ", "Ø£Ù", "ÙŠÙÙ‘", "Ù†Ù’"]],
        "Ø£ÙÙŠÙÙ‘Ù‡ÙØ§": [["Ø£ÙÙŠÙÙ‘", "Ù‡ÙØ§"], ["Ø£Ù", "ÙŠÙÙ‘", "Ù‡ÙØ§"]],
        "Ù…ÙÙ‡Ù’Ù…ÙØ§": [["Ù…ÙÙ‡Ù’", "Ù…ÙØ§"], ["Ù…Ù", "Ù‡Ù’", "Ù…ÙØ§"]],
        "Ø£ÙÙŠÙ’Ù†ÙÙ…ÙØ§": [["Ø£ÙÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"], ["Ø£Ù", "ÙŠÙ’", "Ù†Ù", "Ù…ÙØ§"]],
        "ÙƒÙÙŠÙ’ÙÙÙ…ÙØ§": [["ÙƒÙÙŠÙ’", "ÙÙ", "Ù…ÙØ§"], ["ÙƒÙ", "ÙŠÙ’", "ÙÙ", "Ù…ÙØ§"]],
        "Ù…ÙÙ†Ù’ Ø°ÙØ§": [["Ù…ÙÙ†Ù’", "Ø°ÙØ§"], ["Ù…Ù", "Ù†Ù’", "Ø°ÙØ§"]],
    }

    samples_per_pronoun = num_samples // len(data_variants)

    for pronoun, variants in data_variants.items():
        pronoun_id = PRONOUN_TO_ID[pronoun]

        for _ in range(samples_per_pronoun):
            syllables = random.choice(variants)
            encoded = processor.encode_syllables(syllables)

            X.append(encoded)
            y.append(pronoun_id)

    return X, y


def train_model()
    model: nn.Module, X: List[List[int]], y: List[int], epochs: int = 30
) -> Dict[str, List[float]]:
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    X_tensor = torch.LongTensor(X)
    y_tensor = torch.LongTensor(y)

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_size = int(0.8 * len(X_tensor))
    train_X, test_X = X_tensor[:train_size], X_tensor[train_size:]
    train_y, test_y = y_tensor[:train_size], y_tensor[train_size:]

    # DataLoader
    train_dataset = torch.utils.data.TensorDataset(train_X, train_y)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

    # Optimizer ÙˆØ§Ù„Ù…Ø¹ÙŠØ§Ø±
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        correct_train = 0
        total_train = 0

        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += batch_y.size(0)
            correct_train += (predicted == batch_y).sum().item()

        scheduler.step()

        # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
        model.eval()
        with torch.no_grad():
            test_X_device = test_X.to(device)
            test_outputs = model(test_X_device)
            _, test_predicted = torch.max(test_outputs.data, 1)
            test_accuracy = (test_predicted == test_y.to(device)).float().mean().item()

        model.train()

        train_accuracy = correct_train / total_train
        avg_loss = total_loss / len(train_loader)

        history['train_loss'].append(avg_loss)
        history['train_acc'].append(train_accuracy)
        history['test_acc'].append(test_accuracy)

        if (epoch + 1) % 10 == 0:
            print()
                f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.4f | Test} Acc: {test_accuracy:.4f}}"
            )

    return history


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFERENCE SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class FinalInterrogativeSystem:
    """Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…"""

    def __init__(self):

        self.processor = SimplePhoneticProcessor()
        self.models = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.models['lstm'] = OptimizedLSTM()
            vocab_size=self.processor.vocab_size,
            hidden_size=128,
            num_classes=len(INTERROGATIVE_PRONOUNS))

        self.models['gru'] = OptimizedGRU()
            vocab_size=self.processor.vocab_size,
            hidden_size=128,
            num_classes=len(INTERROGATIVE_PRONOUNS))

        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø²
        for model in self.models.values():
            model.to(self.device)

    def train_all_models(self, num_samples: int = 4000):
        """ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""

        print("ğŸ§  Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        X, y = create_training_data(self.processor, num_samples)

        results = {}

        for model_name, model in self.models.items():
            print(f"\nğŸš€ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {model_name.upper()}...")
            history = train_model(model, X, y, epochs=25)

            results[model_name] = {
                'final_train_acc': history['train_acc'][ 1],
                'final_test_acc': history['test_acc'][ 1],
                'best_test_acc': max(history['test_acc']),
                'history': history,
            }

            print()
                f"âœ… {model_name.upper() - Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©} Ø§Ø®ØªØ¨Ø§Ø±: {max(history['test_acc']):.3f}}"
            )

        return results

    def predict_syllables()
        self, syllables: List[str], model_type: str = 'gru'
    ) -> Dict[str, Any]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³Ù… Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        if model_type not in self.models:
            model_type = 'gru'  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

        model = self.models[model_type]
        model.eval()

        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
        encoded = self.processor.encode_syllables(syllables)
        input_tensor = torch.LongTensor(encoded).unsqueeze(0).to(self.device)

        # Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
            outputs = model(input_tensor)
            probabilities = F.softmax(outputs, dim=1)[0]

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)

        results = {
            'best_prediction': INTERROGATIVE_PRONOUNS[sorted_indices[0].item()],
            'confidence': sorted_probs[0].item(),
            'alternatives': [],
        }

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„
        for i in range(min(3, len(sorted_indices))):
            idx = sorted_indices[i].item()
            prob = sorted_probs[i].item()
            results['alternatives'].append()
                {'pronoun': INTERROGATIVE_PRONOUNS[idx], 'confidence': prob}
            )

        return results

    def comprehensive_test(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…"""

        print("\nğŸ”¬ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„:")

        test_cases = [
            ["Ù…ÙÙ†Ù’"],  # Ù…ÙÙ†
            ["Ù…ÙØ§"],  # Ù…Ø§
            ["Ù…Ù", "ØªÙÙ‰"],  # Ù…ØªÙ‰
            ["Ø£ÙÙŠÙ’", "Ù†Ù"],  # Ø£ÙŠÙ†
            ["ÙƒÙÙŠÙ’", "ÙÙ"],  # ÙƒÙŠÙ
            ["ÙƒÙÙ…Ù’"],  # ÙƒÙ…
            ["Ø£ÙÙŠÙ‘"],  # Ø£ÙŠ
            ["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"],  # Ù„Ù…Ø§Ø°Ø§
            ["Ù…ÙØ§", "Ø°ÙØ§"],  # Ù…Ø§Ø°Ø§
            ["Ù„Ù", "Ù…Ù"],  # Ù„Ù…
        ]

        expected = ["Ù…ÙÙ†", "Ù…ÙØ§", "Ù…ÙØªÙÙ‰", "Ø£ÙÙŠÙ’Ù†Ù", "ÙƒÙÙŠÙ’ÙÙ", "ÙƒÙÙ…Ù’", "Ø£ÙÙŠÙ‘", "Ù„ÙÙ…ÙØ§Ø°ÙØ§", "Ù…ÙØ§Ø°ÙØ§", "Ù„ÙÙ…Ù"]

        for model_name in ['lstm', 'gru']:
            print(f"\n   ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ {model_name.upper()}:")
            correct = 0

            for i, (syllables, exp) in enumerate(zip(test_cases, expected)):
                result = self.predict_syllables(syllables, model_name)
                prediction = result['best_prediction']
                confidence = result['confidence']

                is_correct = prediction == exp
                if is_correct:
                    correct += 1

                status = "âœ…" if is_correct else "âŒ"
                print()
                    f"     {status} {syllables} â†’ {prediction} (Ø«Ù‚Ø©: {confidence:.3f}) [Ù…ØªÙˆÙ‚Ø¹: {exp}]"
                )

            accuracy = correct / len(test_cases)
            print()
                f"     ğŸ“ˆ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {accuracy:.1%} ({correct}/{len(test_cases)})"
            )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main():
    """Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""

    print("ğŸ§  Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ - Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    print("=" * 65)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    system = FinalInterrogativeSystem()

    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
    start_time = time.time()

    results = system.train_all_models(num_samples=3500)

    training_time = time.time() - start_time

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"   â±ï¸  Ø²Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_time:.1f} Ø«Ø§Ù†ÙŠØ©")

    for model_name, result in results.items():
        print()
            f"   ğŸ† {model_name.upper()}: {result['best_test_acc']:.1% Ø£ÙØ¶Ù„} Ø¯Ù‚Ø© Ø§Ø®ØªØ¨Ø§Ø±}"
        )

    # Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„
    system.comprehensive_test()

    # Ø§Ø®ØªØ¨Ø§Ø± ØªÙØµÙŠÙ„ÙŠ
    print("\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± ØªÙØµÙŠÙ„ÙŠ:")
    detailed_test = system.predict_syllables(["Ù„Ù", "Ù…ÙØ§", "Ø°ÙØ§"], 'gru')
    print("   Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: ['Ù„Ù', 'Ù…ÙØ§', 'Ø°ÙØ§']")
    print(f"   Ø§Ù„ØªÙ†Ø¨Ø¤: {detailed_test['best_prediction']}")
    print(f"   Ø§Ù„Ø«Ù‚Ø©: {detailed_test['confidence']:.3f}")
    print("   Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„:")
    for alt in detailed_test['alternatives']:
        print(f"     - {alt['pronoun']: {alt['confidence']:.3f}}")

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('final_interrogative_results.json', 'w', encoding='utf 8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)

    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ!")
    print("ğŸ’¾ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: final_interrogative_results.json")


if __name__ == "__main__":
    main()

