#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
==============================================
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…Ù†Ø¬Ø²Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped-def,misc

# pylint: disable=invalid-name,too-few-public-methods,too-many-instance-attributes,line-too long


import os  # noqa: F401
import sys  # noqa: F401
import time  # noqa: F401
import json  # noqa: F401
import logging  # noqa: F401
from datetime import datetime  # noqa: F401
from pathlib import Path  # noqa: F401

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    processrs=[
        logging.FileProcessr("engine_analysis.log", encoding="utf 8"),
        logging.StreamProcessr(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


class EngineAnalysisSystem:
    """Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø­Ø±ÙƒØ§Øª"""

    def __init__(self):  # type: ignore[no-untyped def]
        """TODO: Add docstring."""
        self.engines_report = {
            "fixed_engines": {
                "AdvancedPhonemeEngine": {
                    "status": "working",
                    "method": "advanced_phoneme_handling",
                    "features": ["IPA conversion", "phonetic features analysis"],
                    "category": "fixed",
                },
                "PhonologyEngine": {
                    "status": "working",
                    "method": "comprehensive_phonology_analysis",
                    "features": [
                        "Phonological rules",
                        "CV patterns",
                        "syllabic_unit analysis",
                    ],
                    "category": "fixed",
                },
                "MorphologyEngine": {
                    "status": "working",
                    "method": "comprehensive_morphological_analysis",
                    "features": [
                        "Root extraction",
                        "pattern recognition",
                        "complexity scoring",
                    ],
                    "category": "fixed",
                },
                "WeightEngine": {
                    "status": "working",
                    "method": "comprehensive_morphological_weight",
                    "features": [
                        "Weight calculation",
                        "prosodic analysis",
                        "pattern classification",
                    ],
                    "category": "fixed",
                },
                "FullPipelineEngine": {
                    "status": "working",
                    "method": "comprehensive_full_pipeline_analysis",
                    "features": [
                        "Integrated analysis",
                        "synthesis",
                        "complexity indexing",
                    ],
                    "category": "fixed",
                },
            },
            "morphophon_engines": {
                "ProfessionalPhonologyAnalyzer": {
                    "status": "working",
                    "method": "professional_phonology",
                    "features": ["Advanced phonological analysis"],
                    "category": "morphophon",
                },
                "RootDatabaseEngine": {
                    "status": "working",
                    "method": "root_database_search",
                    "features": ["Comprehensive root extraction", "database lookup"],
                    "category": "morphophon",
                },
                "SyllabicUnitEncoder": {
                    "status": "working",
                    "method": "syllabic_unit_encoding",
                    "features": ["Advanced syllabic_unit segmentation", "CV encoding"],
                    "category": "morphophon",
                },
            },
            "failed_engines": {
                "DiacriticEngine": {"status": "failed", "category": "failed"},
                "SyllabicUnitEngine": {"status": "failed", "category": "failed"},
                "RootEngine": {"status": "failed", "category": "failed"},
                "DerivationEngine": {"status": "failed", "category": "failed"},
                "InflectionEngine": {"status": "failed", "category": "failed"},
            },
        }

        self.progressive_stages = [
            "phoneme_analysis",
            "diacritic_mapping",
            "syllable_formation",
            "root_extraction",
            "pattern_analysis",
            "derivation_check",
            "inflection_analysis",
            "final_classification",
        ]

    def analyze_engine_status(self):  # type: ignore[no-untyped def]
        """ØªØ­Ù„ÙŠÙ„ Ø­Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª"""
        print("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª...")
        print("=" * 70)

        total_engines = 0
        working_engines = 0
        failed_engines = 0

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
        print("ğŸ“Š Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© (Fixed Engines):")
        for engine_name, info in self.engines_report["fixed_engines"].items():
            total_engines += 1
            if info["status"] == "working":
                working_engines += 1
                print(f"   âœ… {engine_name: {info['method']}}")
            else:
                failed_engines += 1
                print(f"   âŒ {engine_name: {info.get('error',} 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}}")

        # ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆ ÙÙˆÙ†ÙˆÙ„ÙˆØ¬ÙŠ
        print("\nğŸ“Š Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆ-ÙÙˆÙ†ÙˆÙ„ÙˆØ¬ÙŠ (Morphophon Engines):")
        for engine_name, info in self.engines_report["morphophon_engines"].items():
            total_engines += 1
            if info["status"] == "working":
                working_engines += 1
                print(f"   âœ… {engine_name: {info['method']}}")
            else:
                failed_engines += 1
                print(f"   âŒ {engine_name: {info.get('error',} 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©
        print("\nğŸ“Š Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø© (Failed Engines):")
        for engine_name, info in self.engines_report["failed_engines"].items():
            total_engines += 1
            failed_engines += 1
            print(f"   âŒ {engine_name:} ÙŠØ­ØªØ§Ø¬ Ø¥ØµÙ„Ø§Ø­}")

        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        success_rate = (
            (working_engines / total_engines) * 100 if total_engines > 0 else 0
        )

        print("\n" + "=" * 70)
        print("ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª:")
        print(f"   ğŸ¯ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª: {total_engines}")
        print(f"   âœ… Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {working_engines}")
        print(f"   âŒ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {failed_engines}")
        print(f"   ğŸ“Š Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.1f}%")

        return {
            "total": total_engines,
            "working": working_engines,
            "failed": failed_engines,
            "success_rate": success_rate,
        }

    def test_progressive_system(self):  # type: ignore[no-untyped def]
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ"""
        print("\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ...")
        print("=" * 70)

        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
        try:
            from progressive_vector_tracker import (
                ProgressiveArabicVectorTracker,
            )  # noqa: F401

            tracker = ProgressiveArabicVectorTracker()
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ: {e}")
            return None

        # ÙƒÙ„Ù…Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙ†ÙˆØ¹Ø©
        test_words = ["ÙƒØªØ§Ø¨", "Ù…Ø¯Ø±Ø³Ø©", "ÙŠÙƒØªØ¨", "Ø§Ù„Ø·Ø§Ù„Ø¨", "Ø¬Ù…ÙŠÙ„Ø©"]

        results = []
        for word in test_words:
            try:
                print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„Ù…Ø©: {word}")
                start_time = time.time()

                analysis = tracker.track_progressive_analysis(word)

                end_time = time.time()
                processing_time = end_time - start_time

                result = {
                    "word": word,
                    "success": True,
                    "vector_size": len(analysis.final_vector),
                    "phoneme_pairs": len(analysis.phoneme_diacritic_pairs),
                    "syllabic_units": len(analysis.syllabic_units),
                    "root": analysis.morphological_analysis.root,
                    "processing_time": processing_time,
                    "stages_completed": len(analysis.analysis_steps),
                }

                print(
                    f"   âœ… Ù†Ø¬Ø­: {result['vector_size']} Ø¨ÙØ¹Ø¯ ÙÙŠ {processing_time:.4f}s"
                )
                print(
                    f"   ğŸ“Š Ø§Ù„Ø¬Ø°Ø±: {result['root']}, Ù…Ù‚Ø§Ø·Ø¹: {result['syllabic_units']}"
                )  # noqa: E501

                results.append(result)

            except Exception as e:
                print(f"   âŒ ÙØ´Ù„: {str(e)}")
                results.append({"word": word, "success": False, "error": str(e)})

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        successful_tests = [r for r in results if r.get("success", False)]
        [r for r in results if not r.get("success", False)]

        if successful_tests:
            avg_vector_size = sum(r["vector_size"] for r in successful_tests) / len(
                successful_tests
            )
            avg_processing_time = sum(
                r["processing_time"] for r in successful_tests
            ) / len(successful_tests)

            print("\nğŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
            print(f"   âœ… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø§Ø¬Ø­Ø©: {len(successful_tests)/{len(test_words)}}")
            print(f"   ğŸ“ Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ù…ØªØ¬Ù‡: {avg_vector_size:.0f} Ø¨ÙØ¹Ø¯")
            print(f"   â±ï¸ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {avg_processing_time:.4f}s")

        return results

    def check_stage_compatibility(self):  # type: ignore[no-untyped def]
        """ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø±Ø§Ø­Ù„"""
        print("\nğŸ”„ ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©...")
        print("=" * 70)

        stage_status = {}

        # ÙØ­Øµ ÙƒÙ„ Ù…Ø±Ø­Ù„Ø©
        for i, stage in enumerate(self.progressive_stages, 1):
            print(f"{i}. {stage.replace('_',} ' ').title()}")

            # ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø±Ø­Ù„Ø©
            if stage in ["phoneme_analysis", "diacritic_mapping"]:
                # Ù…Ø±Ø§Ø­Ù„ Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
                status = "âœ… Ù…Ø¯Ø¹ÙˆÙ…Ø©"
                working = True
            elif stage in ["syllable_formation"]:
                # Ù…Ø±Ø­Ù„Ø© Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ÙˆØ±ÙÙˆ ÙÙˆÙ†ÙˆÙ„ÙˆØ¬ÙŠ
                status = "âœ… Ù…Ø¯Ø¹ÙˆÙ…Ø©"
                working = True
            elif stage in ["root_extraction", "pattern_analysis"]:
                # Ù…Ø±Ø§Ø­Ù„ Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¬Ø²Ø¦ÙŠØ§Ù‹
                status = "âš ï¸ Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¬Ø²Ø¦ÙŠØ§Ù‹"
                working = True
            else:
                # Ù…Ø±Ø§Ø­Ù„ ØªØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ±
                status = "ğŸ”§ ØªØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ±"
                working = False

            print(f"   {status}")
            stage_status[stage] = working

        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚
        working_stages = sum(1 for working in stage_status.values() if working)
        total_stages = len(stage_status)
        compatibility_rate = (working_stages / total_stages) * 100

        print(f"\nğŸ“Š Ù†Ø³Ø¨Ø© ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø±Ø§Ø­Ù„: {compatibility_rate:.1f}%")

        return stage_status

    def generate_comprehensive_report(self):  # type: ignore[no-untyped def]
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        print("\nğŸ“‹ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„...")
        print("=" * 70)

        # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
        engine_stats = self.analyze_engine_status()
        progressive_results = self.test_progressive_system()
        stage_compatibility = self.check_stage_compatibility()

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = {
            "timestamp": datetime.now().isoformat(),
            "engine_analysis": engine_stats,
            "progressive_system_test": progressive_results,
            "stage_compatibility": stage_compatibility,
            "summary": {
                "overall_status": (
                    "Ø¬ÙŠØ¯" if engine_stats["success_rate"] > 60 else "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
                ),
                "recommendations": self.generate_recommendations(
                    engine_stats, stage_compatibility
                ),
            },
        }

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_file = (
            f"engine_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S').json}"
        )
        with open(report_file, "w", encoding="utf 8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")

        return report

    def generate_recommendations(self, engine_stats, stage_compatibility):  # type: ignore[no-untyped def]
        """Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []

        if engine_stats["success_rate"] < 80:
            recommendations.append("Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡")

        if engine_stats["failed"] > 0:
            recommendations.append(
                "Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: DiacriticEngine, SyllabicUnitEngine, RootEngine, DerivationEngine, InflectionEngine"
            )

        working_stages = sum(1 for working in stage_compatibility.values() if working)
        if working_stages < len(stage_compatibility):
            recommendations.append("ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù„Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ")

        if not recommendations:
            recommendations.append("Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ØŒ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª")

        return recommendations


def main():  # type: ignore[no-untyped def]
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©")
    print("=" * 70)

    analyzer = EngineAnalysisSystem()

    try:
        # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
        report = analyzer.generate_comprehensive_report()

        print("\nğŸ¯ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print(f"   Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {report['summary']['overall_status']}")
        print("   Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        for rec in report["summary"]["recommendations"]:
            print(f"   â€¢ {rec}")

        print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        logger.error(f"Analysis error: {e}")


if __name__ == "__main__":
    main()
