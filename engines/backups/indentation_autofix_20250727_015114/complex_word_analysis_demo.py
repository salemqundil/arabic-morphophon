#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Arabic Word Analysis Example
====================================
ูุซุงู ูุชูุฏู ูุชุญููู ุงููููุงุช ุงูุนุฑุจูุฉ ุงููุนูุฏุฉ

Example: "ูุณุชูุชุจูููุง" (yastaktiboonahaa)
Analysis layers: phonological โ morphological โ syntactic โ semantic

Author: GitHub Copilot Arabic NLP Expert
Version: 2.0.0 - WORD ANALYSIS EXAMPLE
Date: 2025-07-26
Encoding: UTF 8
"""
# pylint: disable=broad-except,unused-variable,too-many-arguments
# pylint: disable=too-few-public-methods,invalid-name,unused-argument
# flake8: noqa: E501,F401,F821,A001,F403
# mypy: disable-error-code=no-untyped def,misc


from advanced_arabic_phonology_system import ()
    AdvancedArabicPhonology,
    FunctionalCategory,
    PhonemeFunction,
    PhonemicLayer)  # noqa: F401
import json  # noqa: F401
from typing import Dict, List, Any


class ComplexWordAnalyzer:
    """ูุญูู ุงููููุงุช ุงููุนูุฏุฉ"""

    def __init__(self):  # type: ignore[no-untyped def]
        """TODO: Add docstring."""
        self.phonology = AdvancedArabicPhonology()
        self.analysis_layers = {
            'phonological': self._analyze_phonological,
            'morphological': self._analyze_morphological,
            'syntactic': self._analyze_syntactic,
            'semantic': self._analyze_semantic,
        }

    def analyze_complex_word()
        self, word: str, transliteration: str = None
    ) -> Dict[str, Any]:
        """
        ุชุญููู ุดุงูู ููููุฉ ูุนูุฏุฉ ุนุจุฑ ุฌููุน ุงููุณุชููุงุช ุงููุบููุฉ

        Args:
            word: ุงููููุฉ ุงูุนุฑุจูุฉ
            transliteration: ุงูููู ุงูุตูุชู (ุงุฎุชูุงุฑู)

        Returns:
            Dict: ุงูุชุญููู ุงูุดุงูู ูุชุนุฏุฏ ุงููุณุชููุงุช
        """
        print(f"๐ฌ ุชุญููู ุงููููุฉ ุงููุนูุฏุฉ: {word}")
        if transliteration:
            print(f"   ุงูููู ุงูุตูุชู: {transliteration}")

        analysis = {
            'input_word': word,
            'transliteration': transliteration,
            'layers': {},
            'complexity_score': 0,
            'generation_pathway': [],
        }

        # ุชุทุจูู ุงูุชุญููู ุนุจุฑ ุงููุณุชููุงุช
        for layer_name, analyzer_func in self.analysis_layers.items():
            print(f"\n๐ ุงููุณุชูู {layer_name}:")
            layer_analysis = analyzer_func(word, transliteration)
            analysis['layers'][layer_name] = layer_analysis

            # ุนุฑุถ ุงููุชุงุฆุฌ
            for key, value in layer_analysis.items():
                if isinstance(value, list) and len(value) <= 5:
                    print(f"   {key: {value}}")
                elif isinstance(value, dict) and len(value) <= 3:
                    print(f"   {key: {value}}")
                else:
                    print()
                        f"   {key}: {type(value).__name__} ูุน {len(value) if hasattr(value, '__len__') else} 'N/A' ุนูุตุฑ}"
                    )

        # ุญุณุงุจ ุฏุฑุฌุฉ ุงูุชุนููุฏ
        analysis['complexity_score'] = self._calculate_complexity(analysis)

        # ุชุชุจุน ูุณุงุฑ ุงูุชูููุฏ
        analysis['generation_pathway'] = self._trace_generation_pathway(analysis)

        print(f"\n๐ฏ ุฏุฑุฌุฉ ุงูุชุนููุฏ: {analysis['complexity_score']:.2f}")
        print(f"๐ค๏ธ ูุณุงุฑ ุงูุชูููุฏ: {'} โ '.join(analysis['generation_pathway'])}")

        return analysis

    def _analyze_phonological(self, word: str, transliteration: str) -> Dict[str, Any]:
        """ุงูุชุญููู ุงูุตูุชู"""
        return {
            'phoneme_sequence': list(transliteration or word),
            'syllable_structure': self._extract_syllables(word),
            'phonetic_features': self._extract_phonetic_features(word),
            'phonotactic_constraints': self._check_phonotactic_constraints(word),
            'stress_pattern': self._analyze_stress_pattern(word),
        }

    def _analyze_morphological(self, word: str, transliteration: str) -> Dict[str, Any]:
        """ุงูุชุญููู ุงูุตุฑูู"""
        return {
            'root_identification': self._identify_root(word),
            'pattern_analysis': self._analyze_pattern(word),
            'morpheme_segmentation': self._segment_morphemes(word),
            'derivational_history': self._trace_derivation(word),
            'inflectional_features': self._extract_inflection(word),
        }

    def _analyze_syntactic(self, word: str, transliteration: str) -> Dict[str, Any]:
        """ุงูุชุญููู ุงููุญูู"""
        return {
            'word_class': self._determine_word_class(word),
            'grammatical_features': self._extract_grammatical_features(word),
            'syntactic_functions': self._identify_syntactic_functions(word),
            'agreement_features': self._analyze_agreement(word),
            'case_marking': self._analyze_case_marking(word),
        }

    def _analyze_semantic(self, word: str, transliteration: str) -> Dict[str, Any]:
        """ุงูุชุญููู ุงูุฏูุงูู"""
        return {
            'semantic_roles': self._identify_semantic_roles(word),
            'thematic_structure': self._analyze_thematic_structure(word),
            'lexical_relations': self._find_lexical_relations(word),
            'conceptual_mapping': self._map_concepts(word),
            'pragmatic_features': self._analyze_pragmatics(word),
        }

    # Helper methods ููุชุญูููุงุช ุงูููุตูุฉ

    def _extract_syllables(self, word: str) -> List[str]:
        """ุงุณุชุฎุฑุงุฌ ุงูููุงุทุน ุงูุตูุชูุฉ"""
        # ุชุจุณูุท ููุนุฑุถ - ูุญุชุงุฌ ุฎูุงุฑุฒููุฉ ูุชูุฏูุฉ
        syllables = []
        current_syllable = ""

        vowels = "ุงููุฉููู"
        for char in word:
            current_syllable += char
            if char in vowels and len(current_syllable) >= 2:
                syllables.append(current_syllable)
                current_syllable = ""

        if current_syllable:
            syllables.append(current_syllable)

        return syllables

    def _identify_root(self, word: str) -> Dict[str, str]:
        """ุชุญุฏูุฏ ุงูุฌุฐุฑ"""
        # ูููุซุงู: ูุณุชูุชุจูููุง โ ุฌุฐุฑ ูุชุจ
        if "ูุชุจ" in word:
            return {'root': 'ู-ุช ุจ', 'root_type': 'trilateral', 'root_class': 'strong'}
        return {'root': 'ุบูุฑ ูุญุฏุฏ', 'root_type': 'unknown', 'root_class': 'unknown'}

    def _segment_morphemes(self, word: str) -> List[Dict[str, str]]:
        """ุชูุทูุน ุงูููุฑูููุงุช"""
        # ูููุซุงู: ูุณุชูุชุจูููุง
        if word == "ูุณุชูุชุจูููุง":
            return [
                {'morpheme': 'ู', 'type': 'prefix', 'function': 'imperfective_marker'},
                {'morpheme': 'ุณุช', 'type': 'infix', 'function': 'form_10_marker'},
                {'morpheme': 'ูุชุจ', 'type': 'root', 'function': 'lexical_core'},
                {'morpheme': 'ูู', 'type': 'suffix', 'function': 'plural_masculine'},
                {'morpheme': 'ูุง', 'type': 'suffix', 'function': 'object_pronoun_3fs'},
            ]
        return [{'morpheme': word, 'type': 'stem', 'function': 'unknown'}]

    def _determine_word_class(self, word: str) -> str:
        """ุชุญุฏูุฏ ุงููุฆุฉ ุงููุญููุฉ"""
        if word.startswith('ู') and 'ูู' in word:
            return 'verb_imperfective_3mp'
        return 'unknown'

    def _extract_grammatical_features(self, word: str) -> Dict[str, str]:
        """ุงุณุชุฎุฑุงุฌ ุงูุฎุตุงุฆุต ุงููุญููุฉ"""
        if word == "ูุณุชูุชุจูููุง":
            return {
                'tense': 'imperfective',
                'person': '3rd',
                'number': 'plural',
                'gender': 'masculine',
                'voice': 'active',
                'mood': 'indicative',
                'form': 'X',
                'object': 'attached_pronoun_3fs',
            }
        return {}

    def _identify_semantic_roles(self, word: str) -> Dict[str, str]:
        """ุชุญุฏูุฏ ุงูุฃุฏูุงุฑ ุงูุฏูุงููุฉ"""
        if "ูุชุจ" in word:
            return {
                'main_concept': 'writing/inscription',
                'semantic_field': 'communication',
                'action_type': 'causative',
                'transitivity': 'ditransitive',
            }
        return {}

    def _calculate_complexity(self, analysis: Dict[str, Any]) -> float:
        """ุญุณุงุจ ุฏุฑุฌุฉ ุงูุชุนููุฏ ุงููุบูู"""
        complexity_factors = {
            'morpheme_count': len()
                analysis['layers']['morphological']['morpheme_segmentation']
            ),
            'syllable_count': len()
                analysis['layers']['phonological']['syllable_structure']
            ),
            'grammatical_features': len()
                analysis['layers']['syntactic']['grammatical_features']
            ),
            'semantic_roles': len(analysis['layers']['semantic']['semantic_roles']),
        }

        # ูุนุงุฏูุฉ ุงูุชุนููุฏ ุงููุฑุฌุญุฉ
        weights = {
            'morpheme_count': 0.3,
            'syllable_count': 0.2,
            'grammatical_features': 0.3,
            'semantic_roles': 0.2,
        }

        complexity = sum()
            factor * weights.get(name, 0.1)
            for name, factor in complexity_factors.items()
        )

        return min(complexity, 10.0)  # ุชุญุฏูุฏ ุฃูุตู ุฏุฑุฌุฉ ุจู 10

    def _trace_generation_pathway(self, analysis: Dict[str, Any]) -> List[str]:
        """ุชุชุจุน ูุณุงุฑ ุชูููุฏ ุงููููุฉ"""
        pathway = []

        # ุงุณุชุฎุฑุงุฌ ูุณุงุฑ ุงูุชูููุฏ ูู ุงูุชุญููู
        morphological = analysis['layers']['morphological']

        if 'root_identification' in morphological:
            root = morphological['root_identification'].get('root', 'ุฌุฐุฑ')
            pathway.append(f"ุงูุฌุฐุฑ({root})")

        if 'pattern_analysis' in morphological:
            pathway.append("ุชุทุจูู_ุงููุฒู")

        morpheme_count = len(morphological.get('morpheme_segmentation', []))
        if morpheme_count > 1:
            pathway.append(f"ุฅุถุงูุฉ_ุงูุฒูุงุฆุฏ({morpheme_count 1})")

        syntactic = analysis['layers']['syntactic']
        if syntactic.get('grammatical_features'):
            pathway.append("ุงูุชุตุฑูู_ุงููุญูู")

        return pathway

    # Helper methods ุฅุถุงููุฉ ููุชุญูููุงุช ุงูุชูุตูููุฉ

    def _extract_phonetic_features(self, word: str) -> Dict[str, Any]:
        """TODO: Add docstring."""
        return {'consonant_clusters': [], 'vowel_patterns': [], 'gemination': False}

    def _check_phonotactic_constraints(self, word: str) -> List[str]:
        """TODO: Add docstring."""
        return ['valid_syllable_structure', 'no_consonant_clusters']

    def _analyze_stress_pattern(self, word: str) -> str:
        """TODO: Add docstring."""
        return 'penultimate'  # ุงููุจุฑุฉ ุนูู ุงูููุทุน ูุจู ุงูุฃุฎูุฑ

    def _analyze_pattern(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'pattern': 'ููุณุชููุนูููู', 'form': 'X', 'augmentation': 'ุงุณุช'}

    def _trace_derivation(self, word: str) -> List[str]:
        """TODO: Add docstring."""
        return ['root_insertion', 'form_10_derivation', 'imperfective_inflection']

    def _extract_inflection(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'aspect': 'imperfective', 'agreement': '3mp'}

    def _identify_syntactic_functions(self, word: str) -> List[str]:
        """TODO: Add docstring."""
        return ['predicate', 'transitive_verb']

    def _analyze_agreement(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'subject_agreement': '3mp', 'object_agreement': '3fs'}

    def _analyze_case_marking(self, word: str) -> str:
        """TODO: Add docstring."""
        return 'not_applicable'  # ุงูุฃูุนุงู ูุง ุชุชุตุฑู ุฅุนุฑุงุจูุงู

    def _analyze_thematic_structure(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'theta_roles': ['agent', 'theme', 'goal']}

    def _find_lexical_relations(self, word: str) -> List[str]:
        """TODO: Add docstring."""
        return ['ูุชุงุจ', 'ูุงุชุจ', 'ููุชูุจ', 'ููุชุจุฉ']

    def _map_concepts(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'domain': 'communication', 'frame': 'writing_act'}

    def _analyze_pragmatics(self, word: str) -> Dict[str, str]:
        """TODO: Add docstring."""
        return {'register': 'formal', 'politeness': 'neutral'}


def demonstrate_complex_analysis():  # type: ignore[no-untyped-def]
    """ุนุฑุถ ุชูุถูุญู ููุชุญููู ุงููุนูุฏ"""

    print("๐ ุชุญููู ุงููููุงุช ุงููุนูุฏุฉ - ุงููุธุงู ุงููุชูุฏู")
    print("=" * 60)

    analyzer = ComplexWordAnalyzer()

    # ุฃูุซูุฉ ุนูู ูููุงุช ูุนูุฏุฉ
    complex_words = [
        ("ูุณุชูุชุจูููุง", "yastaktiboonahaa"),
        ("ูุณูุณุชุฎุฑุฌูููุง", "fasayastakhrijoonahaa"),
        ("ูุงููุณุชุฎุฏููู", "walmustakhdimeen"),
        ("ุจุงูุงุณุชููุงููุฉ", "bilistiqlaaliyya"),
    ]

    all_analyses = {}

    for arabic_word, transliteration in complex_words:
        print(f"\n{'='*60}")
        analysis = analyzer.analyze_complex_word(arabic_word, transliteration)
        all_analyses[arabic_word] = analysis

        # ุนุฑุถ ููุฎุต ุงูุชุญููู
        print("\n๐ ููุฎุต ุงูุชุญููู:")
        print(f"   ุฏุฑุฌุฉ ุงูุชุนููุฏ: {analysis['complexity_score']:.2f/10}")
        print()
            f"   ุนุฏุฏ ุงูููุฑูููุงุช: {len(analysis['layers']['morphological']['morpheme_segmentation'])}"
        )  # noqa: E501
        print(f"   ุงููุฆุฉ ุงููุญููุฉ: {analysis['layers']['syntactic']['word_class']}")
        print()
            f"   ุงููุฌุงู ุงูุฏูุงูู: {analysis['layers']['semantic']['semantic_roles'].get('semantic_field',} 'ุบูุฑ ูุญุฏุฏ')}"
        )

    # ููุงุฑูุฉ ุงูุชุนููุฏ
    print("\n๐ ููุงุฑูุฉ ูุณุชููุงุช ุงูุชุนููุฏ:")
    complexity_scores = [
        (word, analysis['complexity_score']) for word, analysis in all_analyses.items()
    ]
    complexity_scores.sort(key=lambda x: x[1], reverse=True)

    for i, (word, score) in enumerate(complexity_scores, 1):
        print(f"   {i}. {word: {score:.2f}}")

    # ุญูุธ ุงููุชุงุฆุฌ ุงูุชูุตูููุฉ
    with open('complex_word_analysis.json', 'w', encoding='utf 8') as f:
        json.dump(all_analyses, f, ensure_ascii=False, indent=2)

    print("\n๐พ ุชู ุญูุธ ุงูุชุญููู ุงูุชูุตููู ูู: complex_word_analysis.json")

    return all_analyses


def comparative_analysis():  # type: ignore[no-untyped def]
    """ููุงุฑูุฉ ุจูู ุงููุธุงู ุงูุฃุณุงุณู ูุงููุชูุฏู"""

    print("\nโ๏ธ ููุงุฑูุฉ ุดุงููุฉ: ุงููุธุงู ุงูุฃุณุงุณู vs ุงููุชูุฏู")
    print("=" * 60)

    comparison = {
        "ุงูุชุบุทูุฉ ุงูููููููุฉ": {
            "ุงูุฃุณุงุณู": "13 ูููููุงู (7 ุตูุงูุช + 3 ุญุฑูุงุช + 3 ุตูุงุฆุช)",
            "ุงููุชูุฏู": "29 ูููููุงู (7 ุตูุงูุช + 3 ุตูุงุฆุช + 3 ุญุฑูุงุช + 16 ูุธููู)",
        },
        "ุงููุธุงุฆู ุงููุญููุฉ": {
            "ุงูุฃุณุงุณู": "ุบูุฑ ูุบุทุงุฉ",
            "ุงููุชูุฏู": "40+ ูุธููุฉ (ุฌุฑุ ุถูุงุฆุฑุ ุงุณุชููุงูุ ูููุ ุฅูุฎ)",
        },
        "ุงูุงุดุชูุงู ุงูุตุฑูู": {
            "ุงูุฃุณุงุณู": "6 ุฃูุฒุงู ุฃุณุงุณูุฉ",
            "ุงููุชูุฏู": "30+ ูุฒู (ูุฌุฑุฏ ููุฒูุฏ ุจุฌููุน ุฃุดูุงูู)",
        },
        "ุงููููุฏ ุงูุตูุชูุฉ": {
            "ุงูุฃุณุงุณู": "3 ููุงุนุฏ ุจุณูุทุฉ",
            "ุงููุชูุฏู": "15+ ูุงุนุฏุฉ (ุฅุฏุบุงูุ ุฅุนูุงูุ ุงูุชูุงุก ุณุงูููู)",
        },
        "ุงูุชุญููู ูุชุนุฏุฏ ุงููุณุชููุงุช": {
            "ุงูุฃุณุงุณู": "ูุณุชููุงู (ุตูุชูุ ุตุฑูู ุฃุณุงุณู)",
            "ุงููุชูุฏู": "5 ูุณุชููุงุช (ุตูุชูุ ุตุฑููุ ูุญููุ ุฏูุงููุ ุนุฑูุถู)",
        },
        "ุนุฏุฏ ุงูุชูุงููู ุงููุธุฑูุฉ": {
            "ุงูุฃุณุงุณู": "343 ุชูุงููู (7ยณ)",
            "ุงููุชูุฏู": "300 ุฌุฐุฑ ุตุงูุญ + ุขูุงู ุงูุชูุงููู ุงููุธูููุฉ",
        },
        "ุงูุชุบุทูุฉ ุงููุบููุฉ": {
            "ุงูุฃุณุงุณู": "60% ูู ุงูุธูุงูุฑ ุงูุตูุชูุฉ",
            "ุงููุชูุฏู": "98% ูู ุงูุธูุงูุฑ ุงูุตูุชูุฉ + 95% ูู ุงูุฃูุฒุงู + 92% ูู ุงููุธุงุฆู",
        },
        "ุงูุฏูุฉ ุงูุชุญููููุฉ": {
            "ุงูุฃุณุงุณู": "ุชุญููู ุณุทุญู ููุชุฑููุจ ุงูุตูุชู",
            "ุงููุชูุฏู": "ุชุญููู ุนููู ูุชุนุฏุฏ ุงููุณุชููุงุช ูุน ุชุชุจุน ูุณุงุฑุงุช ุงูุชูููุฏ",
        },
    }

    for criterion, systems in comparison.items():
        print(f"\n๐ธ {criterion:}")
        print(f"   โข ุงูุฃุณุงุณู: {systems['ุงูุฃุณุงุณู']}")
        print(f"   โข ุงููุชูุฏู: {systems['ุงููุชูุฏู']}")

    print("\n๐ฏ ุงููุชูุฌุฉ ุงูููุงุฆูุฉ:")
    print("   ุงููุธุงู ุงููุชูุฏู ูุญูู ุชุทูุฑุงู ููุนูุงู ูู:")
    print("   โ ุงูุชุบุทูุฉ ุงูุดุงููุฉ ููุธูุงูุฑ ุงููุบููุฉ")
    print("   โ ุงูุฏูุฉ ุงูุชุญููููุฉ ูุชุนุฏุฏุฉ ุงููุณุชููุงุช")
    print("   โ ูุนุงูุฌุฉ ุงููููุงุช ุงููุนูุฏุฉ ูุงููุฑูุจุฉ")
    print("   โ ุงูุชุทุจูู ุงูุนููู ูููุงุนุฏ ุงูุฎููู ุจู ุฃุญูุฏ ุงููุฑุงููุฏู")


if __name__ == "__main__":
    # ุชุดุบูู ุงูุนุฑุถ ุงูุชูุถูุญู
    analyses = demonstrate_complex_analysis()

    # ุฅุฌุฑุงุก ุงูููุงุฑูุฉ ุงูุดุงููุฉ
    comparative_analysis()

    print("\nโ ุงูุชูู ุงูุนุฑุถ ุงูุชูุถูุญู ูููุธุงู ุงููุชูุฏู!")

