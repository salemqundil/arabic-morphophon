#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Pronouns Generator - Enhanced Version
===========================================
Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†

Enhanced version of the Arabic pronouns generator with improved syllable-to-pronoun
mapping algorithms and better pattern recognition capabilities.

Author: Arabic NLP Expert Team - GitHub Copilot
Version: 2.0.0 - ENHANCED PRONOUNS GENERATOR
Date: 2025-07-24
Encoding: UTF 8
"""

import logging
import re
from typing import Dict, List, Any, Tuple
import difflib
from arabic_pronouns_generator import ArabicPronounsGenerator, ArabicPronounsDatabase

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED SYLLABLE ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedSyllableAnalyzer:
    """Ù…Ø­Ù„Ù„ Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù†Ø·Ù‚ ÙˆØ§Ù„ØªØ´ÙƒÙŠÙ„"""

    def __init__(self):

        # Ù‚Ø§Ù…ÙˆØ³ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ØµÙˆØªÙŠØ©
        self.diacritic_to_phoneme = {
            'Ù': 'a',  # ÙØªØ­Ø©
            'Ù': 'i',  # ÙƒØ³Ø±Ø©
            'Ù': 'u',  # Ø¶Ù…Ø©
            'Ù’': '',  # Ø³ÙƒÙˆÙ†
            'Ù‹': 'an',  # ØªÙ†ÙˆÙŠÙ† ÙØªØ­
            'Ù': 'in',  # ØªÙ†ÙˆÙŠÙ† ÙƒØ³Ø±
            'ÙŒ': 'un',  # ØªÙ†ÙˆÙŠÙ† Ø¶Ù…
            'Ù‘': '',  # Ø´Ø¯Ø© (Ù…Ø¶Ø§Ø¹ÙØ©)
        }

        # Ø£Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠØ© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø§Ù„ØµÙˆØªÙŠ
        self.arabic_to_phoneme = {
            'Ø§': 'aa',
            'Ø¨': 'b',
            'Øª': 't',
            'Ø«': 'th',
            'Ø¬': 'j',
            'Ø­': 'h',
            'Ø®': 'kh',
            'Ø¯': 'd',
            'Ø°': 'dh',
            'Ø±': 'r',
            'Ø²': 'z',
            'Ø³': 's',
            'Ø´': 'sh',
            'Øµ': 's',
            'Ø¶': 'd',
            'Ø·': 't',
            'Ø¸': 'z',
            'Ø¹': 'Ê•',
            'Øº': 'gh',
            'Ù': 'f',
            'Ù‚': 'q',
            'Ùƒ': 'k',
            'Ù„': 'l',
            'Ù…': 'm',
            'Ù†': 'n',
            'Ù‡': 'h',
            'Ùˆ': 'w',
            'ÙŠ': 'y',
            'Ø£': 'a',
            'Ø¥': 'i',
            'Ø¢': 'aa',
            'Ø©': 'h',
            'Ù‰': 'aa',
            'Ø¡': 'Ê”',
        }

    def normalize_syllable(self, syllable: str) -> str:
        """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ"""

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ ÙˆØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£Ø­Ø±Ù
        normalized = syllable.strip()

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ù‡Ù…Ø²Ø©
        normalized = re.sub(r'[Ø£Ø¥Ø¢]', 'Ø§', normalized)
        normalized = re.sub(r'[Ù‰ÙŠ]', 'ÙŠ', normalized)
        normalized = re.sub(r'[Ø©]', 'Ù‡', normalized)

        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        normalized = re.sub(r'[Ù‹ÙŒÙÙ’]+', '', normalized)

        return normalized

    def calculate_syllable_similarity(self, syllable1: str, syllable2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        norm1 = self.normalize_syllable(syllable1)
        norm2 = self.normalize_syllable(syllable2)

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… SequenceMatcher Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()

        # ØªØ­Ø³ÙŠÙ† Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ©
        phonetic_similarity = self._calculate_phonetic_similarity(norm1, norm2)

        # Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ø­
        final_similarity = (similarity * 0.7) + (phonetic_similarity * 0.3)

        return final_similarity

    def _calculate_phonetic_similarity(self, syllable1: str, syllable2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØµÙˆØªÙŠ"""

        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ØµÙˆØªÙŠØ©
        phonetic1 = self._to_phonetic(syllable1)
        phonetic2 = self._to_phonetic(syllable2)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØµÙˆØªÙŠ
        if not phonetic1 or not phonetic2:
            return 0.0

        return difflib.SequenceMatcher(None, phonetic1, phonetic2).ratio()

    def _to_phonetic(self, text: str) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ØµÙˆØªÙŠØ©"""

        phonetic = ""
        i = 0

        while i < len(text):
            char = text[i]

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±Ù
            if char in self.arabic_to_phoneme:
                phonetic += self.arabic_to_phoneme[char]

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ´ÙƒÙŠÙ„
            if i + 1 < len(text) and text[i + 1] in self.diacritic_to_phoneme:
                phonetic += self.diacritic_to_phoneme[text[i + 1]]
                i += 1  # ØªØ®Ø·ÙŠ Ø§Ù„ØªØ´ÙƒÙŠÙ„

            i += 1

        return phonetic


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED PATTERN MATCHER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedPatternMatcher:
    """Ù…Ø·Ø§Ø¨Ù‚ Ø£Ù†Ù…Ø§Ø· Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠØ©"""

    def __init__(self, syllable_analyzer: EnhancedSyllableAnalyzer):

        self.analyzer = syllable_analyzer
        self.similarity_threshold = 0.7  # Ø­Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„

    def fuzzy_match_syllables()
        self, input_syllables: List[str], target_syllables: List[str]
    ) -> Tuple[float, List[str]]:
        """Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¶Ø¨Ø§Ø¨ÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹"""

        if len(input_syllables) != len(target_syllables):
            return 0.0, []

        total_similarity = 0.0
        matched_syllables = []

        for i, (input_syl, target_syl) in enumerate()
            zip(input_syllables, target_syllables)
        ):
            similarity = self.analyzer.calculate_syllable_similarity()
                input_syl, target_syl
            )
            total_similarity += similarity
            matched_syllables.append(f"{input_syl} â†’ {target_syl} ({similarity:.2f})")

        average_similarity = total_similarity / len(input_syllables)

        return average_similarity, matched_syllables

    def find_best_pronoun_matches()
        self, input_syllables: List[str], pronoun_database: ArabicPronounsDatabase
    ) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø§Ù„Ø¶Ù…Ø§Ø¦Ø±"""

        matches = []

        for pronoun in pronoun_database.pronouns:
            # ØªÙ‚Ø³ÙŠÙ… Ù†Øµ Ø§Ù„Ø¶Ù…ÙŠØ± Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹
            pronoun_syllables = self._split_pronoun_to_syllables(pronoun.text)

            if not pronoun_syllables:
                continue

            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ·Ø§Ø¨Ù‚
            similarity, match_details = self.fuzzy_match_syllables()
                input_syllables, pronoun_syllables
            )

            if similarity >= self.similarity_threshold:
                matches.append()
                    {
                        'pronoun': pronoun,
                        'similarity': similarity,
                        'input_syllables': input_syllables,
                        'pronoun_syllables': pronoun_syllables,
                        'match_details': match_details,
                        'confidence': self._calculate_confidence(similarity, pronoun),
                    }
                )

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        matches.sort(key=lambda x: x['similarity'], reverse=True)

        return matches

    def _split_pronoun_to_syllables(self, pronoun_text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¶Ù…ÙŠØ± Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹"""

        # Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø© Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        pronoun_syllables_map = {
            # Ø¶Ù…Ø§Ø¦Ø± Ù…Ù†ÙØµÙ„Ø©
            'Ø£Ù†Ø§': ['Ø£Ù', 'Ù†ÙØ§'],
            'Ù†Ø­Ù†': ['Ù†ÙØ­Ù’', 'Ù†Ù'],
            'Ø£Ù†Øª': ['Ø£ÙÙ†Ù’', 'ØªÙ'],
            'Ø£Ù†ØªÙ': ['Ø£ÙÙ†Ù’', 'ØªÙ'],
            'Ø£Ù†ØªÙ…': ['Ø£ÙÙ†Ù’', 'ØªÙÙ…Ù’'],
            'Ø£Ù†ØªÙ†': ['Ø£ÙÙ†Ù’', 'ØªÙÙ†ÙÙ‘'],
            'Ø£Ù†ØªÙ…Ø§': ['Ø£ÙÙ†Ù’', 'ØªÙ', 'Ù…ÙØ§'],
            'Ù‡Ùˆ': ['Ù‡Ù', 'ÙˆÙ'],
            'Ù‡ÙŠ': ['Ù‡Ù', 'ÙŠÙ'],
            'Ù‡Ù…': ['Ù‡ÙÙ…Ù’'],
            'Ù‡Ù†': ['Ù‡ÙÙ†ÙÙ‘'],
            'Ù‡Ù…Ø§': ['Ù‡Ù', 'Ù…ÙØ§'],
            # Ø¶Ù…Ø§Ø¦Ø± Ù…ØªØµÙ„Ø©
            'Ù€Ù†ÙŠ': ['Ù€Ù†ÙÙŠ'],
            'Ù€ÙŠ': ['Ù€ÙŠ'],
            'Ù€Ù†Ø§': ['Ù€Ù†ÙØ§'],
            'Ù€Ùƒ': ['Ù€ÙƒÙ'],
            'Ù€ÙƒÙ': ['Ù€ÙƒÙ'],
            'Ù€ÙƒÙ…': ['Ù€ÙƒÙÙ…Ù’'],
            'Ù€ÙƒÙ†': ['Ù€ÙƒÙÙ†ÙÙ‘'],
            'Ù€ÙƒÙ…Ø§': ['Ù€ÙƒÙ', 'Ù…ÙØ§'],
            'Ù€Ù‡': ['Ù€Ù‡Ù'],
            'Ù€Ù‡Ø§': ['Ù€Ù‡ÙØ§'],
            'Ù€Ù‡Ù…': ['Ù€Ù‡ÙÙ…Ù’'],
            'Ù€Ù‡Ù†': ['Ù€Ù‡ÙÙ†ÙÙ‘'],
            'Ù€Ù‡Ù…Ø§': ['Ù€Ù‡Ù', 'Ù…ÙØ§'],
        }

        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ù† Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ù…ØªØµÙ„Ø©
        clean_pronoun = pronoun_text.replace('Ù€', '')

        if clean_pronoun in pronoun_syllables_map:
            return pronoun_syllables_map[clean_pronoun]
        elif pronoun_text in pronoun_syllables_map:
            return pronoun_syllables_map[pronoun_text]

        # ØªÙ‚Ø³ÙŠÙ… Ø¨Ø³ÙŠØ· ÙƒØ­Ù„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ
        return [pronoun_text]

    def _calculate_confidence(self, similarity: float, pronoun) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©"""

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
        base_confidence = similarity
        frequency_bonus = pronoun.frequency_score * 0.2  # Ù…ÙƒØ§ÙØ£Ø© Ù„Ù„Ø¶Ù…Ø§Ø¦Ø± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„ØªÙƒØ±Ø§Ø±
        length_penalty = max(0, (len(pronoun.text) - 3) * 0.05)  # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø·ÙˆÙŠÙ„Ø©

        confidence = base_confidence + frequency_bonus - length_penalty

        return min(1.0, max(0.0, confidence))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED PRONOUNS GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedArabicPronounsGenerator(ArabicPronounsGenerator):
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†"""

    def __init__(self):

        super().__init__()
        self.syllable_analyzer = EnhancedSyllableAnalyzer()
        self.pattern_matcher = EnhancedPatternMatcher(self.syllable_analyzer)

        logger.info("ğŸš€ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù† v2.0")

    def generate_pronouns_from_syllables_enhanced()
        self, syllables: List[str]
    ) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ - Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©"""

        logger.info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù† Ø¹Ù† Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹: {syllables}")

        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        matches = self.pattern_matcher.find_best_pronoun_matches()
            syllables, self.pronouns_db
        )

        if not matches:
            return {
                'success': False,
                'message': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¶Ù…Ø§Ø¦Ø± Ù…Ø·Ø§Ø¨Ù‚Ø©',
                'input_syllables': syllables,
                'syllable_pattern': self.pattern_analyzer._determine_syllable_pattern()
                    syllables
                ),
                'confidence': 0.0,
                'pronouns': [],
                'suggestions': self._get_similar_patterns(syllables),
            }

        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        result_pronouns = []
        for match in matches[:5]:  # Ø£ÙØ¶Ù„ 5 ØªØ·Ø§Ø¨Ù‚Ø§Øª
            pronoun_data = {
                'text': match['pronoun'].text,
                'type': match['pronoun'].pronoun_type.value,
                'person': match['pronoun'].person.value,
                'number': match['pronoun'].number.value,
                'gender': match['pronoun'].gender.value,
                'frequency': match['pronoun'].frequency_score,
                'similarity': match['similarity'],
                'confidence': match['confidence'],
                'match_details': match['match_details'],
                'syllable_breakdown': match['pronoun_syllables'],
            }
            result_pronouns.append(pronoun_data)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_confidence = max(match['confidence'] for match in matches)

        return {
            'success': True,
            'input_syllables': syllables,
            'syllable_pattern': self.pattern_analyzer._determine_syllable_pattern()
                syllables
            ),
            'confidence': overall_confidence,
            'total_matches': len(matches),
            'pronouns': result_pronouns,
            'best_match': result_pronouns[0] if result_pronouns else None,
            'analysis': {
                'input_pattern': self._analyze_input_pattern(syllables),
                'match_quality': self._assess_match_quality(matches),
                'recommendations': self._get_recommendations(matches),
            },
        }

    def _get_similar_patterns(self, syllables: List[str]) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø§Ø¨Ù‡Ø© ÙƒØ§Ù‚ØªØ±Ø§Ø­Ø§Øª"""

        suggestions = []
        current_pattern = self.pattern_analyzer._determine_syllable_pattern(syllables)

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø§Ø¨Ù‡Ø©
        for pattern, pattern_pronouns in self.pronouns_db.syllable_patterns.items():
            if pattern != current_pattern and len(len(pattern_pronouns) -> 0) > 0:
                similarity = difflib.SequenceMatcher()
                    None, current_pattern, pattern
                ).ratio()
                if similarity > 0.5:
                    suggestions.append(f"{pattern} ({len(pattern_pronouns)} Ø¶Ù…ÙŠØ±)")

        return suggestions[:3]  # Ø£ÙØ¶Ù„ 3 Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª

    def _analyze_input_pattern(self, syllables: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„"""

        return {
            'syllable_count': len(syllables),
            'pattern': self.pattern_analyzer._determine_syllable_pattern(syllables),
            'complexity': self._calculate_pattern_complexity(syllables),
            'normalized_syllables': [
                self.syllable_analyzer.normalize_syllable(syl) for syl in syllables
            ],
        }

    def _assess_match_quality(self, matches: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""

        if not matches:
            return {'quality': 'Ù…Ù†Ø®ÙØ¶', 'score': 0.0}

        best_similarity = matches[0]['similarity']
        average_confidence = sum(m['confidence'] for m in matches) / len(matches)

        if best_similarity >= 0.9 and average_confidence >= 0.8:
            quality = 'Ù…Ù…ØªØ§Ø²'
        elif best_similarity >= 0.8 and average_confidence >= 0.7:
            quality = 'Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹'
        elif best_similarity >= 0.7 and average_confidence >= 0.6:
            quality = 'Ø¬ÙŠØ¯'
        elif best_similarity >= 0.6:
            quality = 'Ù…Ù‚Ø¨ÙˆÙ„'
        else:
            quality = 'Ù…Ù†Ø®ÙØ¶'

        return {
            'quality': quality,
            'score': best_similarity,
            'confidence': average_confidence,
            'match_count': len(matches),
        }

    def _get_recommendations(self, matches: List[Dict[str, Any]]) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ§Øª"""

        recommendations = []

        if not matches:
            recommendations.append("ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©")
            recommendations.append("Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚Ø§Ø·Ø¹ Ø£Ø¨Ø³Ø·")
            return recommendations

        best_match = matches[0]

        if best_match['similarity'] < 0.8:
            recommendations.append("Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ·Ø§Ø¨Ù‚")

        if best_match['confidence'] < 0.7:
            recommendations.append("Ø¶Ø¹ ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¶Ù…Ø§Ø¦Ø± Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹")

        if len(matches) == 1:
            recommendations.append()
                "Ø¬Ø±Ø¨ Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø¥Ø²Ø§Ù„Ø© Ù…Ù‚Ø·Ø¹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª"
            )

        if not recommendations:
            recommendations.append("Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù…ØªØ§Ø²Ø© - Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙˆØµÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©")

        return recommendations

    def _calculate_pattern_complexity(self, syllables: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·"""

        complexity = len(syllables)  # Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

        for syllable in syllables:
            # ØªØ¹Ù‚ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø·Ø¹
            complexity += len(syllable) * 0.1

            # ØªØ¹Ù‚ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´ÙƒÙŠÙ„
            diacritics = len(re.findall(r'[ÙÙÙÙ’Ù‹ÙŒÙÙ‘]', syllable))
            complexity += diacritics * 0.2

        return complexity


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTING AND DEMONSTRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def demo_enhanced_generator():
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­Ø³Ù†"""

    print("ğŸš€ Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­Ø³Ù†")
    print("=" * 50)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­Ø³Ù†
    generator = EnhancedArabicPronounsGenerator()

    # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
    test_cases = [
        ['Ø£Ù', 'Ù†ÙØ§'],  # Ø£Ù†Ø§
        ['Ù‡Ù', 'ÙˆÙ'],  # Ù‡Ùˆ
        ['Ù‡Ù', 'ÙŠÙ'],  # Ù‡ÙŠ
        ['Ù†ÙØ­Ù’', 'Ù†Ù'],  # Ù†Ø­Ù†
        ['Ø£ÙÙ†Ù’', 'ØªÙ'],  # Ø£Ù†Øª
        ['Ù€Ù†ÙÙŠ'],  # Ù€Ù†ÙŠ
        ['Ù€Ù‡ÙØ§'],  # Ù€Ù‡Ø§
        ['Ù€ÙƒÙ'],  # Ù€Ùƒ
        ['Ø£ÙÙ†Ù’', 'ØªÙÙ…Ù’'],  # Ø£Ù†ØªÙ…
        ['Ù‡ÙÙ…Ù’'],  # Ù‡Ù…
        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø·ÙÙŠÙØ©
        ['Ø£Ù', 'Ù†Ù'],  # Ø£Ù†Ø§ (Ù…Ø¹ Ø­Ø°Ù Ø¢Ø®Ø±)
        ['Ù‡Ù', 'Ùˆ'],  # Ù‡Ùˆ (Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„)
        ['Ù†ÙØ­Ù', 'Ù†Ù'],  # Ù†Ø­Ù† (Ù…Ø¹ ØªØºÙŠÙŠØ± ØªØ´ÙƒÙŠÙ„)
    ]

    for i, syllables in enumerate(test_cases, 1):
        print(f"\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± {i}: {syllables}")
        print(" " * 30)

        result = generator.generate_pronouns_from_syllables_enhanced(syllables)

        if result['success']:
            best_match = result['best_match']
            print(f"âœ… Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚: {best_match['text']}")
            print(f"   Ø§Ù„Ù†ÙˆØ¹: {best_match['type']}")
            print(f"   Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {best_match['similarity']:.2f}")
            print(f"   Ø§Ù„Ø«Ù‚Ø©: {best_match['confidence']:.2f}")
            print(f"   Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {result['analysis']['match_quality']['quality']}")

            if len(result['pronouns']) > 1:
                print(f"   ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {len(result['pronouns'])} - 1}")
        else:
            print(f"âŒ {result['message']}")
            if result.get('suggestions'):
                print(f"   Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª: {', '.join(result['suggestions'])}")


if __name__ == "__main__":
    demo_enhanced_generator()

