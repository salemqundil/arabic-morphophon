"""
ðŸ”¤ Arabic Text Normalizer (Ù…Ø­Ø³Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
Comprehensive normalization for historical, dialectal, and classical Arabic texts,
    Handles spelling variants, diacritics, and era specific conventions
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import re
    class ArabicEra(Enum):
    """Historical eras of Arabic language"""

    PRE_ISLAMIC = "Ø§Ù„Ø¬Ø§Ù‡Ù„ÙŠØ©"
    EARLY_ISLAMIC = "ØµØ¯Ø± Ø§Ù„Ø¥Ø³Ù„Ø§Ù…"
    UMAYYAD = "Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø£Ù…ÙˆÙŠ"
    ABBASID = "Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø¹Ø¨Ø§Ø³ÙŠ"
    ANDALUSIAN = "Ø§Ù„Ø£Ù†Ø¯Ù„Ø³ÙŠ"
    MAMLUK = "Ø§Ù„Ù…Ù…Ù„ÙˆÙƒÙŠ"
    OTTOMAN = "Ø§Ù„Ø¹Ø«Ù…Ø§Ù†ÙŠ"
    MODERN = "Ø§Ù„Ø­Ø¯ÙŠØ«"
    CONTEMPORARY = "Ø§Ù„Ù…Ø¹Ø§ØµØ±"


class ArabicDialect(Enum):
    """Major Arabic dialect groups"""

    CLASSICAL = "ÙØµØ­Ù‰ Ø§Ù„ØªØ±Ø§Ø«"
    MODERN_STANDARD = "ÙØµØ­Ù‰ Ø§Ù„Ø¹ØµØ±"
    EGYPTIAN = "Ù…ØµØ±ÙŠ"
    LEVANTINE = "Ø´Ø§Ù…ÙŠ"
    GULF = "Ø®Ù„ÙŠØ¬ÙŠ"
    MAGHREBI = "Ù…ØºØ±Ø¨ÙŠ"
    IRAQI = "Ø¹Ø±Ø§Ù‚ÙŠ"
    SUDANESE = "Ø³ÙˆØ¯Ø§Ù†ÙŠ"
    YEMENI = "ÙŠÙ…Ù†ÙŠ"


@dataclass,
    class NormalizationRule:
    """Rule for text normalization"""

    pattern: str,
    replacement: str,
    era: Optional[ArabicEra]
    dialect: Optional[ArabicDialect]
    context: List[str]
    priority: int,
    description: str,
    class ArabicNormalizer:
    """Comprehensive Arabic text normalizer"""

    def __init__(self):
    self.historical_variants = self._initialize_historical_variants()
    self.dialect_mappings = self._initialize_dialect_mappings()
    self.diacritic_rules = self._initialize_diacritic_rules()
    self.orthographic_rules = self._initialize_orthographic_rules()
    self.normalization_rules = self._initialize_normalization_rules()

    def _initialize_historical_variants(self) -> Dict[ArabicEra, Dict[str, str]]:
    """Initialize historical spelling variants by era"""
    return {
    ArabicEra.PRE_ISLAMIC: {
                # Ancient script variations
    'Ù„Ø§Ù‡': 'Ø§Ù„Ù„Ù‡',
    'Ø§Ù„Ø±Ø­Ù…Ù†': 'Ø§Ù„Ø±Ø­Ù…Ø§Ù†',
    'Ø¥Ù„Ù‡': 'Ø¥Ù„Ø§Ù‡',
    'ÙƒÙ„Ù…': 'ÙƒÙ„Ø§Ù…',
    'Ø¬Ø¨Ù„': 'Ø¬Ø¨Ø§Ù„',
    'Ù†Ù‡Ø±': 'Ø£Ù†Ù‡Ø§Ø±',
                # Old poetry conventions
    'Ø³ÙŠÙ': 'Ø³ÙŠÙˆÙ',
    'Ø¨ÙŠØª': 'Ø¨ÙŠÙˆØª',
    'ÙØ±Ø³': 'Ø£ÙØ±Ø§Ø³',
    },
    ArabicEra.EARLY_ISLAMIC: {
                # Quranic spelling conventions
    'Ø§Ù„ØµÙ„ÙˆØ©': 'Ø§Ù„ØµÙ„Ø§Ø©',
    'Ø§Ù„Ø²ÙƒÙˆØ©': 'Ø§Ù„Ø²ÙƒØ§Ø©',
    'Ø§Ù„Ø­ÙŠÙˆØ©': 'Ø§Ù„Ø­ÙŠØ§Ø©',
    'Ù…Ø´ÙƒÙˆØ©': 'Ù…Ø´ÙƒØ§Ø©',
    'Ù†Ø¬ÙˆØ©': 'Ù†Ø¬Ø§Ø©',
    'Ø±Ø­Ù…Ø©': 'Ø±Ø­Ù…Øª',  # Ottoman influence
    'Ø­ÙƒÙ…Ø©': 'Ø­ÙƒÙ…Øª',
    },
    ArabicEra.ABBASID: {
                # Classical Arabic standardization
    'Ø¥Ø¨Ù†': 'Ø§Ø¨Ù†',
    'Ø¥Ø¨Ù†Ø©': 'Ø§Ø¨Ù†Ø©',
    'Ø¥Ø«Ù†Ø§Ù†': 'Ø§Ø«Ù†Ø§Ù†',
    'Ø¥Ø«Ù†ÙŠÙ†': 'Ø§Ø«Ù†ÙŠÙ†',
    'Ø¥Ø³Ù…': 'Ø§Ø³Ù…',
    'Ø¥Ø³ØªØ·Ø§Ø¹': 'Ø§Ø³ØªØ·Ø§Ø¹',
                # Philosophical/scientific terms
    'Ø§Ù„ÙÙ„Ø³ÙØ©': 'Ø§Ù„ÙÙ„Ø³ÙÙ‡',
    'Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©': 'Ø§Ù„Ø·Ø¨ÙŠØ¹Ù‡',
    'Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¡': 'Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø£',
    },
    ArabicEra.ANDALUSIAN: {
                # Andalusian Arabic variants
    'Ù‚Ø±Ø·Ø¨Ø©': 'Ù‚Ø±Ø·Ø¨Ù‡',
    'Ø¥Ø´Ø¨ÙŠÙ„ÙŠØ©': 'Ø¥Ø´Ø¨ÙŠÙ„ÙŠÙ‡',
    'ØºØ±Ù†Ø§Ø·Ø©': 'ØºØ±Ù†Ø§Ø·Ù‡',
    'Ø§Ù„Ù…ÙˆØ´Ø­': 'Ø§Ù„Ù…ÙˆØ´Ø­Ø§Øª',
    'Ø§Ù„Ø²Ø¬Ù„': 'Ø§Ù„Ø²Ø¬Ø§Ù„',
                # Romance language influence
    'Ø§Ù„Ø£Ù†Ø¯Ù„Ø³': 'Ø§Ù„Ø£Ù†Ø¯Ù„ÙˆØ³',
    'Ø§Ù„Ø¨Ø±ØªÙ‚Ø§Ù„': 'Ø§Ù„Ù†Ø§Ø±Ù†Ø¬',
    },
    ArabicEra.MODERN: {
                # Modern spelling reforms
    'Ù‡Ø°Ù‡': 'Ù‡Ø§Ø°Ù‡',
    'Ù‡Ø¤Ù„Ø§Ø¡': 'Ù‡Ø§Ø¤Ù„Ø§Ø¡',
    'Ø£ÙˆÙ„Ø¦Ùƒ': 'Ø£ÙˆÙ„Ø§Ø¦Ùƒ',
    'Ø¥Ù…Ø±Ø£Ø©': 'Ø§Ù…Ø±Ø£Ø©',
    'Ù…Ø±Ø£Ø©': 'Ø§Ù…Ø±Ø£Ø©',
                # Borrowed terms standardization
    'Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†': 'Ø§Ù„ØªÙ„ÙØ²ÙŠÙˆÙ†',
    'Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±': 'Ø§Ù„Ø­Ø§Ø³ÙˆØ¨',
    'Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª': 'Ø§Ù„Ø´Ø§Ø¨ÙƒØ©',
    },
    }

    def _initialize_dialect_mappings(self) -> Dict[ArabicDialect, Dict[str, str]]:
    """Initialize dialect-to standard mappings"""
    return {
    ArabicDialect.EGYPTIAN: {
                # Egyptian colloquial to MSA
    'Ø¥ÙŠÙ‡': 'Ù…Ø§',
    'Ø¯Ù‡': 'Ù‡Ø°Ø§',
    'Ø¯ÙŠ': 'Ù‡Ø°Ù‡',
    'Ø¯ÙˆÙ„': 'Ù‡Ø¤Ù„Ø§Ø¡',
    'ÙƒØ¯Ù‡': 'Ù‡ÙƒØ°Ø§',
    'ÙÙŠÙ†': 'Ø£ÙŠÙ†',
    'Ø¥Ù…ØªÙ‰': 'Ù…ØªÙ‰',
    'Ù„ÙŠÙ‡': 'Ù„Ù…Ø§Ø°Ø§',
    'Ø§Ø²Ø§ÙŠ': 'ÙƒÙŠÙ',
    'Ø¹Ø§ÙŠØ²': 'Ø£Ø±ÙŠØ¯',
    'Ø¹Ø§ÙˆØ²': 'Ø£Ø±ÙŠØ¯',
    'Ø¬Ø§ÙŠ': 'Ø¢Øª',
    'Ø±Ø§ÙŠØ­': 'Ø°Ø§Ù‡Ø¨',
    'Ù‚Ø§Ø¹Ø¯': 'Ø¬Ø§Ù„Ø³',
    'ÙˆØ§Ù‚Ù': 'ÙˆØ§Ù‚Ù',
    'Ù†Ø§ÙŠÙ…': 'Ù†Ø§Ø¦Ù…',
    'ØµØ§Ø­ÙŠ': 'Ù…Ø³ØªÙŠÙ‚Ø¸',
    'Ø¬Ø¹Ø§Ù†': 'Ø¬Ø§Ø¦Ø¹',
    'Ø¹Ø·Ø´Ø§Ù†': 'Ø¹Ø·Ø´',
    },
    ArabicDialect.LEVANTINE: {
                # Levantine to MSA
    'Ø´Ùˆ': 'Ù…Ø§',
    'Ù‡Ø§Ø¯': 'Ù‡Ø°Ø§',
    'Ù‡Ø§ÙŠ': 'Ù‡Ø°Ù‡',
    'Ù‡Ø¯ÙˆÙ„': 'Ù‡Ø¤Ù„Ø§Ø¡',
    'Ù‡ÙŠÙƒ': 'Ù‡ÙƒØ°Ø§',
    'ÙˆÙŠÙ†': 'Ø£ÙŠÙ†',
    'Ø¥ÙŠÙ…ØªØ§': 'Ù…ØªÙ‰',
    'Ù„ÙŠØ´': 'Ù„Ù…Ø§Ø°Ø§',
    'ÙƒÙŠÙ': 'ÙƒÙŠÙ',
    'Ø¨Ø¯ÙŠ': 'Ø£Ø±ÙŠØ¯',
    'Ø¨Ø¯Ùˆ': 'ÙŠØ±ÙŠØ¯',
    'Ø¬Ø§ÙŠ': 'Ø¢Øª',
    'Ø±Ø§ÙŠØ­': 'Ø°Ø§Ù‡Ø¨',
    'Ù‚Ø§Ø¹Ø¯': 'Ø¬Ø§Ù„Ø³',
    'ÙˆØ§Ù‚Ù': 'ÙˆØ§Ù‚Ù',
    'Ù†Ø§ÙŠÙ…': 'Ù†Ø§Ø¦Ù…',
    'ØµØ§Ø­ÙŠ': 'Ù…Ø³ØªÙŠÙ‚Ø¸',
    },
    ArabicDialect.GULF: {
                # Gulf Arabic to MSA
    'Ø´Ù†Ùˆ': 'Ù…Ø§',
    'Ø°Ø§': 'Ù‡Ø°Ø§',
    'Ø°ÙŠ': 'Ù‡Ø°Ù‡',
    'Ø°ÙˆÙ„': 'Ù‡Ø¤Ù„Ø§Ø¡',
    'Ø¬Ø°ÙŠ': 'Ù‡ÙƒØ°Ø§',
    'ÙˆÙŠÙ†': 'Ø£ÙŠÙ†',
    'Ù…ØªØ§': 'Ù…ØªÙ‰',
    'Ù„ÙŠØ´': 'Ù„Ù…Ø§Ø°Ø§',
    'Ø´Ù„ÙˆÙ†': 'ÙƒÙŠÙ',
    'Ø£Ø¨ÙŠ': 'Ø£Ø±ÙŠØ¯',
    'ÙŠØ¨ÙŠ': 'ÙŠØ±ÙŠØ¯',
    'Ø¬Ø§ÙŠ': 'Ø¢Øª',
    'Ø±Ø§ÙŠØ­': 'Ø°Ø§Ù‡Ø¨',
    'Ù‚Ø§Ø¹Ø¯': 'Ø¬Ø§Ù„Ø³',
    'ÙˆØ§Ù‚Ù': 'ÙˆØ§Ù‚Ù',
    'Ù†Ø§ÙŠÙ…': 'Ù†Ø§Ø¦Ù…',
    'ØµØ§Ø­ÙŠ': 'Ù…Ø³ØªÙŠÙ‚Ø¸',
    },
    ArabicDialect.MAGHREBI: {
                # Maghrebi to MSA (simplified)
    'Ø¢Ø´': 'Ù…Ø§',
    'Ù‡Ø§Ø°Ø§': 'Ù‡Ø°Ø§',
    'Ù‡Ø§Ø°ÙŠ': 'Ù‡Ø°Ù‡',
    'Ù‡Ø§Ø°Ùˆ': 'Ù‡Ø¤Ù„Ø§Ø¡',
    'Ù‡ÙƒØ°Ø§': 'Ù‡ÙƒØ°Ø§',
    'ÙÙŠÙ†': 'Ø£ÙŠÙ†',
    'ÙÙˆÙ‚ØªØ§Ø´': 'Ù…ØªÙ‰',
    'Ø¹Ù„Ø§Ø´': 'Ù„Ù…Ø§Ø°Ø§',
    'ÙƒÙŠÙØ§Ø´': 'ÙƒÙŠÙ',
    'Ø¨ØºÙŠØª': 'Ø£Ø±ÙŠØ¯',
    'Ø¬Ø§ÙŠ': 'Ø¢Øª',
    'Ù…Ø§Ø´ÙŠ': 'Ø°Ø§Ù‡Ø¨',
    'Ù‚Ø§Ø¹Ø¯': 'Ø¬Ø§Ù„Ø³',
    'ÙˆØ§Ù‚Ù': 'ÙˆØ§Ù‚Ù',
    'Ø±Ø§Ù‚Ø¯': 'Ù†Ø§Ø¦Ù…',
    'ÙØ§ÙŠÙ‚': 'Ù…Ø³ØªÙŠÙ‚Ø¸',
    },
    }

    def _initialize_diacritic_rules(self) -> Dict[str, NormalizationRule]:
    """Initialize diacritic normalization rules"""
    return {
    'remove_all_diacritics': NormalizationRule(
    pattern=r'[ÙŽÙÙÙ‘Ù’Ù°Ù±Ù²Ù³Ù´ÙµÙ¶Ù·Ù¸Ù¹ÙºÙ»Ù¼Ù½Ù¾Ù¿]',
    replacement='',
    era=None,
    dialect=None,
    context=['casual_text', 'modern_writing'],
    priority=1,
    description='Remove all diacritical marks',
    ),
    'preserve_essential_diacritics': NormalizationRule(
    pattern=r'[ÙŽÙÙ]',
    replacement='',
    era=None,
    dialect=None,
    context=['classical_text', 'quranic_text'],
    priority=10,  # High priority to preserve,
    description='Preserve essential diacritics in classical texts',
    ),
    'normalize_tanween': NormalizationRule(
    pattern=r'[Ù‹ÙŒÙ]',
    replacement='',
    era=None,
    dialect=None,
    context=['simplified_text'],
    priority=2,
    description='Remove tanween marks',
    ),
    'normalize_shadda': NormalizationRule(
    pattern=r'Ù‘',
    replacement='',
    era=None,
    dialect=None,
    context=['simplified_text'],
    priority=3,
    description='Remove shadda (gemination) marks',
    ),
    }

    def _initialize_orthographic_rules(self) -> List[NormalizationRule]:
    """Initialize orthographic normalization rules"""
    return [
            # Alef variations,
    NormalizationRule(
    pattern=r'[Ø¢Ø£Ø¥]',
    replacement='Ø§',
    era=None,
    dialect=None,
    context=['general'],
    priority=1,
    description='Normalize alef variations',
    ),
            # Yeh variations,
    NormalizationRule(
    pattern=r'[Ù‰Ø¦ÙŠ]',
    replacement='ÙŠ',
    era=None,
    dialect=None,
    context=['general'],
    priority=1,
    description='Normalize yeh variations',
    ),
            # Teh marbuta,
    NormalizationRule(
    pattern=r'Ø©',
    replacement='Ù‡',
    era=None,
    dialect=None,
    context=['phonetic_matching'],
    priority=2,
    description='Normalize teh marbuta to heh',
    ),
            # Waw hamza,
    NormalizationRule(
    pattern=r'Ø¤',
    replacement='Ùˆ',
    era=None,
    dialect=None,
    context=['simplified'],
    priority=2,
    description='Normalize waw with hamza',
    ),
            # Yeh hamza,
    NormalizationRule(
    pattern=r'Ø¦',
    replacement='ÙŠ',
    era=None,
    dialect=None,
    context=['simplified'],
    priority=2,
    description='Normalize yeh with hamza',
    ),
    ]

    def _initialize_normalization_rules(self) -> List[NormalizationRule]:
    """Initialize comprehensive normalization rules"""
    rules = []

        # Add diacritic rules,
    rules.extend(self.diacritic_rules.values())

        # Add orthographic rules,
    rules.extend(self.orthographic_rules)

        # Sort by priority,
    rules.sort(key=lambda r: r.priority)

    return rules,
    def normalize_text(
    self,
    text: str,
    target_era: Optional[ArabicEra] = None,
    target_dialect: Optional[ArabicDialect] = None,
    context: List[str] = None,
    preserve_diacritics: bool = False,
    ) -> Dict:
    """Comprehensive text normalization"""

        if context is None:
    context = ['general']

    original_text = text,
    normalized_text = text

        # Step 1: Historical variant normalization,
    if target_era:
    normalized_text = self._normalize_historical_variants(
    normalized_text, target_era
    )

        # Step 2: Dialect normalization,
    if target_dialect and target_dialect != ArabicDialect.CLASSICAL:
    normalized_text = self._normalize_dialect(normalized_text, target_dialect)

        # Step 3: Orthographic normalization,
    normalized_text = self._apply_orthographic_rules(normalized_text, context)

        # Step 4: Diacritic handling,
    if not preserve_diacritics:
    normalized_text = self._normalize_diacritics(normalized_text, context)

        # Step 5: Final cleanup,
    normalized_text = self._final_cleanup(normalized_text)

        # Generate normalization report,
    changes = self._generate_change_report(original_text, normalized_text)

    return {
    'original_text': original_text,
    'normalized_text': normalized_text,
    'target_era': target_era.value if target_era else None,
    'target_dialect': target_dialect.value if target_dialect else None,
    'context': context,
    'preserve_diacritics': preserve_diacritics,
    'changes_made': changes,
    'similarity_score': self._calculate_similarity(
    original_text, normalized_text
    ),
    }

    def _normalize_historical_variants(self, text: str, era: ArabicEra) -> str:
    """Normalize historical spelling variants"""
        if era not in self.historical_variants:
    return text,
    normalized = text,
    variants = self.historical_variants[era]

        for old_form, new_form in variants.items():
    normalized = re.sub(rf'\b{re.escape(old_form)}\b', new_form, normalized)

    return normalized,
    def _normalize_dialect(self, text: str, target_dialect: ArabicDialect) -> str:
    """Normalize dialectal forms to standard Arabic"""
        if target_dialect not in self.dialect_mappings:
    return text,
    normalized = text,
    mappings = self.dialect_mappings[target_dialect]

        for dialect_form, standard_form in mappings.items():
    normalized = re.sub(
    rf'\b{re.escape(dialect_form)}\b', standard_form, normalized
    )

    return normalized,
    def _apply_orthographic_rules(self, text: str, context: List[str]) -> str:
    """Apply orthographic normalization rules"""
    normalized = text,
    for rule in self.orthographic_rules:
            if not rule.context or any(ctx in context for ctx in rule.context):
    normalized = re.sub(rule.pattern, rule.replacement, normalized)

    return normalized,
    def _normalize_diacritics(self, text: str, context: List[str]) -> str:
    """Normalize diacritical marks based on context"""
    normalized = text

        # Apply diacritic rules based on context,
    for rule_name, rule in self.diacritic_rules.items():
            if rule.context and any(ctx in context for ctx in rule.context):
                if (
    rule_name != 'preserve_essential_diacritics'
    ):  # Skip preservation rule,
    normalized = re.sub(rule.pattern, rule.replacement, normalized)

    return normalized,
    def _final_cleanup(self, text: str) -> str:
    """Final text cleanup and standardization"""
        # Remove extra whitespace,
    cleaned = re.sub(r'\s+', ' ', text)

        # Remove leading/trailing whitespace,
    cleaned = cleaned.strip()

        # Normalize punctuation,
    cleaned = re.sub(r'[ØŒØØŽØØ˜Ø™ØšØØ‘Ø’Ø“Ø”Ø•Ø–Ø—]', 'ØŒ', cleaned)
    cleaned = re.sub(r'[Ø›Øž]', 'Ø›', cleaned)
    cleaned = re.sub(r'[ØŸ]', 'ØŸ', cleaned)

    return cleaned,
    def _generate_change_report(self, original: str, normalized: str) -> List[Dict]:
    """Generate report of changes made during normalization"""
    changes = []

        # Simple word-level comparison,
    original_words = original.split()
    normalized_words = normalized.split()

        for i, (orig_word, norm_word) in enumerate(
    zip(original_words, normalized_words)
    ):
            if orig_word != norm_word:
    changes.append(
    {
    'position': i,
    'original': orig_word,
    'normalized': norm_word,
    'change_type': self._classify_change(orig_word, norm_word),
    }
    )

    return changes,
    def _classify_change(self, original: str, normalized: str) -> str:
    """Classify the type of change made"""
        # Remove diacritics for comparison,
    orig_no_diac = re.sub(r'[ÙŽÙÙÙ‘Ù’Ù°]', '', original)
    norm_no_diac = re.sub(r'[ÙŽÙÙÙ‘Ù’Ù°]', '', normalized)

        if orig_no_diac == norm_no_diac:
    return 'diacritic_removal'
        elif len(original) != len(normalized):
    return 'length_change'
        elif original[0] != normalized[0]:
    return 'initial_change'
        else:
    return 'character_substitution'

    def _calculate_similarity(self, text1: str, text2: str) -> float:
    """Calculate similarity between original and normalized text"""
        if not text1 or not text2:
    return 0.0

        # Simple character-level similarity,
    max_len = max(len(text1), len(text2))
    matches = sum(1 for i, j in zip(text1, text2) if i == j)

    return matches / max_len,
    def detect_era(self, text: str) -> Tuple[ArabicEra, float]:
    """Detect the most likely historical era of a text"""
    era_scores = {}

        for era, variants in self.historical_variants.items():
    score = 0,
    for old_form in variants.keys():
                if re.search(rf'\b{re.escape(old_form)}\b', text):
    score += 1

            # Normalize by total possible variants,
    era_scores[era] = score / len(variants) if variants else 0,
    if not era_scores:
    return ArabicEra.MODERN, 0.0,
    best_era = max(era_scores, key=era_scores.get)
    confidence = era_scores[best_era]

    return best_era, confidence,
    def detect_dialect(self, text: str) -> Tuple[ArabicDialect, float]:
    """Detect the most likely dialect of a text"""
    dialect_scores = {}

        for dialect, mappings in self.dialect_mappings.items():
    score = 0,
    for dialect_form in mappings.keys():
                if re.search(rf'\b{re.escape(dialect_form)}\b', text):
    score += 1

            # Normalize by total possible forms,
    dialect_scores[dialect] = score / len(mappings) if mappings else 0,
    if not dialect_scores:
    return ArabicDialect.MODERN_STANDARD, 0.0,
    best_dialect = max(dialect_scores, key=dialect_scores.get)
    confidence = dialect_scores[best_dialect]

    return best_dialect, confidence,
    def convert_between_eras(
    self, text: str, source_era: ArabicEra, target_era: ArabicEra
    ) -> Dict:
    """Convert text between different historical eras"""

        # First normalize from source era,
    intermediate = self._normalize_historical_variants(text, source_era)

        # Then apply target era conventions (reverse mapping)
        if target_era in self.historical_variants:
    target_variants = {
    v: k for k, v in self.historical_variants[target_era].items()
    }
    final_text = intermediate,
    for standard_form, era_form in target_variants.items():
    final_text = re.sub(
    rf'\b{re.escape(standard_form)}\b', era_form, final_text
    )
        else:
    final_text = intermediate,
    return {
    'original_text': text,
    'source_era': source_era.value,
    'target_era': target_era.value,
    'converted_text': final_text,
    'conversion_quality': self._assess_conversion_quality(text, final_text),
    }

    def _assess_conversion_quality(self, original: str, converted: str) -> Dict:
    """Assess the quality of era conversion"""
    return {
    'character_similarity': self._calculate_similarity(original, converted),
    'word_count_original': len(original.split()),
    'word_count_converted': len(converted.split()),
    'length_ratio': len(converted) / len(original) if original else 0,
    'preservation_score': 1.0
    - abs(len(original) - len(converted))
    / max(len(original), len(converted), 1),
    }
