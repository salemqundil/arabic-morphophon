#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Arabic Function Words Generator using Syllable Database
======================================================
Ù…ÙˆÙ„Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©,
    ÙŠØ³ØªØ®Ø¯Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© (22,218 Ù…Ù‚Ø·Ø¹) Ù„ØªÙˆÙ„ÙŠØ¯:
- Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± (ÙÙŠØŒ Ø¹Ù„Ù‰ØŒ Ø¥Ù„Ù‰ØŒ ...)
- Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¹Ø·Ù (ÙˆØŒ ÙØŒ Ø«Ù…ØŒ ...)
- Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù†ÙÙŠ (Ù„Ø§ØŒ Ù„Ù…ØŒ Ù„Ù†ØŒ ...)
- Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù… (Ù…Ù†ØŒ Ù…Ø§Ø°Ø§ØŒ Ù…ØªÙ‰ØŒ ...)
- Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ù„Ø¥Ø´Ø§Ø±Ø© (Ø§Ù„ØŒ Ù‡Ø°Ø§ØŒ ØªÙ„ÙƒØŒ ...)

Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ·ÙˆØ±:
- ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ (6 ÙØ¦Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©)
- Ø£Ù†Ù…Ø§Ø· ØµÙˆØªÙŠØ© Ù…Ø®ØµØµØ© Ù„ÙƒÙ„ ÙØ¦Ø©
- Ù‚ÙŠÙˆØ¯ ØµÙˆØªÙŠØ© Ø°ÙƒÙŠØ© ÙˆÙ…ØªØ®ØµØµØ©
- Ù†Ø¸Ø§Ù… ØªØ´Ø§Ø¨Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
- ØªØ­Ù„ÙŠÙ„ ØµÙˆØªÙŠ Ø´Ø§Ù…Ù„
- ØªÙ‚Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠ ØªÙØµÙŠÙ„ÙŠ,
    Author: GitHub Copilot Arabic NLP Expert,
    Version: 2.0.0 - ENHANCED FUNCTION WORDS GENERATOR,
    Date: 2025-07-26,
    Encoding: UTF 8
"""

import json
    import re
    import random
    from typing import Dict, List, Optional, Any
    from dataclasses import dataclass, field
    from enum import Enum
    import logging

# Configure logging,
    logging.basicConfig()
    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNCTION WORD CATEGORIES - ØªØµÙ†ÙŠÙ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class FunctionWordCategory(Enum):
    """ØªØµÙ†ÙŠÙ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ"""

    PREPOSITIONS = "prepositions"  # Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø±,
    CONJUNCTIONS = "conjunctions"  # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¹Ø·Ù,
    PARTICLES = "particles"  # Ø§Ù„Ø£Ø¯ÙˆØ§Øª,
    INTERROGATIVES = "interrogatives"  # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…,
    NEGATIONS = "negations"  # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù†ÙÙŠ,
    DETERMINERS = "determiners"  # Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ¯,
    CONDITIONALS = "conditionals"  # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø´Ø±Ø·,
    RELATIVE_PRONOUNS = "relative_pronouns"  # Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙˆØµÙˆÙ„Ø©


@dataclass,
    class FunctionWordPattern:
    """Ù†Ù…Ø· Ø­Ø±Ù Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ"""

    category: FunctionWordCategory,
    syllable_patterns: List[str]
    phonetic_constraints: Dict[str, Any]
    semantic_features: List[str]
    frequency_weight: float = 1.0


@dataclass,
    class GeneratedFunctionWord:
    """Ø­Ø±Ù Ù…Ø¹Ù†Ù‰ Ù…ÙˆÙ„Ø¯"""

    word: str,
    category: FunctionWordCategory,
    pattern: str,
    syllable_breakdown: List[str]
    phonetic_features: Dict[str, Any]
    similarity_score: float,
    is_authentic: bool = False,
    examples: List[str] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC PHONETIC ANALYZER - Ù…Ø­Ù„Ù„ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicPhoneticAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„ØµÙˆØªÙŠØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ"""

    def __init__(self):

        # Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©,
    self.consonants = {
    'Ø¡',
    'Ø¨',
    'Øª',
    'Ø«',
    'Ø¬',
    'Ø­',
    'Ø®',
    'Ø¯',
    'Ø°',
    'Ø±',
    'Ø²',
    'Ø³',
    'Ø´',
    'Øµ',
    'Ø¶',
    'Ø·',
    'Ø¸',
    'Ø¹',
    'Øº',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    'Ùˆ',
    'ÙŠ',
    }

        # Ø§Ù„ØµÙˆØ§Ø¦Øª ÙˆØ§Ù„Ø­Ø±ÙƒØ§Øª,
    self.vowels = {
    'short': {'Ù', 'Ù', 'Ù'},  # Ø­Ø±ÙƒØ§Øª Ù‚ØµÙŠØ±Ø©
    'long': {'Ø§', 'ÙŠ', 'Ùˆ'},  # Ø­Ø±ÙˆÙ Ù…Ø¯
    'tanween': {'Ù‹', 'ÙŒ', 'Ù'},  # ØªÙ†ÙˆÙŠÙ†
    }

        # Ø¹Ù„Ø§Ù…Ø§Øª Ø£Ø®Ø±Ù‰,
    self.diacritics = {'Ù’', 'Ù‘', 'Ù°', 'Û¡'}  # Ø³ÙƒÙˆÙ†  # Ø´Ø¯Ø©  # Ø£Ù„Ù Ø®Ù†Ø¬Ø±ÙŠØ©  # Ø³ÙƒÙˆÙ† ØµØºÙŠØ±

        # Ø£Ù†Ù…Ø§Ø· ØµÙˆØªÙŠØ© ØµØ¹Ø¨Ø© (ÙŠØ¬Ø¨ ØªØ¬Ù†Ø¨Ù‡Ø§)
    self.difficult_patterns = [
    r'(.)\1\1',  # Ø«Ù„Ø§Ø«Ø© Ø£Ø­Ø±Ù Ù…ØªØªØ§Ù„ÙŠØ©,
    r'[Ù‚Ø·ØµØ¶Ø¸][ÙƒØªØ«]',  # ØªØªØ§Ø¨Ø¹ ØµÙˆØ§Ù…Øª Ø«Ù‚ÙŠÙ„Ø©,
    r'Ø¡Ø¡',  # Ù‡Ù…Ø²ØªØ§Ù† Ù…ØªØªØ§Ù„ÙŠØªØ§Ù†,
    r'[Ù‘Ù’][Ù‘Ù’]',  # Ø³ÙƒÙˆÙ†Ø§Ù† Ø£Ùˆ Ø´Ø¯ØªØ§Ù† Ù…ØªØªØ§Ù„ÙŠØªØ§Ù†
    ]

    def analyze_word(self, word: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""

    analysis = {
    'length': len(word),
    'syllable_count': self._count_syllables(word),
    'consonant_count': self._count_consonants(word),
    'vowel_count': self._count_vowels(word),
    'initial_sound': word[0] if word else '',
    'final_sound': word[ 1] if word else '',
    'vowel_pattern': self._extract_vowel_pattern(word),
    'has_sukoon': 'Ù’' in word,
    'has_shadda': 'Ù‘' in word,
    'has_tanween': any(t in word for t in self.vowels['tanween']),
    'phonetic_weight': self._calculate_phonetic_weight(word),
    'is_difficult': self._has_difficult_patterns(word),
    }

    return analysis,
    def _count_syllables(self, word: str) -> int:
    """Ø¹Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©"""
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ·: Ø¹Ø¯ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¯,
    vowel_count = 0,
    vowel_count += len([c for c in word if c in self.vowels['short']])
    vowel_count += len([c for c in word if c in self.vowels['long']])
    vowel_count += len([c for c in word if c in self.vowels['tanween']])
    return max(1, vowel_count)

    def _count_consonants(self, word: str) -> int:
    """Ø¹Ø¯ Ø§Ù„ØµÙˆØ§Ù…Øª"""
    return len([c for c in word if c in self.consonants])

    def _count_vowels(self, word: str) -> int:
    """Ø¹Ø¯ Ø§Ù„ØµÙˆØ§Ø¦Øª ÙˆØ§Ù„Ø­Ø±ÙƒØ§Øª"""
    vowel_count = 0,
    vowel_count += len([c for c in word if c in self.vowels['short']])
    vowel_count += len([c for c in word if c in self.vowels['long']])
    vowel_count += len([c for c in word if c in self.vowels['tanween']])
    return vowel_count,
    def _extract_vowel_pattern(self, word: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø· Ø§Ù„Ø­Ø±ÙƒØ§Øª"""
    pattern = []
        for char in word:
            if char in self.vowels['short']:
    pattern.append('V')
            elif char in self.vowels['long']:
    pattern.append('VV')
            elif char in self.vowels['tanween']:
    pattern.append('VN')
            elif char == 'Ù’':
    pattern.append('0')
    return ''.join(pattern)

    def _calculate_phonetic_weight(self, word: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµÙˆØªÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©"""
    weight = 0.0

        # ÙˆØ²Ù† Ø§Ù„Ø·ÙˆÙ„,
    weight += len(word) * 0.1

        # ÙˆØ²Ù† Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©,
    heavy_consonants = {'Ù‚', 'Ø·', 'Øµ', 'Ø¶', 'Ø¸', 'Ø¹', 'Øº', 'Ø®', 'Ø­'}
    weight += len([c for c in word if c in heavy_consonants]) * 0.5

        # ÙˆØ²Ù† Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©,
    weight += len([c for c in word if c in self.vowels['long']]) * 0.3

        # ÙˆØ²Ù† Ø§Ù„ØªØ´Ø¯ÙŠØ¯,
    weight += word.count('Ù‘') * 0.4,
    return weight,
    def _has_difficult_patterns(self, word: str) -> bool:
    """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø£Ù†Ù…Ø§Ø· ØµÙˆØªÙŠØ© ØµØ¹Ø¨Ø©"""
        for pattern in self.difficult_patterns:
            if re.search(pattern, word):
    return True,
    return False,
    def calculate_similarity(self, word1: str, word2: str) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØµÙˆØªÙŠ Ø¨ÙŠÙ† ÙƒÙ„Ù…ØªÙŠÙ†"""

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø·ÙˆÙ„,
    length_sim = 1 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø£Ø­Ø±Ù,
    set1, set2 = set(word1), set(word2)
    char_sim = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø­Ø±ÙƒØ§Øª,
    pattern1 = self._extract_vowel_pattern(word1)
    pattern2 = self._extract_vowel_pattern(word2)
    pattern_sim = 1 if pattern1 == pattern2 else 0.5 if pattern1 and pattern2 else 0

        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©,
    start_sim = 1 if word1[:1] == word2[:1] else 0,
    end_sim = 1 if word1[-1:] == word2[-1:] else 0

        # Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ø­,
    similarity = ()
    0.3 * length_sim
    + 0.4 * char_sim
    + 0.2 * pattern_sim
    + 0.05 * start_sim
    + 0.05 * end_sim
    )

    return similarity


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENHANCED FUNCTION WORDS GENERATOR - Ù…ÙˆÙ„Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø­Ø³Ù†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EnhancedArabicFunctionWordsGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    def __init__(self, syllables_database: Optional[List[Dict]] = None):

    self.syllables_db = syllables_database or self._create_mock_syllables()
    self.phonetic_analyzer = ArabicPhoneticAnalyzer()

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯,
    self._load_function_word_patterns()
    self._load_authentic_function_words()

    logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.syllables_db)} Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠ")
    logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.function_word_patterns)} Ù†Ù…Ø· Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ")

    def _create_mock_syllables(self) -> List[Dict]:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹"""

    mock_syllables = []

        # Ù…Ù‚Ø§Ø·Ø¹ CV,
    consonants = [
    'Ø¨',
    'Øª',
    'Ø¬',
    'Ø¯',
    'Ø±',
    'Ø³',
    'Ø¹',
    'Ù',
    'Ù‚',
    'Ùƒ',
    'Ù„',
    'Ù…',
    'Ù†',
    'Ù‡',
    'Ùˆ',
    'ÙŠ',
    ]
    vowels = ['Ù', 'Ù', 'Ù']

        for c in consonants:
            for v in vowels:
    mock_syllables.append()
    {
    'syllable': c + v,
    'pattern': 'CV',
    'consonants': [c],
    'vowels': [v],
    'weight': 'light',
    }
    )

        # Ù…Ù‚Ø§Ø·Ø¹ CVC,
    end_consonants = ['Ù†', 'Ø±', 'Ù„', 'Ù…', 'Øª', 'Ø¯', 'Ø³', 'Ùƒ']
        for c1 in consonants[:8]:  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯,
    for v in vowels:
                for c2 in end_consonants:
    mock_syllables.append()
    {
    'syllable': c1 + v + c2,
    'pattern': 'CVC',
    'consonants': [c1, c2],
    'vowels': [v],
    'weight': 'heavy',
    }
    )

        # Ù…Ù‚Ø§Ø·Ø¹ VC Ù„Ù„Ø¨Ø¯Ø§ÙŠØ©,
    for v in ['Ø£Ù', 'Ø¥Ù', 'Ø£Ù']:
            for c in ['Ù„', 'Ù†', 'Ù…', 'Øª']:
    mock_syllables.append()
    {
    'syllable': v + c,
    'pattern': 'VC',
    'consonants': [c],
    'vowels': [v[1:]],  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‡Ù…Ø²Ø© Ù…Ù† Ø§Ù„Ø­Ø±ÙƒØ©
    'weight': 'medium',
    }
    )

    return mock_syllables,
    def _load_function_word_patterns(self):
    """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©"""

    self.function_word_patterns = {
            # Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± - ØªÙ…ÙŠÙ„ Ù„ØªÙƒÙˆÙ† Ù‚ØµÙŠØ±Ø© ÙˆÙ…ØªØ­Ø±ÙƒØ©,
    FunctionWordCategory.PREPOSITIONS: [
    FunctionWordPattern()
    category=FunctionWordCategory.PREPOSITIONS,
    syllable_patterns=['CV', 'V', 'CVC'],
    phonetic_constraints={
    'max_syllables': 2,
    'preferred_initial': ['Ù', 'Ø¹', 'Ø¨', 'Ù„', 'Ù…', 'Ø¥'],
    'avoid_heavy_consonants': True,
    'prefer_liquid_consonants': ['Ù„', 'Ø±', 'Ù†', 'Ù…'],
    },
    semantic_features=['spatial', 'directional', 'locative'],
    frequency_weight=1.0),
    ],
            # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¹Ø·Ù - Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    FunctionWordCategory.CONJUNCTIONS: [
    FunctionWordPattern()
    category=FunctionWordCategory.CONJUNCTIONS,
    syllable_patterns=['CV', 'V'],
    phonetic_constraints={
    'max_syllables': 1,
    'preferred_initial': ['Ùˆ', 'Ù', 'Ø«', 'Ø£'],
    'single_consonant_preferred': True,
    'avoid_complex_clusters': True,
    },
    semantic_features=['connective', 'additive', 'sequential'],
    frequency_weight=1.2),
    ],
            # Ø§Ù„Ø£Ø¯ÙˆØ§Øª - Ù…ØªÙ†ÙˆØ¹Ø©,
    FunctionWordCategory.PARTICLES: [
    FunctionWordPattern()
    category=FunctionWordCategory.PARTICLES,
    syllable_patterns=['CV', 'CVC', 'CVCV'],
    phonetic_constraints={
    'max_syllables': 2,
    'prefer_short_vowels': True,
    'avoid_long_words': True,
    },
    semantic_features=['modal', 'aspectual', 'temporal'],
    frequency_weight=0.9),
    ],
            # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø³ØªÙÙ‡Ø§Ù…,
    FunctionWordCategory.INTERROGATIVES: [
    FunctionWordPattern()
    category=FunctionWordCategory.INTERROGATIVES,
    syllable_patterns=['CV', 'CVC', 'CVCV'],
    phonetic_constraints={
    'max_syllables': 2,
    'preferred_initial': ['Ù‡', 'Ø£', 'Ù…', 'Ùƒ'],
    'interrogative_markers': True,
    },
    semantic_features=['question', 'wh word', 'polar'],
    frequency_weight=0.8),
    ],
            # Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù†ÙÙŠ,
    FunctionWordCategory.NEGATIONS: [
    FunctionWordPattern()
    category=FunctionWordCategory.NEGATIONS,
    syllable_patterns=['CV', 'CVC'],
    phonetic_constraints={
    'max_syllables': 2,
    'preferred_initial': ['Ù„', 'Ù…', 'Ù†'],
    'negative_markers': True,
    },
    semantic_features=['negative', 'denial', 'prohibition'],
    frequency_weight=0.9),
    ],
            # Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ¯,
    FunctionWordCategory.DETERMINERS: [
    FunctionWordPattern()
    category=FunctionWordCategory.DETERMINERS,
    syllable_patterns=['V', 'CV', 'CVC'],
    phonetic_constraints={
    'max_syllables': 2,
    'definite_markers': True,
    'preferred_initial': ['Ø£', 'Ø§Ù„', 'Ù‡'],
    },
    semantic_features=['definite', 'demonstrative', 'quantifier'],
    frequency_weight=1.1),
    ],
    }

    def _load_authentic_function_words(self):
    """ØªØ­Ù…ÙŠÙ„ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø£ØµÙŠÙ„Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡"""

    self.authentic_words = {
    FunctionWordCategory.PREPOSITIONS: [
    'ÙÙŠ',
    'Ø¹Ù„Ù‰',
    'Ø¥Ù„Ù‰',
    'Ù…Ù†',
    'Ø¹Ù†',
    'Ù„Ø¯Ù‰',
    'Ø£Ù…Ø§Ù…',
    'Ø®Ù„Ù',
    'ØªØ­Øª',
    'ÙÙˆÙ‚',
    'Ø¨ÙŠÙ†',
    'Ø¹Ù†Ø¯',
    'Ù„Ø¯Ù†',
    'Ù…Ù†Ø°',
    'Ù…Ø°',
    'Ø­ØªÙ‰',
    'Ø³ÙˆÙ‰',
    'Ø®Ù„Ø§',
    'Ø¹Ø¯Ø§',
    'Ø­Ø§Ø´Ø§',
    ],
    FunctionWordCategory.CONJUNCTIONS: [
    'Ùˆ',
    'Ù',
    'Ø«Ù…',
    'Ø£Ùˆ',
    'Ø£Ù…',
    'Ù„ÙƒÙ†',
    'ØºÙŠØ±',
    'Ø³ÙˆÙ‰',
    'Ø¥Ù…Ø§',
    'Ø¨Ù„',
    ],
    FunctionWordCategory.PARTICLES: [
    'Ù‚Ø¯',
    'Ù„Ù‚Ø¯',
    'ÙƒØ§Ù†',
    'Ø¥Ù†',
    'Ø£Ù†',
    'ÙƒÙŠ',
    'Ù„ÙƒÙŠ',
    'Ø­ØªÙ‰',
    'Ù„Ø¹Ù„',
    'Ù„ÙŠØª',
    ],
    FunctionWordCategory.INTERROGATIVES: [
    'Ù‡Ù„',
    'Ø£',
    'Ù…Ø§',
    'Ù…ÙÙ†',
    'Ù…ØªÙ‰',
    'Ø£ÙŠÙ†',
    'ÙƒÙŠÙ',
    'Ù„Ù…Ø§Ø°Ø§',
    'Ø£ÙŠ',
    'ÙƒÙ…',
    ],
    FunctionWordCategory.NEGATIONS: [
    'Ù„Ø§',
    'Ù…Ø§',
    'Ù„Ù†',
    'Ù„Ù…',
    'Ù„ÙŠØ³',
    'ØºÙŠØ±',
    'Ø³ÙˆÙ‰',
    ],
    FunctionWordCategory.DETERMINERS: [
    'Ø§Ù„',
    'Ù‡Ø°Ø§',
    'Ù‡Ø°Ù‡',
    'Ø°Ù„Ùƒ',
    'ØªÙ„Ùƒ',
    'Ø£ÙŠ',
    'ÙƒÙ„',
    'Ø¨Ø¹Ø¶',
    'Ø¬Ù…ÙŠØ¹',
    ],
    }

    def generate_function_words()
    self,
    category: FunctionWordCategory,
    count: int = 50,
    similarity_threshold: float = 0.3) -> List[GeneratedFunctionWord]:
    """ØªÙˆÙ„ÙŠØ¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ù„ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø©"""

    logger.info(f"Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ {count} ÙƒÙ„Ù…Ø© Ù…Ù† ÙØ¦Ø© {category.value}")

    patterns = self.function_word_patterns.get(category, [])
        if not patterns:
    logger.warning(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ù†Ù…Ø§Ø· Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„ÙØ¦Ø© {category.value}")
    return []

    generated_words = []
    attempts = 0,
    max_attempts = count * 10  # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø£ÙƒØ«Ø± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù†ÙˆØ¹ÙŠØ©,
    while len(generated_words) < count and attempts < max_attempts:
    attempts += 1

            # Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…Ø· Ø¹Ø´ÙˆØ§Ø¦ÙŠ,
    pattern = random.choice(patterns)

            # ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø© Ù…Ø±Ø´Ø­Ø©,
    candidate = self._generate_candidate_word(pattern)

            if candidate:
                # ØªØ­Ù„ÙŠÙ„ ØµÙˆØªÙŠ,
    phonetic_analysis = self.phonetic_analyzer.analyze_word(candidate)

                # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙˆØ¯,
    if self._satisfies_constraints(candidate, pattern, phonetic_analysis):

                    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙŠÙ„Ø©,
    similarity_score = self._calculate_authenticity_similarity()
    candidate, category
    )

                    if similarity_score >= similarity_threshold:

                        # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©,
    generated_word = GeneratedFunctionWord()
    word=candidate,
    category=category,
    pattern=self._extract_syllable_pattern(candidate),
    syllable_breakdown=self._breakdown_syllables(candidate),
    phonetic_features=phonetic_analysis,
    similarity_score=similarity_score,
    is_authentic=candidate,
    in self.authentic_words.get(category, []))

    generated_words.append(generated_word)

                        if len(generated_words) % 10 == 0:
    logger.info(f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(generated_words)} ÙƒÙ„Ù…Ø©")

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡,
    generated_words.sort(key=lambda x: x.similarity_score, reverse=True)

    logger.info(f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(generated_words)} ÙƒÙ„Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù† Ø£ØµÙ„ {count} Ù…Ø·Ù„ÙˆØ¨Ø©")

    return generated_words,
    def _generate_candidate_word(self, pattern: FunctionWordPattern) -> Optional[str]:
    """ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø© Ù…Ø±Ø´Ø­Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø·"""

    syllable_patterns = pattern.syllable_patterns,
    max_syllables = pattern.phonetic_constraints.get('max_syllables', 2)

        # Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹,
    num_syllables = random.randint(1, min(max_syllables, 2))

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹,
    word_syllables = []
        for i in range(num_syllables):
            # Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…Ø· Ù…Ù‚Ø·Ø¹ Ù…Ù†Ø§Ø³Ø¨,
    syllable_pattern = random.choice(syllable_patterns)

            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ù†Ù…Ø·,
    matching_syllables = [
    syl,
    for syl in self.syllables_db,
    if syl.get('pattern') == syllable_pattern
    ]

            if matching_syllables:
    chosen_syllable = random.choice(matching_syllables)
    word_syllables.append(chosen_syllable['syllable'])

        if word_syllables:
    candidate = ''.join(word_syllables)
    return self._apply_phonetic_adjustments(candidate, pattern)

    return None,
    def _apply_phonetic_adjustments()
    self, word: str, pattern: FunctionWordPattern
    ) -> str:
    """ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØµÙˆØªÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©"""

    constraints = pattern.phonetic_constraints

        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙØ¶Ù„Ø§Ù‹
    preferred_initial = constraints.get('preferred_initial', [])
        if preferred_initial and word and word[0] not in preferred_initial:
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„,
    word = random.choice(preferred_initial) + word[1:]

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ©,
    word = re.sub(r'(.)\1+', r'\1', word)

        # ØªØ·Ø¨ÙŠÙ‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©,
    word = self._apply_simple_vocalization(word)

    return word,
    def _apply_simple_vocalization(self, word: str) -> str:
    """ØªØ·Ø¨ÙŠÙ‚ ØªØ´ÙƒÙŠÙ„ Ø¨Ø³ÙŠØ· ÙˆÙ…Ù†Ø§Ø³Ø¨"""

        if len(word) == 1:
            # Ø­Ø±Ù ÙˆØ§Ø­Ø¯ - Ø¥Ø¶Ø§ÙØ© Ø­Ø±ÙƒØ© Ù…Ù†Ø§Ø³Ø¨Ø©,
    return word + random.choice(['Ù', 'Ù', 'Ù'])
        elif len(word) == 2:
            # Ø­Ø±ÙØ§Ù† - ØªØ´ÙƒÙŠÙ„ Ù…ØªÙˆØ§Ø²Ù†,
    return word[0] + random.choice(['Ù', 'Ù']) + word[1]
        else:
            # Ø£ÙƒØ«Ø± Ù…Ù† Ø­Ø±ÙÙŠÙ† - ØªØ´ÙƒÙŠÙ„ Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠ,
    result = word[0] + random.choice(['Ù', 'Ù'])
            for i in range(1, len(word)):
    result += word[i]
                if i < len(word) - 1 and random.random() < 0.3:
    result += random.choice(['Ù', 'Ù', 'Ù’'])
    return result,
    def _satisfies_constraints()
    self, word: str, pattern: FunctionWordPattern, analysis: Dict[str, Any]
    ) -> bool:
    """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© ØªØ­Ù‚Ù‚ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""

    constraints = pattern.phonetic_constraints

        # ÙØ­Øµ Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©,
    max_syllables = constraints.get('max_syllables', 3)
        if analysis['syllable_count'] > max_syllables:
    return False

        # ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØµØ¹Ø¨Ø©,
    if analysis['is_difficult']:
    return False

        # ÙØ­Øµ Ø§Ù„ÙˆØ²Ù† Ø§Ù„ØµÙˆØªÙŠ,
    if analysis['phonetic_weight'] > 2.0:  # ÙˆØ²Ù† Ø«Ù‚ÙŠÙ„ Ø¬Ø¯Ø§Ù‹
    return False

        # ÙØ­Øµ ØªØ¬Ù†Ø¨ Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©,
    if constraints.get('avoid_heavy_consonants', False):
    heavy_consonants = {'Ù‚', 'Ø·', 'Øµ', 'Ø¶', 'Ø¸'}
            if any(c in word for c in heavy_consonants):
    return False

        # ÙØ­Øµ ØªÙØ¶ÙŠÙ„ Ø§Ù„ØµÙˆØ§Ù…Øª Ø§Ù„Ø³Ø§Ø¦Ù„Ø©,
    if constraints.get('prefer_liquid_consonants', False):
    liquid_consonants = {'Ù„', 'Ø±', 'Ù†', 'Ù…'}
    consonant_count = analysis['consonant_count']
    liquid_count = len([c for c in word if c in liquid_consonants])
            if consonant_count > 0 and liquid_count / consonant_count < 0.5:
    return False,
    return True,
    def _calculate_authenticity_similarity()
    self, word: str, category: FunctionWordCategory
    ) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ØµÙŠÙ„Ø©"""

    authentic_words = self.authentic_words.get(category, [])
        if not authentic_words:
    return 0.5  # Ù…ØªÙˆØ³Ø· Ø§ÙØªØ±Ø§Ø¶ÙŠ

        # Ø­Ø³Ø§Ø¨ Ø£Ù‚ØµÙ‰ ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø£ÙŠ ÙƒÙ„Ù…Ø© Ø£ØµÙŠÙ„Ø©,
    max_similarity = 0.0,
    for auth_word in authentic_words:
    similarity = self.phonetic_analyzer.calculate_similarity(word, auth_word)
    max_similarity = max(max_similarity, similarity)

    return max_similarity,
    def _extract_syllable_pattern(self, word: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø· Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©"""

        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø· CV,
    pattern = ""
        for char in word:
            if char in self.phonetic_analyzer.consonants:
    pattern += "C"
            elif char in self.phonetic_analyzer.vowels['short']:
    pattern += "V"
            elif char in self.phonetic_analyzer.vowels['long']:
    pattern += "VV"

    return pattern,
    def _breakdown_syllables(self, word: str) -> List[str]:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹"""

        # ØªÙ‚Ø³ÙŠÙ… Ù…Ø¨Ø³Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· CV,
    syllables = []
    current_syllable = ""

        for i, char in enumerate(word):
    current_syllable += char

            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø±Ù Ø§Ù„ØªØ§Ù„ÙŠ ØµØ§Ù…Øª ÙˆØ§Ù„Ø­Ø§Ù„ÙŠ ØµØ§Ø¦ØªØŒ Ø§Ù†Ù‡Ù Ø§Ù„Ù…Ù‚Ø·Ø¹,
    if ()
    i < len(word) - 1,
    and char in self.phonetic_analyzer.vowels['short']
    and word[i + 1] in self.phonetic_analyzer.consonants
    ):

    syllables.append(current_syllable)
    current_syllable = ""

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ø£Ø®ÙŠØ±,
    if current_syllable:
    syllables.append(current_syllable)

    return syllables if syllables else [word]

    def generate_comprehensive_report()
    self, results: Dict[FunctionWordCategory, List[GeneratedFunctionWord]]
    ) -> str:
    """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¹Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""

    report = []
    report.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    report.append("ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„ØªÙˆÙ„ÙŠØ¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
    report.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    report.append("")

    total_generated = sum(len(words) for words in results.values())
    report.append(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {total_generated}")
    report.append("")

        for category, words in results.items():
            if not words:
    continue,
    report.append(f"â–¶ {category.value:}")
    report.append(f"  Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {len(words)}")

            # Ø£ÙØ¶Ù„ 5 ÙƒÙ„Ù…Ø§Øª,
    top_words = sorted(words, key=lambda x: x.similarity_score, reverse=True)[
    :5
    ]
    report.append("  Ø£ÙØ¶Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©:")

            for i, word in enumerate(top_words, 1):
    authenticity = ()
    "âœ“ Ø£ØµÙŠÙ„Ø©"
                    if word.is_authentic,
    else f"ØªØ´Ø§Ø¨Ù‡: {word.similarity_score:.2f}"
    )
    report.append(f"    {i}. {word.word} - {authenticity}")
    report.append(f"       Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {'} + '.join(word.syllable_breakdown)}")

    report.append("")

    return "\n".join(report)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMO AND TESTING FUNCTIONS - ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def demo_function_words_generation():
    """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ"""

    print("ğŸ”µ Ù…ÙˆÙ„Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†")
    print("=" * 50)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯,
    generator = EnhancedArabicFunctionWordsGenerator()

    # ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø©,
    categories_to_test = [
    FunctionWordCategory.PREPOSITIONS,
    FunctionWordCategory.CONJUNCTIONS,
    FunctionWordCategory.PARTICLES,
    FunctionWordCategory.INTERROGATIVES,
    FunctionWordCategory.NEGATIONS,
    FunctionWordCategory.DETERMINERS,
    ]

    all_results = {}

    for category in categories_to_test:
    print(f"\nğŸ”¸ ØªÙˆÙ„ÙŠØ¯ {category.value}...")
    results = generator.generate_function_words(category, count=20)
    all_results[category] = results,
    if results:
    print(f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(results)} ÙƒÙ„Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­")

            # Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ 3 ÙƒÙ„Ù…Ø§Øª,
    top_3 = results[:3]
            for i, word in enumerate(top_3, 1):
    auth_mark = ()
    "âœ“" if word.is_authentic else f"({word.similarity_score:.2f})"
    )
    print(f"  {i}. {word.word {auth_mark}}")
        else:
    print("Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„Ù…Ø§Øª")

    # ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„,
    print("\n" + "=" * 60)
    print(generator.generate_comprehensive_report(all_results))

    return all_results,
    if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ,
    demo_function_words_generation()
    """Common syllable patterns in Arabic function words"""

    CV = "CV"  # Consonant + Vowel (most common)
    CVC = "CVC"  # Consonant + Vowel + Consonant,
    VC = "VC"  # Vowel + Consonant (rare, but exists)
    CVV = "CVV"  # Consonant + Long Vowel,
    CVVC = "CVVC"  # Consonant + Long Vowel + Consonant


@dataclass,
    class FunctionWordCandidate:
    """Generated function word candidate"""

    word: str,
    syllable_pattern: List[str]
    syllable_components: List[Dict[str, Any]]
    word_type: Optional[FunctionWordCategory]

    # Linguistic features,
    phonological_weight: float,
    morphological_complexity: float,
    frequency_estimate: float

    # Similarity scores,
    similarity_score: float = 0.0,
    closest_known_word: str = ""

    # Validation,
    is_valid: bool = True,
    validation_errors: List[str] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARABIC FUNCTION WORDS GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ArabicFunctionWordsGenerator:
    """
    Comprehensive Arabic function words generator using syllable database,
    Ù…ÙˆÙ„Ø¯ Ø­Ø±ÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠØ©
    """

    def __init__()
    self, syllables_db_file: str = "complete_arabic_syllable_inventory.json"
    ):
    """Initialize the function words generator"""

        # Load syllable database,
    self.syllables_db = self._load_syllables_database(syllables_db_file)

        # Initialize patterns and known function words,
    self.common_patterns = self._initialize_common_patterns()
    self.known_function_words = self._load_known_function_words()

        # Analysis results,
    self.generated_candidates: List[FunctionWordCandidate] = []
    self.validated_function_words: List[FunctionWordCandidate] = []

    logger.info()
    f"ArabicFunctionWordsGenerator initialized with {len(self.syllables_db)} syllables"
    )

    def _load_syllables_database(self, db_file: str) -> Dict[str, List[Dict[str, Any]]]:
    """Load the comprehensive syllable database"""

        try:
            with open(db_file, 'r', encoding='utf 8') as f:
    syllables_data = json.load(f)

    logger.info()
    f"Loaded syllable database with {sum(len(syllables) for syllables} in syllables_data.values())} syllables"
    )
    return syllables_data,
    except FileNotFoundError:
    logger.error(f"âŒ Syllables database {db_file} not found")
    return {}
        except Exception as e:
    logger.error(f"âŒ Error loading syllables database: {e}")
    return {}

    def _initialize_common_patterns(self) -> List[List[str]]:
    """Initialize common syllable patterns for function words"""

    return [
            # Single syllable patterns (most common for function words)
    ['CV'],  # ÙÙŠØŒ Ù„ÙØŒ Ø¨ÙØŒ ÙƒÙ
    ['CVC'],  # Ù…ÙÙ†ØŒ Ø¹ÙÙ†ØŒ Ù‚ÙØ¯ØŒ Ù„ÙÙ…
    ['VC'],  # Ø¥Ù (in Ø¥Ù„Ù‰), Ø£Ù (interrogative)
    ['CVV'],  # Ù„Ø§ØŒ Ù…Ø§ (with long vowels)
            # Two syllable patterns (for longer function words)
    ['CV', 'CV'],  # Ø¥ÙÙ„Ù‰ØŒ Ø¹ÙÙ„Ù‰ØŒ Ù„ÙÙƒÙÙ†
    ['CV', 'CVC'],  # Ø¹ÙÙ„ÙÙŠÙ’Ùƒ (compound forms)
    ['CVC', 'CV'],  # Ø­ÙØªÙÙ‘Ù‰ØŒ Ù„ÙØ¯ÙÙ‰
    ['CV', 'CVV'],  # Ø¥ÙØ°Ø§ØŒ Ù‡ÙØ°Ø§
    ['VC', 'CV'],  # Ø¥ÙÙ„Ù‰ pattern
    ['CVV', 'CV'],  # Ù„Ø§Ù…ÙÙ‘Ø§ØŒ ÙƒÙ„Ø§Ù…ÙÙ‘Ø§
            # Three syllable patterns (rare but exist)
    ['CV', 'CV', 'CV'],  # Ù„ÙØ¹ÙÙ„ÙÙ‘ØŒ Ø¹ÙØ³ÙÙ‰
    ['CVC', 'CV', 'CV'],  # Ù…ÙØ«Ù’Ù„ÙÙ…Ø§
    ]

    def _load_known_function_words(self) -> Dict[FunctionWordType, List[str]]:
    """Load comprehensive list of known Arabic function words"""

    return {
    FunctionWordType.PREPOSITION: [
    'ÙÙŠ',
    'Ù…Ù†',
    'Ø¥Ù„Ù‰',
    'Ø¹Ù„Ù‰',
    'Ø¹Ù†',
    'Ø¨',
    'Ù„',
    'Ùƒ',
    'Ù…Ø¹',
    'Ø¨ÙŠÙ†',
    'ØªØ­Øª',
    'ÙÙˆÙ‚',
    'Ø£Ù…Ø§Ù…',
    'Ø®Ù„Ù',
    'Ø­ÙˆÙ„',
    'Ø¶Ø¯',
    'Ù†Ø­Ùˆ',
    'ØµÙˆØ¨',
    'Ø¹Ù†Ø¯',
    'Ù„Ø¯Ù‰',
    'Ø³ÙˆÙ‰',
    'Ø®Ù„Ø§',
    'Ø¹Ø¯Ø§',
    'Ø­Ø§Ø´Ø§',
    'Ù…Ø§ Ø¹Ø¯Ø§',
    ],
    FunctionWordType.CONJUNCTION: [
    'Ùˆ',
    'Ù',
    'Ø«Ù…',
    'Ø£Ùˆ',
    'Ø£Ù…',
    'Ù„ÙƒÙ†',
    'Ù„ÙƒÙ†',
    'ØºÙŠØ±',
    'Ø³ÙˆÙ‰',
    'Ø¨Ù„',
    'Ù„Ø§',
    'Ø¥Ù…Ø§',
    'ÙƒÙ„Ø§',
    'ÙƒÙ„ØªØ§',
    'Ø­ØªÙ‰',
    'Ø¥Ø°',
    ],
    FunctionWordType.PARTICLE: [
    'Ù‚Ø¯',
    'Ù„Ù‚Ø¯',
    'Ø³ÙˆÙ',
    'Ø³Ù€',
    'ÙƒØ§Ù†',
    'Ù„ÙŠØ³',
    'Ù…Ø§ Ø²Ø§Ù„',
    'Ù…Ø§ ÙŠØ²Ø§Ù„',
    'Ù…Ø§ Ø¯Ø§Ù…',
    'Ù…Ø§ Ø§Ù†ÙÙƒ',
    'Ù…Ø§ Ø¨Ø±Ø­',
    'Ù…Ø§ ÙØªØ¦',
    'Ø¥Ù†',
    'Ø£Ù†',
    'ÙƒÙŠ',
    ],
    FunctionWordType.NEGATION: [
    'Ù„Ø§',
    'Ù…Ø§',
    'Ù„Ù†',
    'Ù„Ù…',
    'Ù„Ù…Ø§',
    'Ù„ÙŠØ³',
    'Ù…Ø§ Ù„ÙŠØ³',
    'ØºÙŠØ±',
    'Ø³ÙˆÙ‰',
    'Ø¥Ù„Ø§',
    'Ø®Ù„Ø§',
    'Ø¹Ø¯Ø§',
    'Ø­Ø§Ø´Ø§',
    ],
    FunctionWordType.INTERROGATIVE: [
    'Ù‡Ù„',
    'Ø£',
    'Ù…Ø§',
    'Ù…Ù†',
    'Ù…ØªÙ‰',
    'Ø£ÙŠÙ†',
    'ÙƒÙŠÙ',
    'ÙƒÙ…',
    'Ø£ÙŠ',
    'Ù…Ø§Ø°Ø§',
    'Ù„Ù…Ø§Ø°Ø§',
    'Ø£ÙŠÙ†',
    'Ø£Ù†Ù‰',
    'ÙƒÙŠÙÙ…Ø§',
    'Ù…Ù‡Ù…Ø§',
    ],
    FunctionWordType.CONDITIONAL: [
    'Ø¥Ù†',
    'Ø¥Ø°Ø§',
    'Ù„Ùˆ',
    'Ù„ÙˆÙ„Ø§',
    'Ù„ÙˆÙ…Ø§',
    'Ø¥Ø°',
    'Ø­ÙŠØ«',
    'ÙƒÙ„Ù…Ø§',
    'Ù…Ù‡Ù…Ø§',
    'Ø£ÙŠÙ†Ù…Ø§',
    'Ø­ÙŠØ«Ù…Ø§',
    'Ù…ØªÙ‰',
    'Ø£ÙŠ',
    'ÙƒÙŠÙÙ…Ø§',
    ],
    FunctionWordType.EMPHASIS: [
    'Ø¥Ù†',
    'Ø£Ù†',
    'Ù„Ù‚Ø¯',
    'Ù‚Ø¯',
    'Ù„Ø§Ù…',
    'Ù†ÙˆÙ†',
    'ÙƒØ§Ù',
    'Ø¥ÙŠØ§Ùƒ',
    'Ù†Ø¹Ù…',
    'Ø¨Ù„Ù‰',
    'ÙƒÙ„Ø§',
    'Ø­Ù‚Ø§',
    'ÙØ¹Ù„Ø§',
    ],
    FunctionWordType.VOCATIVE: [
    'ÙŠØ§',
    'Ø£ÙŠ',
    'Ù‡ÙŠØ§',
    'ÙŠØ§ Ø£ÙŠÙ‡Ø§',
    'ÙŠØ§ Ø£ÙŠØªÙ‡Ø§',
    'Ø£ÙŠØ§',
    'Ù‡ÙŠØª',
    ],
    }

    def generate_candidates()
    self, max_syllables: int = 3
    ) -> List[FunctionWordCandidate]:
    """Generate function word candidates based on syllable patterns"""

    logger.info("ğŸ”§ Starting function word generation...")

    candidates = []

        for pattern in self.common_patterns:
            if len(pattern) > max_syllables:
    continue

            # Get syllables for each position in the pattern,
    syllable_sets = []
            for syllable_type in pattern:
                if syllable_type in self.syllables_db:
                    # Filter syllables by pattern and frequency,
    suitable_syllables = [
    syll,
    for syll in self.syllables_db[syllable_type]
                        if self._is_suitable_for_function_word(syll, syllable_type)
    ]
    syllable_sets.append(suitable_syllables)
                else:
    logger.warning()
    f"âš ï¸ Syllable type {syllable_type} not found in database"
    )
    syllable_sets.append([])

            if not all(syllable_sets):  # Skip if any position has no suitable syllables,
    continue

            # Generate combinations for this pattern,
    pattern_candidates = self._generate_pattern_combinations()
    pattern, syllable_sets
    )
    candidates.extend(pattern_candidates)

    logger.info()
    f"Generated {len(pattern_candidates) candidates for pattern} {' '.join(pattern)}}"
    )

    logger.info(f"âœ… Generated {len(candidates)} total candidates")
    return candidates,
    def _is_suitable_for_function_word()
    self, syllable: Dict[str, Any], syllable_type: str
    ) -> bool:
    """Check if a syllable is suitable for function words"""

        # Check frequency - function words typically use high-frequency syllables,
    if syllable.get('frequency_estimate', 0) < 0.01:  # 1% threshold,
    return False

        # Check phonological complexity - function words prefer simple syllables,
    features = syllable.get('features', {})
        if features.get('articulatory_complexity', 0)  > 2.0:
    return False

        # Avoid certain complex consonant clusters in onset/coda,
    onset = syllable.get('onset', [])
    coda = syllable.get('coda', [])

        # Function words rarely have complex onsets,
    if len(onset) > 1:
    return False

        # Function words rarely have complex codas,
    if len(coda) > 1:
    return False

        # Avoid pharyngeal and uvular sounds in function words (they're rare)'
    restricted_sounds = {'Ø¹', 'Øº', 'Ø®', 'Ù‚', 'Ø¸', 'Ø¶'}
        if any(sound in restricted_sounds for sound in onset + coda):
    return False,
    return True,
    def _generate_pattern_combinations()
    self, pattern: List[str], syllable_sets: List[List[Dict[str, Any]]]
    ) -> List[FunctionWordCandidate]:
    """Generate all combinations for a specific pattern"""

    candidates = []

        # Limit combinations to prevent memory issues,
    max_combinations_per_pattern = 1000,
    combination_count = 0,
    for combination in product(*syllable_sets):
            if combination_count >= max_combinations_per_pattern:
    break

            # Construct the word,
    word_text = ''.join(syll['text'] for syll in combination)

            # Calculate linguistic features,
    phonological_weight = sum()
    syll.get('prosodic_weight', 1.0) for syll in combination
    )
    morphological_complexity = sum()
    syll.get('features', {}).get('morphological_potential', 0.0)
                for syll in combination
    )
    frequency_estimate = sum()
    syll.get('frequency_estimate', 0.0) for syll in combination
    ) / len(combination)

            # Create candidate,
    candidate = FunctionWordCandidate()
    word=word_text,
    syllable_pattern=pattern,
    syllable_components=list(combination),
    word_type=None,  # Will be determined later,
    phonological_weight=phonological_weight,
    morphological_complexity=morphological_complexity,
    frequency_estimate=frequency_estimate)

    candidates.append(candidate)
    combination_count += 1,
    return candidates,
    def validate_and_filter_candidates()
    self, candidates: List[FunctionWordCandidate]
    ) -> List[FunctionWordCandidate]:
    """Validate candidates and filter by linguistic constraints"""

    logger.info("ğŸ” Validating and filtering candidates...")

    validated = []

        for candidate in candidates:
            # Apply validation rules,
    self._validate_function_word_constraints(candidate)

            if candidate.is_valid:
                # Calculate similarity to known function words,
    self._calculate_similarity_scores(candidate)
    validated.append(candidate)

        # Sort by similarity score and frequency,
    validated.sort()
    key=lambda x: (x.similarity_score, x.frequency_estimate), reverse=True
    )

    logger.info(f"âœ… Validated {len(validated)} candidates")
    return validated,
    def _validate_function_word_constraints()
    self, candidate: FunctionWordCandidate
    ) -> None:
    """Apply linguistic constraints specific to function words"""

    word = candidate.word,
    errors = []

        # 1. Length constraint - function words are typically short,
    if len(word) > 6:  # Max 6 characters (generous for Arabic)
    errors.append("Word too long for function word")

        if len(word) < 1:
    errors.append("Word too short")

        # 2. Cannot start with diacritics,
    if word and word[0] in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°Ù±':
    errors.append("Cannot start with diacritic")

        # 3. Cannot end with certain characters,
    if word and word[-1] in 'Ù‘Ù’':
    errors.append("Cannot end with shadda or sukun")

        # 4. Avoid impossible phonological sequences,
    if re.search(r'[Ø¡Ø£Ø¥Ø¢]{2,}', word):  # Multiple hamzas,
    errors.append("Invalid hamza sequence")

        if re.search(r'[Ù‹ÙŒÙÙÙÙ]{3,}', word):  # Too many consecutive diacritics,
    errors.append("Too many consecutive diacritics")

        # 5. Must contain at least one consonant,
    arabic_consonants = 'Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠØ¡Ø£Ø¥Ø¢'
        if not any(char in arabic_consonants for char in word):
    errors.append("Must contain at least one consonant")

        # 6. Check Unicode normalization,
    normalized = unicodedata.normalize('NFC', word)
        if normalized != word:
    candidate.word = normalized

        # Update validation status,
    candidate.validation_errors = errors,
    candidate.is_valid = len(errors) == 0,
    def _calculate_similarity_scores(self, candidate: FunctionWordCandidate) -> None:
    """Calculate similarity to known function words"""

    word = candidate.word,
    max_similarity = 0.0,
    closest_word = ""
    best_type = None

        # Check similarity to all known function words,
    for word_type, known_words in self.known_function_words.items():
            for known_word in known_words:
                # Calculate similarity (using difflib for simplicity)
    similarity = difflib.SequenceMatcher(None, word, known_word).ratio()

                if similarity > max_similarity:
    max_similarity = similarity,
    closest_word = known_word,
    best_type = word_type

        # Update candidate with similarity information,
    candidate.similarity_score = max_similarity,
    candidate.closest_known_word = closest_word,
    candidate.word_type = best_type,
    def classify_by_similarity_threshold()
    self, candidates: List[FunctionWordCandidate], similarity_threshold: float = 0.6
    ) -> Dict[str, List[FunctionWordCandidate]]:
    """Classify candidates by similarity to known function words"""

        classification = {
    'high_similarity': [],  # > similarity_threshold
    'medium_similarity': [],  # 0.3 - similarity_threshold
    'low_similarity': [],  # < 0.3
    'exact_matches': [],  # = 1.0
    }

        for candidate in candidates:
    score = candidate.similarity_score,
    if score == 1.0:
                classification['exact_matches'].append(candidate)
            elif score >= similarity_threshold:
                classification['high_similarity'].append(candidate)
            elif score >= 0.3:
                classification['medium_similarity'].append(candidate)
            else:
                classification['low_similarity'].append(candidate)

    return classification,
    def generate_comprehensive_function_words()
    self, max_candidates: int = 500, similarity_threshold: float = 0.6
    ) -> Dict[str, Any]:
    """Main function to generate comprehensive function words"""

    logger.info("ğŸš€ Starting comprehensive Arabic function words generation...")

        import time,
    start_time = time.time()

        # Step 1: Generate candidates,
    candidates = self.generate_candidates()

        # Step 2: Validate and filter,
    validated_candidates = self.validate_and_filter_candidates(candidates)

        # Step 3: Take top candidates,
    top_candidates = validated_candidates[:max_candidates]

        # Step 4: Classify by similarity,
    classification = self.classify_by_similarity_threshold()
    top_candidates, similarity_threshold
    )

        # Step 5: Generate statistics,
    processing_time = time.time() - start_time,
    results = {
    'metadata': {
    'generator': 'ArabicFunctionWordsGenerator',
    'version': '1.0.0',
    'generated_date': '2025-07 24',
    'total_candidates_generated': len(candidates),
    'validated_candidates': len(validated_candidates),
    'top_candidates': len(top_candidates),
    'processing_time': processing_time,
    'similarity_threshold': similarity_threshold,
    },
    'statistics': {
    'syllable_database_size': sum()
    len(syllables) for syllables in self.syllables_db.values()
    ),
    'patterns_used': len(self.common_patterns),
    'known_function_words': sum()
    len(words) for words in self.known_function_words.values()
    ),
    'exact_matches': len(classification['exact_matches']),
    'high_similarity': len(classification['high_similarity']),
    'medium_similarity': len(classification['medium_similarity']),
    'low_similarity': len(classification['low_similarity']),
    },
    'classification': {
    category: [
    {
    'word': candidate.word,
    'pattern': ' '.join(candidate.syllable_pattern),
    'similarity_score': candidate.similarity_score,
    'closest_known_word': candidate.closest_known_word,
    'word_type': ()
    candidate.word_type.value if candidate.word_type else None
    ),
    'frequency_estimate': candidate.frequency_estimate,
    'phonological_weight': candidate.phonological_weight,
    }
                    for candidate in candidates_list
    ]
                for category, candidates_list in classification.items()
    },
    'top_function_words': [
    {
    'word': candidate.word,
    'pattern': ' '.join(candidate.syllable_pattern),
    'similarity_score': candidate.similarity_score,
    'closest_known_word': candidate.closest_known_word,
    'word_type': ()
    candidate.word_type.value if candidate.word_type else None
    ),
    'syllable_components': [
    syll['text'] for syll in candidate.syllable_components
    ],
    'linguistic_features': {
    'frequency_estimate': candidate.frequency_estimate,
    'phonological_weight': candidate.phonological_weight,
    'morphological_complexity': candidate.morphological_complexity,
    },
    }
                for candidate in top_candidates[:50]  # Top 50 for detailed analysis
    ],
    }

    logger.info("ğŸ¯ GENERATION COMPLETE!")
    logger.info(f"   Total candidates: {len(candidates)}")
    logger.info(f"   Validated: {len(validated_candidates)}")
    logger.info(f"   Exact matches: {len(classification['exact_matches'])}")
    logger.info(f"   High similarity: {len(classification['high_similarity'])}")
    logger.info(f"   Processing time: {processing_time:.2f seconds}")

    return results,
    def save_results()
    self, results: Dict[str, Any], filename: str = "arabic_function_words.json"
    ) -> None:
    """Save generation results to file"""

        with open(filename, 'w', encoding='utf 8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ’¾ Results saved to: {filename}")
    logger.info()
    f"   File size: ~{len(json.dumps(results, ensure_ascii=False))} / 1024:.1f} KB"
    )


def main():
    """Main function to demonstrate the function words generator"""

    logger.info("ğŸš€ ARABIC FUNCTION WORDS GENERATOR")
    logger.info("=" * 70)

    # Initialize generator,
    generator = ArabicFunctionWordsGenerator()

    # Generate function words,
    results = generator.generate_comprehensive_function_words()
    max_candidates=500, similarity_threshold=0.6
    )

    # Save results,
    generator.save_results(results)

    # Display sample results,
    logger.info("\nğŸ“Š SAMPLE RESULTS:")

    # Show exact matches,
    exact_matches = results['classification']['exact_matches']
    if exact_matches:
    logger.info(f"\nâœ… EXACT MATCHES ({len(exact_matches)}):")
        for match in exact_matches[:10]:
    logger.info()
    f"   {match['word']} ({match['pattern'])} - {match['word_type']}}"
    )

    # Show high similarity,
    high_sim = results['classification']['high_similarity']
    if high_sim:
    logger.info(f"\nğŸ¯ HIGH SIMILARITY ({len(high_sim)}):")
        for candidate in high_sim[:10]:
    logger.info()
    f"   {candidate['word']} ({candidate['pattern']}) - "
    f"Similar to: {candidate['closest_known_word']} "
    f"({candidate['similarity_score']:.2f)}"
    )

    # Final summary,
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ† ARABIC FUNCTION WORDS GENERATION COMPLETE")
    logger.info("=" * 70)
    logger.info("Generator: ArabicFunctionWordsGenerator v1.0.0")
    logger.info()
    f"Total Candidates: {results['metadata']['total_candidates_generated']}"
    )
    logger.info(f"Validated: {results['metadata']['validated_candidates']}")
    logger.info(f"Exact Matches: {results['statistics']['exact_matches']}")
    logger.info(f"High Similarity: {results['statistics']['high_similarity']}")
    logger.info()
    f"Processing Time: {results['metadata']['processing_time']:.2f} seconds"
    )
    logger.info("=" * 70)

    return generator, results,
    if __name__ == "__main__":
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†,
    demo_function_words_generation()

